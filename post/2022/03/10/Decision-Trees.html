<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
<meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="twitter:card" content="summary" /><title>Intro to Ensembles | MoSpace</title>
<meta name="description" content="From Decision Trees to ensembles with Random Forests">

<!-- SEO and meta information for twitter/whatsupp cards --><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Intro to Ensembles | MoSpace</title>
<meta name="generator" content="Jekyll v4.0.1" />
<meta property="og:title" content="Intro to Ensembles" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="From Decision Trees to ensembles with Random Forests" />
<meta property="og:description" content="From Decision Trees to ensembles with Random Forests" />
<link rel="canonical" href="https://moraouf.github.io/MoSpace/post/2022/03/10/Decision-Trees.html" />
<meta property="og:url" content="https://moraouf.github.io/MoSpace/post/2022/03/10/Decision-Trees.html" />
<meta property="og:site_name" content="MoSpace" />
<meta property="og:image" content="https://moraouf.github.io/MoSpace/images/posts/DTPost.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-03-10T00:00:00-06:00" />
<script type="application/ld+json">
{"description":"From Decision Trees to ensembles with Random Forests","@type":"BlogPosting","publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"https://moraouf.github.io/MoSpace/images/logo.png"}},"url":"https://moraouf.github.io/MoSpace/post/2022/03/10/Decision-Trees.html","headline":"Intro to Ensembles","dateModified":"2022-03-10T00:00:00-06:00","datePublished":"2022-03-10T00:00:00-06:00","image":"https://moraouf.github.io/MoSpace/images/posts/DTPost.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://moraouf.github.io/MoSpace/post/2022/03/10/Decision-Trees.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<!-- Feed --><link type="application/atom+xml" rel="alternate" href="https://moraouf.github.io/MoSpace/atom.xml" title="MoSpace" /><!-- CSS --><link rel="shortcut icon" type="image/png" href="/MoSpace/images/logo.png">
<link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="https://moraouf.github.io/MoSpace/assets/css/main.css" />
<noscript><link rel="stylesheet" href="https://moraouf.github.io/MoSpace/assets/css/noscript.css" /></noscript>
<link rel="stylesheet" href="https://moraouf.github.io/MoSpace/assets/stylesheets/main.css" />
<link rel="stylesheet" href="https://moraouf.github.io/MoSpace/assets/css/academicons.min.css"/>

<!-- Analytics --><!-- Math -->

    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


  </head>
  <body class="is-loading">

    <!-- Wrapper -->
      <div id="wrapper" class="fade-in">

        <!-- Header -->
        <header id="header">
          <a href="https://moraouf.github.io/MoSpace/" class="logo">MoSpace</a>
        </header>

        <!-- Nav -->
          <nav id="nav">

            <ul class="links">
  <li class=""><a href="https://moraouf.github.io/MoSpace/">Home</a></li>
  <li class=""><a href="https://moraouf.github.io/MoSpace/blog/">Blog</a></li>
  <li class=""><a href="https://moraouf.github.io/MoSpace/projects/">Projects</a></li>
  <!-- <li class=""><a href="https://moraouf.github.io/MoSpace/categories/">Categories</a></li> -->
  <li class=""><a href="https://moraouf.github.io/MoSpace/tags/">Tags</a></li>
  <li class=""><a href="https://moraouf.github.io/MoSpace/about/">About me</a></li>
  <li class=""><a href="https://drive.google.com/file/d/19H51_xoyo2tGBWKdXFZNAEAPuulesaV9/view?usp=sharing">My CV</a></li>
</ul>

<ul class="icons">
  <li class=""><a href="https://moraouf.github.io/MoSpace/search" class="icon fa-search"><span class="label">Search</span></a></li>
  <li><a href="https://github.com/MoRaouf" class="icon fa-github" rel="nofollow"><span class="label">GitHub</span></a></li>
  <!-- <li><a href="https://twitter.com/_MRaouf" class="icon fa-twitter" rel="nofollow"><span class="label">Twitter</span></a></li>-->
  <li><a href="https://linkedin.com/in/" class="icon fa-linkedin" rel="nofollow"><span class="label">LinkedIn</span></a></li>
</ul>


          </nav>

        <!-- Main -->
        <div id="main">

          <!-- Post -->
          <section class="post">
            
            <p><article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Intro to Ensembles</h1><p class="page-description">From Decision Trees to ensembles with Random Forests</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2022-03-10T00:00:00-06:00" itemprop="datePublished">
        Mar 10, 2022
      </time>• 



<span class="read-time" title="Estimated read time">

  11 min read

</span>

    </p>

    
      <p class="category-tags"><i class="fa fa-folder category-tags-icon"></i></i>
      
        <a class="category-tags-link" href="/MoSpace/categories/#post">post</a>
        
      
      </p>
    

    
      <p class="category-tags"><i class="fa fa-tags category-tags-icon"></i></i>
      
        <a class="category-tags-link" href="/MoSpace/tags/#Decision Tress">Decision Tress</a>
        &nbsp;
      
        <a class="category-tags-link" href="/MoSpace/tags/#Random Forest">Random Forest</a>
        &nbsp;
      
        <a class="category-tags-link" href="/MoSpace/tags/#Ensembles">Ensembles</a>
        
      
      </p>
    

    </header>
  <br> 
  <br>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#decision-trees">Decision Trees</a>
<ul>
<li class="toc-entry toc-h3"><a href="#entropy">Entropy</a></li>
<li class="toc-entry toc-h3"><a href="#information-gain">Information Gain</a></li>
<li class="toc-entry toc-h3"><a href="#gini-index">Gini Index</a></li>
<li class="toc-entry toc-h3"><a href="#overfitting">Overfitting</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#ensembles">Ensembles</a>
<ul>
<li class="toc-entry toc-h3"><a href="#predict_proba">predict_proba()</a></li>
<li class="toc-entry toc-h3"><a href="#ensembles-prediction">Ensembles Prediction</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#random-forests">Random Forests</a>
<ul>
<li class="toc-entry toc-h3"><a href="#seeding">Seeding</a></li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#using-bagging--random-feature-subsets-to-form-a-random-forest">Using Bagging &amp; Random feature subsets to form a Random Forest</a></li>
<li class="toc-entry toc-h1"><a href="#well-build-10-trees">We’ll build 10 trees</a></li>
<li class="toc-entry toc-h1"><a href="#each-bag-will-have-60-of-the-number-of-original-rows">Each “bag” will have 60% of the number of original rows</a></li>
</ul><h2 id="decision-trees">
<a class="anchor" href="#decision-trees" aria-hidden="true"><span class="octicon octicon-link"></span></a><strong>Decision Trees</strong>
</h2>

<ul>
  <li>The main idea of a decision tree is to <strong>identify the features which contain the most information regarding the target feature</strong> <strong>and then split the dataset</strong> along the values of these features <strong>such that the target feature values at the resulting nodes are as pure as possible</strong>. <strong>A feature that best separates the uncertainty from information about the target feature is said to be the most informative feature.</strong>
</li>
  <li><strong>It’s useful to think of a decision tree as a flow of data rows. When we make a split, some rows will go to the right, and some will go to the left. As we build the tree deeper and deeper, each node “receives” fewer and fewer rows.</strong></li>
  <li>The process of building a decision tree involves asking a question at every instance and then continuing with the split, When there are multiple features that decide the target value of a particular instance, <strong>which feature should be chosen as the root node to start the splitting process? And in which order should we continue choosing the features at every further split at a node?</strong>
    <ul>
      <li>Here comes the need to measure the informativeness of the features and use the feature with the most information as the feature to split the data on. This informativeness is given by a measure called ‘<strong>information gain</strong>’.</li>
    </ul>
  </li>
  <li>Decision trees typically suffer from high variance. The entire structure of a decision tree can change if we make a minor alteration to its training data. By restricting the depth of the tree, we increase the bias and decrease the variance. If we restrict the depth too much, we increase bias to the point where it underfits.
<br><br>
</li>
</ul>

<h3 id="entropy">
<a class="anchor" href="#entropy" aria-hidden="true"><span class="octicon octicon-link"></span></a>Entropy</h3>

<ul>
  <li>It is used to <strong>measure the impurity or randomness of a dataset</strong>. Imagine choosing a yellow ball from a box of just yellow balls (say 100 yellow balls). Then this box is said to have 0 entropy which implies 0 impurity or total purity.
    <ul>
      <li>
        <p>$Entropy(x)=-\sum[P(x=k)*log_2(P(x=k))]$ , <strong>where x is the target feature we are predicting for.</strong></p>
      </li>
      <li>
        <p><strong>Where $P(x=k)$ is the probability that a target feature takes a specific value, k</strong>. This $P(x=k)$ simply means the proportion of value $k$ over all the number of samples in the target feature. Ex: a target feature of colors (30 red &amp; 70 blue), then $P(x=red)=0.3$ &amp; $P(x=blue)=0.7$.</p>

        <p><img src="/MoSpace/images/posts/DT1.png" alt=""></p>
      </li>
      <li>
        <p><strong>Logarithm of fractions (log with base 2) in the equation gives a negative value and hence a ‘-‘ sign is used in entropy formula to negate these negative values</strong>. The maximum value for entropy depends on the number of classes:</p>
        <ul>
          <li>2 classes: Max entropy is 1</li>
          <li>4 Classes: Max entropy is 2</li>
          <li>8 Classes: Max entropy is 3</li>
          <li>16 classes: Max entropy is 4<br><br>
</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="information-gain">
<a class="anchor" href="#information-gain" aria-hidden="true"><span class="octicon octicon-link"></span></a>Information Gain</h3>

<ul>
  <li>$IG(T,A)=Entropy(T)−∑_{v \in A}\frac{\vert T_v \vert}{\vert T \vert}*Entropy(T_v)$</li>
  <li>We first calculate the entropy of the target feature $T$ before the split. —→» $Entropy(T)$</li>
  <li>Variable $A$ is the feature we will split on. For each unique value $v$ in $A$, we compute the number of rows of $T$ in which $A$ takes on the value $v$ and divide it by the total number of rows in $A$. This simply will give weighted entropy for each value $v$. —→» $\frac{\vert T_v \vert}{\vert T \vert}$</li>
  <li>Next, we multiply the results by the entropy of the target feature where the rows of $A$ is $v$. <strong>(Simply, we isolate the rows where $A=v$ &amp; calculate post-split entropy for feature T)</strong>
</li>
  <li>We add all of these subset entropies together, then subtract from the overall entropy to get information gain.</li>
  <li>
    <p><strong>We choose the feature that gives the HIGHEST information gain, as this indicates lower post-split entropies for the selected feature to split on &amp; more informative &amp; pure nodes.</strong><br><br></p>

    <p>Example:</p>

    <p><img src="/MoSpace/images/posts/DT2.png" alt=""></p>

    <ul>
      <li>Here when $P(age=0)=4/5$ &amp; $P(age=1)=1/5$, so these are the weights ($\frac{\vert T_v \vert}{\vert T \vert}$) of which we will multiply post-split entropies by.</li>
      <li>We split the dataset into two parts, one part where $age=0$ &amp; another when $age=1$. We calculate the entropy for the resulting two part.</li>
      <li>The first part entropy when $age=0$ will have 4 samples of the target $T$ &amp; will equal $-(\frac{2}{4}log_2\frac{2}{4}+\frac{2}{4}log_2\frac{2}{4})$</li>
      <li>The second part entropy when $age=1$ will have 1 sample of the target $T$ &amp; will equal $-(\frac{1}{5}log_2\frac{1}{5}+\frac{0}{5}log_2\frac{0}{5})$<br><br>
</li>
    </ul>
  </li>
</ul>

<h3 id="gini-index">
<a class="anchor" href="#gini-index" aria-hidden="true"><span class="octicon octicon-link"></span></a>Gini Index</h3>

<ul>
  <li>$GI=1-\sum(P(x=k))^2$</li>
  <li>It is calculated by subtracting the sum of squared probabilities of each value $k$ from target feature $T$, from one, in the current node we are splitting to.</li>
  <li><strong>A feature with a lower Gini index is chosen for a split.</strong></li>
  <li>It favors larger partitions and easy to implement whereas information gain favors smaller partitions with distinct values.
<br><br>
</li>
</ul>

<h3 id="overfitting">
<a class="anchor" href="#overfitting" aria-hidden="true"><span class="octicon octicon-link"></span></a>Overfitting</h3>

<p>Three ways to combat overfitting:</p>

<ul>
  <li>“<strong>Prune</strong>” the tree after we build it to remove unnecessary leaves.</li>
  <li>Use <strong>ensembles of trees</strong> to blend the predictions of many trees.</li>
  <li>
<strong>Restrict the depth</strong> of the tree while we’re building it.<br><br><br>
</li>
</ul>

<h2 id="ensembles">
<a class="anchor" href="#ensembles" aria-hidden="true"><span class="octicon octicon-link"></span></a><strong>Ensembles</strong>
</h2>

<h3 id="predict_proba">
<a class="anchor" href="#predict_proba" aria-hidden="true"><span class="octicon octicon-link"></span></a><code class="highlighter-rouge">predict_proba()</code>
</h3>

<ul>
  <li>
<code class="highlighter-rouge">DecisionTreeClassifier.predict_proba()</code> will predict a probability from 0 to 1 that a given class is the right one for a row. This calculation is done through naive bayes, &amp; independence between feature is taken into consideration $[ P(class \vert feature 1, feature 2, feature 3 …) = P(class)<em>P(feature 1)</em>P(feature 2)*P(feature 3)]$.
    <ul>
      <li>Because <code class="highlighter-rouge">0</code> and <code class="highlighter-rouge">1</code> are our two classes, we’ll get a matrix containing the number of rows in the dataset, and two columns.  <code class="highlighter-rouge">predict_proba()</code> will return a result that looks like this:
        <div class="highlighter-rouge">
<div class="highlight"><pre class="highlight"><code>   0      1
  0.7    0.3
  0.2    0.8
  0.1    0.9
</code></pre></div>        </div>
      </li>
      <li>Each row will correspond to a prediction.  The first column is the probability that the prediction is a <code class="highlighter-rouge">0</code>, and the second column is the probability that the prediction is a <code class="highlighter-rouge">1</code>.  Each row adds up to <code class="highlighter-rouge">1</code>.
<br><br>
</li>
    </ul>
  </li>
  <li>The more “diverse” or dissimilar the models we use to construct an ensemble are, the stronger their combined predictions will be (assuming that all of the models have about the same accuracy). Ensembling a decision tree and a logistic regression model, for example, will result in stronger predictions than ensembling two decision trees with similar parameters.  That’s because those two models use very different approaches to arrive at their answers.
    <ul>
      <li>If we build two different decision trees, the models are approaching the same problem in slightly different ways, and building different trees because we used different parameters for each one.  Each tree makes different predictions in different areas.  Even though both trees have about the same accuracy, when we combine them, the result is stronger because it leverages the strengths of both approaches.<br><br>
</li>
    </ul>
  </li>
  <li>On the other hand, if the models we ensemble are very similar in how they make predictions, ensembling will result in a negligible boost.</li>
  <li>Ensembling models with very different accuracies generally won’t improve overall accuracy.  Ensembling a model with a <code class="highlighter-rouge">0.75</code> AUC and a model with a <code class="highlighter-rouge">0.85</code> AUC on a test set will usually result in an AUC somewhere in between the two original values.</li>
  <li>
<strong>min_impurity_decrease</strong> float, default=0.0
A node will be split if this split induces a decrease of the impurity greater than or equal to this value.<br><br>
</li>
</ul>

<h3 id="ensembles-prediction">
<a class="anchor" href="#ensembles-prediction" aria-hidden="true"><span class="octicon octicon-link"></span></a>Ensembles Prediction</h3>

<ul>
  <li>There are multiple methods to get predictions for ensembles of models.</li>
  <li>
<strong>Majority Voting:</strong>
    <ul>
      <li>One method is majority voting, in which <strong>each classifier gets a “vote,” and the most commonly voted value for each row “wins.</strong>”  This only works if there are more than two classifiers (and ideally an odd number, so we don’t have to write a rule to break ties).
        <div class="highlighter-rouge">
<div class="highlight"><pre class="highlight"><code>  DT1        DT2       DT3        Final Prediction
  0           1         0                0
  1           1         1                1
  0           0         1                0
  1           0         0                0
</code></pre></div>        </div>
      </li>
    </ul>
  </li>
  <li>
<strong>Row mean:</strong>
    <ul>
      <li>If we use the <code class="highlighter-rouge">predict_proba()</code> method on two classifiers to generate probabilities as results, take the mean for each row, and then round the results, we’ll get ensemble predictions.<br><br>
</li>
    </ul>
  </li>
</ul>

<h2 id="random-forests">
<a class="anchor" href="#random-forests" aria-hidden="true"><span class="octicon octicon-link"></span></a><strong>Random Forests</strong>
</h2>

<ul>
  <li>A random forest is an ensemble of decision trees. In order to make ensembling effective, we have to introduce variation into each individual decision tree model.</li>
  <li>When we instantiate a <code class="highlighter-rouge">RandomForestClassifier</code> or <code class="highlighter-rouge">RandomForestRegressor</code>, we pass in an <code class="highlighter-rouge">n_estimators</code> parameter that <strong>indicates how many trees to build</strong>.  While adding more trees usually improves accuracy, it also increases the overall time the model takes to train.
    <ul>
      <li><code class="highlighter-rouge">clf = RandomForestClassifier(n_estimators=5, random_state=1, min_samples_leaf=2)</code></li>
    </ul>
  </li>
  <li>
<strong>Bootstrapping</strong> is a statistical resampling technique that involves <strong>random sampling of a dataset <em>with replacement</em></strong>.</li>
  <li>There are two main ways to introduce variation in a random forest – <strong>bagging</strong> and <strong>random feature subsets</strong>.<br><br>
</li>
  <li>Bagging, “Bootstrap Aggregation”:
    <ul>
      <li>Bootstrap aggregation means we have groups (aggregations) of bags where each bag a sampled (bootstrapped) samples of the original dataset.</li>
      <li>In a random forest, we don’t train each tree on the entire data set. <strong>We train it on a random sample of the data, or a “bag,”</strong> instead. We perform this sampling <strong>with replacement</strong>, which means that after we select a row from the data we’re sampling, we put the row back in the data so it can be picked again. Some rows from the original data may appear in the “bag” multiple times.</li>
      <li>When bagging with decision trees, we are less concerned about individual trees overfitting the training data. For this reason and for efficiency, the individual decision trees are grown deep (e.g. few training samples at each leaf-node of the tree) and the trees are not pruned. These trees will have both high variance and low bias. These are important characteristics of sub-models when combining predictions using bagging.<br><br>
</li>
    </ul>
  </li>
  <li>Random Feature Subsets:
    <ul>
      <li>We can also repeat our random subset selection process in scikit-learn.  We just set the <code class="highlighter-rouge">splitter</code> parameter on <code class="highlighter-rouge">DecisionTreeClassifier</code> to <code class="highlighter-rouge">"random"</code>, and the <code class="highlighter-rouge">max_features</code> parameter to<code class="highlighter-rouge">"auto"</code>.  If we have N columns, this will pick a subset of features of size √N, compute the Gini coefficient for each (or information gain), and split the node on the best column in the subset.</li>
      <li>
<strong>splitter</strong>{“best”, “random”}, default=”best”
        <ul>
          <li>The strategy used to choose the split at each node. Supported strategies are “best” to choose the best split and “random” to choose the best random split.</li>
        </ul>
      </li>
      <li>
<strong>max_features</strong> int, float or {“auto”, “sqrt”, “log2”}, default=None
        <ul>
          <li>The number of features to consider when looking for the best split</li>
        </ul>
      </li>
      <li>
        <div class="language-python highlighter-rouge">
<div class="highlight"><pre class="highlight"><code>  <span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
  <span class="n">clf</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</code></pre></div>        </div>
        <p><br></p>
      </li>
    </ul>
  </li>
  <li>The main strengths of a random forest are:
    <ul>
      <li>Very accurate predictions - Random forests achieve near state-of-the-art performance on many machine learning tasks. Along with neural networks and gradient-boosted trees, they’re typically one of the top-performing algorithms.</li>
      <li>Resistance to overfitting - Due to their construction, random forests are fairly resistant to overfitting. We still need to set and tweak parameters like <code class="highlighter-rouge">max_depth</code> though.</li>
    </ul>
  </li>
</ul>

<h3 id="seeding">
<a class="anchor" href="#seeding" aria-hidden="true"><span class="octicon octicon-link"></span></a>Seeding</h3>

<ul>
  <li>The random seed isn’t strictly a hyperparameter, but we introduce it here to highlight that this external parameter can play a role in the effectiveness of training. While this is usually minor, if the model is very complex, and/or the amount of data available is small, the test-set performance of the model can be markedly different if two different seeds are used. In such situations, often it pays to run training with multiple different seeds to assess to what degree your model design is adequate, and to what degree your performance is simply ‘blind luck’.</li>
</ul>

<p>```python {style=”font-size: 16px;”}</p>
<h1 id="using-bagging--random-feature-subsets-to-form-a-random-forest">
<a class="anchor" href="#using-bagging--random-feature-subsets-to-form-a-random-forest" aria-hidden="true"><span class="octicon octicon-link"></span></a>Using Bagging &amp; Random feature subsets to form a Random Forest</h1>
<h1 id="well-build-10-trees">
<a class="anchor" href="#well-build-10-trees" aria-hidden="true"><span class="octicon octicon-link"></span></a>We’ll build 10 trees</h1>
<p>tree_count = 10</p>

<h1 id="each-bag-will-have-60-of-the-number-of-original-rows">
<a class="anchor" href="#each-bag-will-have-60-of-the-number-of-original-rows" aria-hidden="true"><span class="octicon octicon-link"></span></a>Each “bag” will have 60% of the number of original rows</h1>
<p>bag_proportion = 0.6</p>

<p>predictions = []
for i in range(tree_count):
	# BAGGING
    # We select 60% of the rows from train, sampling with replacement
    # We set a random state to ensure we’ll be able to replicate our results
    # We set it to i instead of a fixed value so we don’t get the same sample every time
    bag = train.sample(<strong>frac=bag_proportion</strong>, replace=True, random_state=i)</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Fit a decision tree model to the "bag"
# Random Feature Subsets
clf = DecisionTreeClassifier(random_state=1, min_samples_leaf=2, splitter="random", max_features="auto")
clf.fit(bag[columns], bag["high_income"])

# Using the model, make predictions on the test data
predictions.append(clf.predict_proba(test[columns])[:,1])
</code></pre></div></div>

<p>combined = numpy.sum(predictions, axis=0) / 10
rounded = numpy.round(combined)</p>

<p>print(roc_auc_score(test[“high_income”], rounded))
```</p>

<pre><code class="lang-python">#Using Bagging &amp; Random feature subsets to form a Random Forest
# We'll build 10 trees
tree_count = 10

#Each "bag" will have 60% of the number of original rows
bag_proportion = 0.6

predictions = []
for i in range(tree_count):
	# BAGGING
    # We select 60% of the rows from train, sampling with replacement
    # We set a random state to ensure we'll be able to replicate our results
    # We set it to i instead of a fixed value so we don't get the same sample every time
    bag = train.sample(frac=bag_proportion, replace=True, random_state=i)
    
    # Fit a decision tree model to the "bag"
	# Random Feature Subsets
    clf = DecisionTreeClassifier(random_state=1, min_samples_leaf=2, splitter="random", max_features="auto")
    clf.fit(bag[columns], bag["high_income"])
    
    # Using the model, make predictions on the test data
    predictions.append(clf.predict_proba(test[columns])[:,1])

combined = numpy.sum(predictions, axis=0) / 10
rounded = numpy.round(combined)

print(roc_auc_score(test["high_income"], rounded))</code></pre>

<p><br><br>
<u>**Useful links**</u>:
<br><br>
<a href="https://blog.clairvoyantsoft.com/entropy-information-gain-and-gini-index-the-crux-of-a-decision-tree-99d0cdc699f4">Entropy, Information gain and Gini Index; the crux of a Decision Tree</a></p>

<p><a href="https://medium.datadriveninvestor.com/decision-tree-adventures-2-explanation-of-decision-tree-classifier-parameters-84776f39a28">Decision Tree Adventures 2 - Explanation of Decision Tree Classifier Parameters</a></p>

  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="MoRaouf/MoSpace"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/MoSpace/post/2022/03/10/Decision-Trees.html" hidden></a>
</article>
</p>
          </section>

          <!-- Footer -->
            <footer>
              <ul class="actions">
                <li><a href="https://moraouf.github.io/MoSpace/" class="button">Home</a></li>
                <li><a href="https://moraouf.github.io/MoSpace/blog/" class="button">Blog</a></li>
                <li><a href="https://moraouf.github.io/MoSpace/projects/" class="button">Projects</a></li>
              </ul>
            </footer>
          </div>

        <!-- Footer -->
        <!-- Copyright -->
<div id="copyright">
  <ul>
    <li>&copy; MoRaouf |
      <a href="https://github.com/MoRaouf" class="icon fa-github" rel="nofollow"><span class="label">GitHub</span></a> |
      <a href="https://twitter.com/_MRaouf" class="icon fa-twitter" rel="nofollow"><span class="label">Twitter</span></a> |
      <a href="https://orcid.org/" class="ai ai-orcid" rel="nofollow"><span class="label">ORCID</span></a> |
      <a href="https://scholar.google.es/citations?user=" class="ai ai-google-scholar" rel="nofollow"><span class="label">Google Scholar</span></a> |
      
      <a href="/MoSpace/atom.xml" title="Atom Feed" class="icon fa-rss" rel="nofollow">
        <span class="label">Subscribe</span>
      </a>
      
    </li>
    <li>Powered by <a href="https://fastpages.fast.ai/">fastpages</a></li>
    <li>Design by <a href="https://html5up.net" rel="nofollow">HTML5 UP</a> &
      <a href="https://github.com/mmistakes/jekyll-theme-basically-basic">Basically Basic</a></li>
    <li>Jekyll Integration by <a href="https://jekyllup.com/">JekyllUp</a> </li>
  </ul>
</div>


      </div>

    <!-- Scripts -->
    <!-- DYN -->
<script src="https://moraouf.github.io/MoSpace/assets/js/jquery.min.js"></script>
<script src="https://moraouf.github.io/MoSpace/assets/js/jquery.scrollex.min.js"></script>
<script src="https://moraouf.github.io/MoSpace/assets/js/jquery.scrolly.min.js"></script>
<script src="https://moraouf.github.io/MoSpace/assets/js/skel.min.js"></script>
<script src="https://moraouf.github.io/MoSpace/assets/js/util.js"></script>
<script src="https://moraouf.github.io/MoSpace/assets/js/main.js"></script>


  </body>
</html>
