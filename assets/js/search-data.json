{
  
    
        "post0": {
            "title": "PSO Algorithm for training ANNs",
            "content": "Particle Swarm Optimization Algorithm . import numpy as np import pandas as pd import matplotlib.pyplot as plt import time np.random.seed(10) . def objective_fucntion(x): y = np.sum(x **2) return y . def objective_fucntion_group(x): y = np.sum(x **2, axis = 1) return y . #PSO parameters p = 10 c1 = 1.5 c2 = 1 c3 = 1.5 iterations = 3000 w1 = 0.9 - ((0.9-0.4)/iterations)*np.linspace(0, iterations, iterations) w2 = 0.5 d = 5 n_informants = 3 . particles = np.random.uniform(size = (p, d)) V = np.random.uniform(size = (p, d)) costs = np.zeros((p,1)) costs = objective_fucntion_group(particles) pbest = np.copy(particles) pbest_cost = np.copy(costs) gbest = pbest[pbest_cost.argmin(), :] gbest_cost = pbest_cost.min() overall_best_cost = np.zeros((iterations, 1)) . gbest . array([0.22479665, 0.19806286, 0.76053071, 0.16911084, 0.08833981]) . informants = {} for i in range(1, len(particles) +1): informants[&quot;p&quot;+str(i)] = particles[np.random.choice(particles.shape[0], n_informants, replace=False)] informants_indices = {} for i in range(1, len(particles) +1): informants_indices[&quot;p&quot;+str(i)] = np.random.choice(particles.shape[0], n_informants, replace=False) . informants_indices . {&#39;p1&#39;: array([7, 8, 2]), &#39;p2&#39;: array([0, 4, 6]), &#39;p3&#39;: array([6, 5, 9]), &#39;p4&#39;: array([3, 1, 4]), &#39;p5&#39;: array([4, 9, 2]), &#39;p6&#39;: array([8, 7, 6]), &#39;p7&#39;: array([3, 6, 7]), &#39;p8&#39;: array([2, 8, 1]), &#39;p9&#39;: array([8, 5, 7]), &#39;p10&#39;: array([8, 7, 0])} . informants . {&#39;p1&#39;: array([[0.30070006, 0.11398436, 0.82868133, 0.04689632, 0.62628715], [0.8052232 , 0.52164715, 0.90864888, 0.31923609, 0.09045935], [0.68535982, 0.95339335, 0.00394827, 0.51219226, 0.81262096]]), &#39;p2&#39;: array([[0.68535982, 0.95339335, 0.00394827, 0.51219226, 0.81262096], [0.30070006, 0.11398436, 0.82868133, 0.04689632, 0.62628715], [0.8052232 , 0.52164715, 0.90864888, 0.31923609, 0.09045935]]), &#39;p3&#39;: array([[0.54254437, 0.14217005, 0.37334076, 0.67413362, 0.44183317], [0.61252607, 0.72175532, 0.29187607, 0.91777412, 0.71457578], [0.68535982, 0.95339335, 0.00394827, 0.51219226, 0.81262096]]), &#39;p4&#39;: array([[0.54254437, 0.14217005, 0.37334076, 0.67413362, 0.44183317], [0.68535982, 0.95339335, 0.00394827, 0.51219226, 0.81262096], [0.43401399, 0.61776698, 0.51313824, 0.65039718, 0.60103895]]), &#39;p5&#39;: array([[0.61252607, 0.72175532, 0.29187607, 0.91777412, 0.71457578], [0.30070006, 0.11398436, 0.82868133, 0.04689632, 0.62628715], [0.22479665, 0.19806286, 0.76053071, 0.16911084, 0.08833981]]), &#39;p6&#39;: array([[0.54254437, 0.14217005, 0.37334076, 0.67413362, 0.44183317], [0.43401399, 0.61776698, 0.51313824, 0.65039718, 0.60103895], [0.54758616, 0.819287 , 0.19894754, 0.8568503 , 0.35165264]]), &#39;p7&#39;: array([[0.61252607, 0.72175532, 0.29187607, 0.91777412, 0.71457578], [0.22479665, 0.19806286, 0.76053071, 0.16911084, 0.08833981], [0.75464769, 0.29596171, 0.88393648, 0.32551164, 0.1650159 ]]), &#39;p8&#39;: array([[0.54254437, 0.14217005, 0.37334076, 0.67413362, 0.44183317], [0.22479665, 0.19806286, 0.76053071, 0.16911084, 0.08833981], [0.61252607, 0.72175532, 0.29187607, 0.91777412, 0.71457578]]), &#39;p9&#39;: array([[0.30070006, 0.11398436, 0.82868133, 0.04689632, 0.62628715], [0.54758616, 0.819287 , 0.19894754, 0.8568503 , 0.35165264], [0.8052232 , 0.52164715, 0.90864888, 0.31923609, 0.09045935]]), &#39;p10&#39;: array([[0.54758616, 0.819287 , 0.19894754, 0.8568503 , 0.35165264], [0.30070006, 0.11398436, 0.82868133, 0.04689632, 0.62628715], [0.61252607, 0.72175532, 0.29187607, 0.91777412, 0.71457578]])} . lbest = [] lbest_cost = [] for p in informants: local_informants = informants[p] l_costs = objective_fucntion_group(local_informants) lbest.append(local_informants[l_costs.argmin(), :]) lbest_cost.append(l_costs.min()) lbest = np.array(lbest) lbest_cost = np.array(lbest_cost) . lbest . array([[0.30070006, 0.11398436, 0.82868133, 0.04689632, 0.62628715], [0.30070006, 0.11398436, 0.82868133, 0.04689632, 0.62628715], [0.54254437, 0.14217005, 0.37334076, 0.67413362, 0.44183317], [0.54254437, 0.14217005, 0.37334076, 0.67413362, 0.44183317], [0.22479665, 0.19806286, 0.76053071, 0.16911084, 0.08833981], [0.54254437, 0.14217005, 0.37334076, 0.67413362, 0.44183317], [0.22479665, 0.19806286, 0.76053071, 0.16911084, 0.08833981], [0.22479665, 0.19806286, 0.76053071, 0.16911084, 0.08833981], [0.30070006, 0.11398436, 0.82868133, 0.04689632, 0.62628715], [0.30070006, 0.11398436, 0.82868133, 0.04689632, 0.62628715]]) . def update_informants(local_informnts_indices, X): local_informnts_indices = list(local_informnts_indices) new_local_informants = X[local_informnts_indices , :] return new_local_informants . PSO without Informants . X = np.copy(particles) for i in range(iterations): for j in range(10): V[j] = w1[i] * V[j] + c1 * np.random.rand(d) * (pbest[j] - X[j])+ c3 * np.random.rand(d) * (gbest - X[j]) X[j] = X[j] + V[j] costs[j] = objective_fucntion(X[j]) if costs[j] &lt; pbest_cost[j]: pbest[j] = X[j] pbest_cost[j] = costs[j] if costs[j] &lt; gbest_cost: gbest = X[j] gbest_cost = costs[j] overall_best_cost[i] = gbest_cost . pso_global = gbest pso_global . array([-1.43505619e-94, 2.95694915e-94, 3.36451706e-94, -1.63339754e-94, -2.10542732e-94]) . plt.figure(figsize = (10,10)) plt.semilogy(overall_best_cost) plt.xlim([0, iterations]) plt.title(&quot;PSO without Informants&quot;, fontsize = 20) . Text(0.5, 1.0, &#39;PSO without Informants&#39;) . gbest_cost . 2.922372131128654e-187 . PSO with Informants included . X = np.copy(particles) informants_new = informants.copy() for i in range(iterations): for j in range(10): V[j] = w1[i] * V[j] + c1 * np.random.rand(d) * (pbest[j] - X[j])+ c2 * np.random.rand(d) * (lbest[j] - X[j])+ c3 * np.random.rand(d) * (gbest - X[j]) X[j] = X[j] + V[j] costs[j] = objective_fucntion(X[j]) if costs[j] &lt; pbest_cost[j]: pbest[j] = X[j] pbest_cost[j] = costs[j] if costs[j] &lt; gbest_cost: gbest = X[j] gbest_cost = costs[j] #update informants lcosts = np.zeros((n_informants,1)) for i, p in enumerate(informants_new): informants_new[p] = update_informants(informants_indices[p] , X) lcosts = objective_fucntion_group(informants_new[p]) local_informants = informants_new[p] if lcosts.min() &lt; lbest_cost[i]: lbest[i] = local_informants[lcosts.argmin(), :] lbest_cost[i] = lcosts.min() overall_best_cost[i] = gbest_cost . pso_local_global = gbest pso_local_global . array([ 5.09279709e-124, 1.68712815e-123, 5.68051795e-124, -6.28723826e-124, -4.46644209e-123]) . plt.figure(figsize = (10,10)) plt.semilogy(overall_best_cost) plt.xlim([0, iterations]) plt.title(&quot;PSO with Informants&quot;, fontsize = 20) . Text(0.5, 1.0, &#39;PSO with Informants&#39;) . gbest_cost . 2.377284861761937e-245 . Trying different hyperparameters . X = np.copy(particles) it = 5000 c4 = 0.7 c5 = 2 c6 = 0.8 overall_best_cost = np.zeros((it, 1)) for i in range(it): for j in range(10): V[j] = w2 * V[j] + c4 * np.random.rand(d) * (pbest[j] - X[j])+ c6 * np.random.rand(d) * (gbest - X[j]) X[j] = X[j] + V[j] costs[j] = objective_fucntion(X[j]) if costs[j] &lt; pbest_cost[j]: pbest[j] = X[j] pbest_cost[j] = costs[j] if costs[j] &lt; gbest_cost: gbest = X[j] gbest_cost = costs[j] overall_best_cost[i] = gbest_cost . plt.figure(figsize = (10,10)) plt.semilogy(overall_best_cost) plt.xlim([0, iterations]) plt.title(&quot;PSO without Informants&quot;, fontsize = 20) . Text(0.5, 1.0, &#39;PSO without Informants&#39;) . Training ANN with PSO . df = pd.read_csv(&#39;data_banknote_authentication.txt&#39;,names=[&#39;VAR&#39;, &#39;SKEW&#39;, &#39;Curt&#39;, &#39;Entropy&#39;, &#39;Class&#39;]) data = np.array(df) print(&quot;Input Data shape: &quot;, data.shape) . Input Data shape: (1372, 5) . #input data &amp; target X_data = data[:, :4].T Y = data[:, 4, None].T #ANN layers m = 1372 n_x = 4 n_h = 5 n_y = 1 #PSO dimensions &amp; No of particles d_fb = int(n_x*n_h + n_h*n_y + n_h + n_y) #No of dimensions for forward propagation p = int(10) #No of particles for forward propagation print(&quot;X_data shape: &quot;, X_data.shape) print(&quot;Y shape: &quot;, Y.shape) . X_data shape: (4, 1372) Y shape: (1, 1372) . def initialize_particles(p, d_fb): fb_particles = np.random.uniform(size = (p, d_fb)) fb_V = np.random.uniform(size = (p, d_fb)) return fb_particles, fb_V . def sigmoid(Z): return 1/(1+np.exp(-Z)) . def forward_propagation_one(X_data, one_particle_fb, n_x, n_h, n_y): W1 = one_particle_fb[0:20].reshape((n_h,n_x)) b1 = one_particle_fb[20:25].reshape((n_h,1)) W2 = one_particle_fb[25:30].reshape((n_y,n_h)) b2 = one_particle_fb[30:31].reshape((n_y,1)) Z1 = np.dot(W1, X_data)+b1 A1 = sigmoid(Z1) Z2 = np.dot(W2, A1)+b2 A2 = sigmoid(Z2) return A2 . def compute_cost(A, Y): m = Y.shape[1] cost = -(1/m) * np.sum(Y * np.log(A) + (1-Y) * np.log(1-A)) return cost . def compute_cost_2(A, Y): m = Y.shape[1] cost = (1/m)*np.sum((Y-A)**2) return cost . def compute_accuracy(X_data, Y, one_particle_fb, n_x, n_h, n_y): A= forward_propagation_one(X_data, one_particle_fb, n_x, n_h, n_y) A = np.where(A&lt;0.5, 0, 1) acc = np.mean(A == Y) return acc . Training ANN using informed PSO . Using Cross-Entropy as cost function . particles_fb = np.random.uniform(size = (p, d_fb)) V_fb = np.random.uniform(size = (p, d_fb)) costs = np.zeros((p,1)) for i, particle in enumerate(particles_fb): fb_output = forward_propagation_one(X_data, particle, n_x, n_h, n_y) costs[i] = compute_cost(fb_output, Y) pbest = np.copy(particles_fb) pbest_cost = np.copy(costs) gbest = pbest[pbest_cost.argmin(), :] gbest_cost = pbest_cost.min() overall_best_cost = np.zeros((iterations, 1)) # print(particles_fb, &quot; n n&quot;) # print(V_fb) . #creating informants for ANN informants_fb = {} for i in range(1, len(particles_fb) +1): informants_fb[&quot;p&quot;+str(i)] = particles_fb[np.random.choice(particles_fb.shape[0], n_informants, replace=False)] informants_indices_fb = {} for i in range(1, len(particles_fb) +1): informants_indices_fb[&quot;p&quot;+str(i)] = np.random.choice(particles_fb.shape[0], n_informants, replace=False) #Choosing lbest for each particle lbest = [] lbest_cost = [] for p in informants_fb: local_informants_fb = informants_fb[p] l_costs = np.zeros((3,1)) for i, l in enumerate(local_informants_fb): fb_output = forward_propagation_one(X_data, l, n_x, n_h, n_y) l_costs[i] = compute_cost(fb_output, Y) lbest.append(local_informants_fb[l_costs.argmin(), :]) lbest_cost.append(l_costs.min()) lbest = np.array(lbest) lbest_cost = np.array(lbest_cost) . # Training ANN with PSO X = np.copy(particles_fb) V = np.copy(V_fb) informants_new = informants_fb.copy() t0 = time.time() for i in range(iterations): for j in range(10): V[j] = w1[i] * V[j] + c1 * np.random.rand(d_fb) * (pbest[j] - X[j])+ c2 * np.random.rand(d_fb) * (lbest[j] - X[j])+ c3 * np.random.rand(d_fb) * (gbest - X[j]) X[j] = X[j] + V[j] A2 = forward_propagation_one(X_data, X[j], n_x, n_h, n_y) costs[j] = compute_cost(A2, Y) # print(costs) #Update pbest &amp; gbest if costs[j] &lt; pbest_cost[j]: pbest[j] = X[j] pbest_cost[j] = costs[j] if costs[j] &lt; gbest_cost: gbest = X[j] gbest_cost = costs[j] # print(gbest_cost) #update informants lcosts = np.zeros((n_informants,1)) for i, p in enumerate(informants_new): informants_new[p] = update_informants(informants_indices_fb[p] , X) local_informants = informants_new[p] #Calculate lcosts for i, l in enumerate(local_informants_fb): fb_output = forward_propagation_one(X_data, l, n_x, n_h, n_y) lcosts[i] = compute_cost(fb_output, Y) #Update lbest if lcosts.min() &lt; lbest_cost[i]: lbest[i] = local_informants[lcosts.argmin(), :] lbest_cost[i] = lcosts.min() # print(gbest_cost) overall_best_cost[i] = gbest_cost t1 = time.time() print(&quot;*&quot;*60) print(&quot;Elapsed time for the training is: &quot;,t1-t0) . &lt;ipython-input-28-f92a4d38fec1&gt;:5: RuntimeWarning: divide by zero encountered in log cost = -(1/m) * np.sum(Y * np.log(A) + (1-Y) * np.log(1-A)) &lt;ipython-input-28-f92a4d38fec1&gt;:5: RuntimeWarning: invalid value encountered in multiply cost = -(1/m) * np.sum(Y * np.log(A) + (1-Y) * np.log(1-A)) &lt;ipython-input-26-d2ade04057a6&gt;:2: RuntimeWarning: overflow encountered in exp return 1/(1+np.exp(-Z)) . ************************************************************ Elapsed time for the training is: 17.68942403793335 . compute_accuracy(X_data, Y, gbest, n_x, n_h, n_y) . &lt;ipython-input-26-d2ade04057a6&gt;:2: RuntimeWarning: overflow encountered in exp return 1/(1+np.exp(-Z)) . 0.9110787172011662 . gbest_cost . array([0.39647052]) . Trying with different hyperparameters . # Training ANN with PSO X = np.copy(particles_fb) V = np.copy(V_fb) informants_new = informants_fb.copy() c4 = 0.7 c5 = 0.5 c6 = 0.8 t0 = time.time() for i in range(iterations): for j in range(10): V[j] = w1[i] * V[j] + c4 * np.random.rand(d_fb) * (pbest[j] - X[j])+ c5 * np.random.rand(d_fb) * (lbest[j] - X[j])+ c6 * np.random.rand(d_fb) * (gbest - X[j]) X[j] = X[j] + V[j] A2 = forward_propagation_one(X_data, X[j], n_x, n_h, n_y) costs[j] = compute_cost(A2, Y) # print(costs) #Update pbest &amp; gbest if costs[j] &lt; pbest_cost[j]: pbest[j] = X[j] pbest_cost[j] = costs[j] if costs[j] &lt; gbest_cost: gbest = X[j] gbest_cost = costs[j] # print(gbest_cost) #update informants lcosts = np.zeros((n_informants,1)) for i, p in enumerate(informants_new): informants_new[p] = update_informants(informants_indices_fb[p] , X) local_informants = informants_new[p] #Calculate lcosts for i, l in enumerate(local_informants_fb): fb_output = forward_propagation_one(X_data, l, n_x, n_h, n_y) lcosts[i] = compute_cost(fb_output, Y) #Update lbest if lcosts.min() &lt; lbest_cost[i]: lbest[i] = local_informants[lcosts.argmin(), :] lbest_cost[i] = lcosts.min() # print(gbest_cost) overall_best_cost[i] = gbest_cost t1 = time.time() print(&quot;*&quot;*60) print(&quot;Elapsed time for the training is: &quot;,t1-t0) . &lt;ipython-input-26-d2ade04057a6&gt;:2: RuntimeWarning: overflow encountered in exp return 1/(1+np.exp(-Z)) &lt;ipython-input-28-f92a4d38fec1&gt;:5: RuntimeWarning: divide by zero encountered in log cost = -(1/m) * np.sum(Y * np.log(A) + (1-Y) * np.log(1-A)) &lt;ipython-input-28-f92a4d38fec1&gt;:5: RuntimeWarning: invalid value encountered in multiply cost = -(1/m) * np.sum(Y * np.log(A) + (1-Y) * np.log(1-A)) . ************************************************************ Elapsed time for the training is: 16.136921405792236 . compute_accuracy(X_data, Y, gbest, n_x, n_h, n_y) . 0.8666180758017493 . gbest_cost . array([0.41470815]) . Using MSE as a cost function . d_fb = int(n_x*n_h + n_h*n_y + n_h + n_y) #No of dimensions for forward propagation p = int(10) particles_fb = np.random.uniform(size = (10, 31)) V_fb = np.random.uniform(size = (10, 31)) costs = np.zeros((p,1)) for i, particle in enumerate(particles_fb): fb_output = forward_propagation_one(X_data, particle, n_x, n_h, n_y) costs[i] = compute_cost_2(fb_output, Y) pbest = np.copy(particles_fb) pbest_cost = np.copy(costs) gbest = pbest[pbest_cost.argmin(), :] gbest_cost = pbest_cost.min() overall_best_cost = np.zeros((iterations, 1)) # print(particles_fb, &quot; n n&quot;) # print(V_fb) . #creating informants for ANN informants_fb = {} for i in range(1, len(particles_fb) +1): informants_fb[&quot;p&quot;+str(i)] = particles_fb[np.random.choice(particles_fb.shape[0], n_informants, replace=False)] informants_indices_fb = {} for i in range(1, len(particles_fb) +1): informants_indices_fb[&quot;p&quot;+str(i)] = np.random.choice(particles_fb.shape[0], n_informants, replace=False) #Choosing lbest for each particle lbest = [] lbest_cost = [] for p in informants_fb: local_informants_fb = informants_fb[p] l_costs = np.zeros((3,1)) for i, l in enumerate(local_informants_fb): fb_output = forward_propagation_one(X_data, l, n_x, n_h, n_y) l_costs[i] = compute_cost_2(fb_output, Y) lbest.append(local_informants_fb[l_costs.argmin(), :]) lbest_cost.append(l_costs.min()) lbest = np.array(lbest) lbest_cost = np.array(lbest_cost) . # Training ANN with PSO X = np.copy(particles_fb) V = np.copy(V_fb) informants_new = informants_fb.copy() t0 = time.time() for i in range(iterations): for j in range(10): V[j] = w1[i] * V[j] + c1 * np.random.rand(d_fb) * (pbest[j] - X[j])+ c2 * np.random.rand(d_fb) * (lbest[j] - X[j])+ c3 * np.random.rand(d_fb) * (gbest - X[j]) X[j] = X[j] + V[j] A2 = forward_propagation_one(X_data, X[j], n_x, n_h, n_y) costs[j] = compute_cost_2(A2, Y) # print(costs) #Update pbest &amp; gbest if costs[j] &lt; pbest_cost[j]: pbest[j] = X[j] pbest_cost[j] = costs[j] if costs[j] &lt; gbest_cost: gbest = X[j] gbest_cost = costs[j] # print(gbest_cost) #update informants lcosts = np.zeros((n_informants,1)) for i, p in enumerate(informants_new): informants_new[p] = update_informants(informants_indices_fb[p] , X) local_informants = informants_new[p] #Calculate lcosts for i, l in enumerate(local_informants_fb): fb_output = forward_propagation_one(X_data, l, n_x, n_h, n_y) lcosts[i] = compute_cost_2(fb_output, Y) #Update lbest if lcosts.min() &lt; lbest_cost[i]: lbest[i] = local_informants[lcosts.argmin(), :] lbest_cost[i] = lcosts.min() # print(gbest_cost) overall_best_cost[i] = gbest_cost t1 = time.time() print(&quot;*&quot;*60) print(&quot;Elapsed time for the training is: &quot;,t1-t0) . &lt;ipython-input-26-d2ade04057a6&gt;:2: RuntimeWarning: overflow encountered in exp return 1/(1+np.exp(-Z)) . ************************************************************ Elapsed time for the training is: 23.069150924682617 . compute_accuracy(X_data, Y, gbest, n_x, n_h, n_y) . &lt;ipython-input-26-d2ade04057a6&gt;:2: RuntimeWarning: overflow encountered in exp return 1/(1+np.exp(-Z)) . 0.7842565597667639 . gbest_cost . array([0.21574344]) . Trying with different hyperparameters . # Training ANN with PSO X = np.copy(particles_fb) V = np.copy(V_fb) informants_new = informants_fb.copy() c4 = 0.7 c5 = 0.5 c6 = 0.8 t0 = time.time() for i in range(iterations): for j in range(10): V[j] = w1[i] * V[j] + c4 * np.random.rand(d_fb) * (pbest[j] - X[j])+ c5 * np.random.rand(d_fb) * (lbest[j] - X[j])+ c6 * np.random.rand(d_fb) * (gbest - X[j]) X[j] = X[j] + V[j] A2 = forward_propagation_one(X_data, X[j], n_x, n_h, n_y) costs[j] = compute_cost_2(A2, Y) # print(costs) #Update pbest &amp; gbest if costs[j] &lt; pbest_cost[j]: pbest[j] = X[j] pbest_cost[j] = costs[j] if costs[j] &lt; gbest_cost: gbest = X[j] gbest_cost = costs[j] # print(gbest_cost) #update informants lcosts = np.zeros((n_informants,1)) for i, p in enumerate(informants_new): informants_new[p] = update_informants(informants_indices_fb[p] , X) local_informants = informants_new[p] #Calculate lcosts for i, l in enumerate(local_informants_fb): fb_output = forward_propagation_one(X_data, l, n_x, n_h, n_y) lcosts[i] = compute_cost_2(fb_output, Y) #Update lbest if lcosts.min() &lt; lbest_cost[i]: lbest[i] = local_informants[lcosts.argmin(), :] lbest_cost[i] = lcosts.min() # print(gbest_cost) overall_best_cost[i] = gbest_cost t1 = time.time() print(&quot;*&quot;*60) print(&quot;Elapsed time for the training is: &quot;,t1-t0) . &lt;ipython-input-26-d2ade04057a6&gt;:2: RuntimeWarning: overflow encountered in exp return 1/(1+np.exp(-Z)) . ************************************************************ Elapsed time for the training is: 23.40227246284485 . compute_accuracy(X_data, Y, gbest, n_x, n_h, n_y) . &lt;ipython-input-26-d2ade04057a6&gt;:2: RuntimeWarning: overflow encountered in exp return 1/(1+np.exp(-Z)) . 0.9591836734693877 . gbest_cost . array([0.04081633]) .",
            "url": "https://moraouf.github.io/MoSpace/project/2021/12/05/ANN-PSO.html",
            "relUrl": "/project/2021/12/05/ANN-PSO.html",
            "date": " • Dec 5, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Holiday Package Recommendations using different ML algorithms",
            "content": "Intro . As a travel-holic person, I thought to choose a tourism-related dataset where I can apply different ML algorithms on it. The chosen dataset is for a travel company that collected different details about its customers, &amp; it wants to predict whether their new customers will buy their advertised packages or no. . # import libraries import numpy as np import pandas as pd import matplotlib.pyplot as plt %matplotlib inline import seaborn as sns . #read in the dataset data = pd.read_csv(&quot;Holiday Package Prediction.csv&quot;) . data.head() . CustomerID ProdTaken Age TypeofContact CityTier DurationOfPitch Occupation Gender NumberOfPersonVisiting NumberOfFollowups ProductPitched PreferredPropertyStar MaritalStatus NumberOfTrips Passport PitchSatisfactionScore OwnCar NumberOfChildrenVisiting Designation MonthlyIncome . 0 200000 | 1 | 41.0 | Self Enquiry | 3 | 6.0 | Salaried | Female | 3 | 3.0 | Deluxe | 3.0 | Single | 1.0 | 1 | 2 | 1 | 0.0 | Manager | 20993.0 | . 1 200001 | 0 | 49.0 | Company Invited | 1 | 14.0 | Salaried | Male | 3 | 4.0 | Deluxe | 4.0 | Divorced | 2.0 | 0 | 3 | 1 | 2.0 | Manager | 20130.0 | . 2 200002 | 1 | 37.0 | Self Enquiry | 1 | 8.0 | Free Lancer | Male | 3 | 4.0 | Basic | 3.0 | Single | 7.0 | 1 | 3 | 0 | 0.0 | Executive | 17090.0 | . 3 200003 | 0 | 33.0 | Company Invited | 1 | 9.0 | Salaried | Female | 2 | 3.0 | Basic | 3.0 | Divorced | 2.0 | 1 | 5 | 1 | 1.0 | Executive | 17909.0 | . 4 200004 | 0 | NaN | Self Enquiry | 1 | 8.0 | Small Business | Male | 2 | 3.0 | Basic | 4.0 | Divorced | 1.0 | 0 | 5 | 1 | 0.0 | Executive | 18468.0 | . data.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 4888 entries, 0 to 4887 Data columns (total 20 columns): # Column Non-Null Count Dtype -- -- 0 CustomerID 4888 non-null int64 1 ProdTaken 4888 non-null int64 2 Age 4662 non-null float64 3 TypeofContact 4863 non-null object 4 CityTier 4888 non-null int64 5 DurationOfPitch 4637 non-null float64 6 Occupation 4888 non-null object 7 Gender 4888 non-null object 8 NumberOfPersonVisiting 4888 non-null int64 9 NumberOfFollowups 4843 non-null float64 10 ProductPitched 4888 non-null object 11 PreferredPropertyStar 4862 non-null float64 12 MaritalStatus 4888 non-null object 13 NumberOfTrips 4748 non-null float64 14 Passport 4888 non-null int64 15 PitchSatisfactionScore 4888 non-null int64 16 OwnCar 4888 non-null int64 17 NumberOfChildrenVisiting 4822 non-null float64 18 Designation 4888 non-null object 19 MonthlyIncome 4655 non-null float64 dtypes: float64(7), int64(7), object(6) memory usage: 763.9+ KB . As we can see, we have couple of columns which have missing values &amp; need to be processed . print(&quot;Unique Values of columns: n&quot;) for col in data.columns: print(&quot;{}: {}&quot;.format(col, data[col].unique())) . Unique Values of columns: CustomerID: [200000 200001 200002 ... 204885 204886 204887] ProdTaken: [1 0] Age: [41. 49. 37. 33. nan 32. 59. 30. 38. 36. 35. 31. 34. 28. 29. 22. 53. 21. 42. 44. 46. 39. 24. 43. 50. 27. 26. 48. 55. 45. 56. 23. 51. 40. 54. 58. 20. 25. 19. 57. 52. 47. 18. 60. 61.] TypeofContact: [&#39;Self Enquiry&#39; &#39;Company Invited&#39; nan] CityTier: [3 1 2] DurationOfPitch: [ 6. 14. 8. 9. 30. 29. 33. 22. 21. 32. 25. 27. 11. 17. 15. 13. 12. 16. 10. 31. 18. nan 24. 35. 28. 20. 26. 34. 23. 5. 19. 126. 7. 36. 127.] Occupation: [&#39;Salaried&#39; &#39;Free Lancer&#39; &#39;Small Business&#39; &#39;Large Business&#39;] Gender: [&#39;Female&#39; &#39;Male&#39; &#39;Fe Male&#39;] NumberOfPersonVisiting: [3 2 1 4 5] NumberOfFollowups: [ 3. 4. 2. 5. nan 1. 6.] ProductPitched: [&#39;Deluxe&#39; &#39;Basic&#39; &#39;Standard&#39; &#39;Super Deluxe&#39; &#39;King&#39;] PreferredPropertyStar: [ 3. 4. 5. nan] MaritalStatus: [&#39;Single&#39; &#39;Divorced&#39; &#39;Married&#39; &#39;Unmarried&#39;] NumberOfTrips: [ 1. 2. 7. 5. 6. 3. 4. 19. 21. 8. nan 20. 22.] Passport: [1 0] PitchSatisfactionScore: [2 3 5 4 1] OwnCar: [1 0] NumberOfChildrenVisiting: [ 0. 2. 1. nan 3.] Designation: [&#39;Manager&#39; &#39;Executive&#39; &#39;Senior Manager&#39; &#39;AVP&#39; &#39;VP&#39;] MonthlyIncome: [20993. 20130. 17090. ... 22097. 22995. 21471.] . data.describe() . CustomerID ProdTaken Age CityTier DurationOfPitch NumberOfPersonVisiting NumberOfFollowups PreferredPropertyStar NumberOfTrips Passport PitchSatisfactionScore OwnCar NumberOfChildrenVisiting MonthlyIncome . count 4888.000000 | 4888.000000 | 4662.000000 | 4888.000000 | 4637.000000 | 4888.000000 | 4843.000000 | 4862.000000 | 4748.000000 | 4888.000000 | 4888.000000 | 4888.000000 | 4822.000000 | 4655.000000 | . mean 202443.500000 | 0.188216 | 37.622265 | 1.654255 | 15.490835 | 2.905074 | 3.708445 | 3.581037 | 3.236521 | 0.290917 | 3.078151 | 0.620295 | 1.187267 | 23619.853491 | . std 1411.188388 | 0.390925 | 9.316387 | 0.916583 | 8.519643 | 0.724891 | 1.002509 | 0.798009 | 1.849019 | 0.454232 | 1.365792 | 0.485363 | 0.857861 | 5380.698361 | . min 200000.000000 | 0.000000 | 18.000000 | 1.000000 | 5.000000 | 1.000000 | 1.000000 | 3.000000 | 1.000000 | 0.000000 | 1.000000 | 0.000000 | 0.000000 | 1000.000000 | . 25% 201221.750000 | 0.000000 | 31.000000 | 1.000000 | 9.000000 | 2.000000 | 3.000000 | 3.000000 | 2.000000 | 0.000000 | 2.000000 | 0.000000 | 1.000000 | 20346.000000 | . 50% 202443.500000 | 0.000000 | 36.000000 | 1.000000 | 13.000000 | 3.000000 | 4.000000 | 3.000000 | 3.000000 | 0.000000 | 3.000000 | 1.000000 | 1.000000 | 22347.000000 | . 75% 203665.250000 | 0.000000 | 44.000000 | 3.000000 | 20.000000 | 3.000000 | 4.000000 | 4.000000 | 4.000000 | 1.000000 | 4.000000 | 1.000000 | 2.000000 | 25571.000000 | . max 204887.000000 | 1.000000 | 61.000000 | 3.000000 | 127.000000 | 5.000000 | 6.000000 | 5.000000 | 22.000000 | 1.000000 | 5.000000 | 1.000000 | 3.000000 | 98678.000000 | . From the chart above, we can clearly see that the numerical features have different ranges, hence scaling should be considered. . data.describe(include = &quot;object&quot;) . TypeofContact Occupation Gender ProductPitched MaritalStatus Designation . count 4863 | 4888 | 4888 | 4888 | 4888 | 4888 | . unique 2 | 4 | 3 | 5 | 4 | 5 | . top Self Enquiry | Salaried | Male | Basic | Married | Executive | . freq 3444 | 2368 | 2916 | 1842 | 2340 | 1842 | . travel = data.copy() . numeric_cols = [col for col in travel.columns if data[col].dtype in [np.int64, np.float64]] print(numeric_cols) . [&#39;CustomerID&#39;, &#39;ProdTaken&#39;, &#39;Age&#39;, &#39;CityTier&#39;, &#39;DurationOfPitch&#39;, &#39;NumberOfPersonVisiting&#39;, &#39;NumberOfFollowups&#39;, &#39;PreferredPropertyStar&#39;, &#39;NumberOfTrips&#39;, &#39;Passport&#39;, &#39;PitchSatisfactionScore&#39;, &#39;OwnCar&#39;, &#39;NumberOfChildrenVisiting&#39;, &#39;MonthlyIncome&#39;] . object_cols = [col for col in travel.columns if data[col].dtype == object] print(object_cols) . [&#39;TypeofContact&#39;, &#39;Occupation&#39;, &#39;Gender&#39;, &#39;ProductPitched&#39;, &#39;MaritalStatus&#39;, &#39;Designation&#39;] . Data Preprocessing &amp; Visualization . travel[numeric_cols].hist(figsize=(20,15)) plt.show() . plt.figure(figsize=(16,10)) for i, col in zip(range(1, 7), object_cols): plt.subplot(2, 3, i) travel[col].value_counts().plot(kind = &quot;bar&quot;) plt.title(str(col)) plt.xticks(rotation = 15) plt.show() . travel.loc[data[&quot;Occupation&quot;] == &quot;Free Lancer&quot;, [&quot;Occupation&quot;]] = &quot;Salaried&quot; . travel[&quot;Gender&quot;].value_counts() . Male 2916 Female 1817 Fe Male 155 Name: Gender, dtype: int64 . travel.loc[data[&quot;Gender&quot;] == &quot;Fe Male&quot;, [&quot;Gender&quot;]] = &quot;Female&quot; . travel[&quot;Gender&quot;].value_counts() . Male 2916 Female 1972 Name: Gender, dtype: int64 . travel[&quot;MaritalStatus&quot;].value_counts() . Married 2340 Divorced 950 Single 916 Unmarried 682 Name: MaritalStatus, dtype: int64 . travel.loc[data[&quot;MaritalStatus&quot;] == &quot;Unmarried&quot;, [&quot;MaritalStatus&quot;]] = &quot;Single&quot; . travel[&quot;MaritalStatus&quot;].value_counts() . Married 2340 Single 1598 Divorced 950 Name: MaritalStatus, dtype: int64 . Dealing with Outliers . plt.figure(figsize = (15,5)) for i, col in zip(range(1,4),[&quot;DurationOfPitch&quot;, &quot;NumberOfTrips&quot;, &quot;MonthlyIncome&quot;]): plt.subplot(1,3, i) travel.boxplot(column = col) plt.show() . travel.loc[travel[&quot;MonthlyIncome&quot;] &lt; 5000, &quot;MonthlyIncome&quot;] . 142 1000.0 2586 4678.0 Name: MonthlyIncome, dtype: float64 . # After finding outliers indices, we drop them travel = travel.drop(travel.index[[38, 2482, 385, 816, 2829, 3260, 142, 2586, 1434,3878]]) . Features Correlation . travel[numeric_cols].corr()[&quot;ProdTaken&quot;] . CustomerID 0.056792 ProdTaken 1.000000 Age -0.146591 CityTier 0.087495 DurationOfPitch 0.084515 NumberOfPersonVisiting 0.009953 NumberOfFollowups 0.112551 PreferredPropertyStar 0.100368 NumberOfTrips 0.013816 Passport 0.260593 PitchSatisfactionScore 0.050603 OwnCar -0.011985 NumberOfChildrenVisiting 0.006712 MonthlyIncome -0.134025 Name: ProdTaken, dtype: float64 . plt.figure(figsize=(10,7)) corr = travel.corr() # corr = corr.iloc[1:, :-1] mask = np.triu(np.ones_like(corr), k=1) sns.heatmap(corr, xticklabels=corr.columns.values, yticklabels=corr.columns.values, mask = mask, annot = True) . &lt;AxesSubplot:&gt; . Dealing with Missing Data . travel.isnull().sum() . CustomerID 0 ProdTaken 0 Age 225 TypeofContact 25 CityTier 0 DurationOfPitch 251 Occupation 0 Gender 0 NumberOfPersonVisiting 0 NumberOfFollowups 45 ProductPitched 0 PreferredPropertyStar 25 MaritalStatus 0 NumberOfTrips 140 Passport 0 PitchSatisfactionScore 0 OwnCar 0 NumberOfChildrenVisiting 66 Designation 0 MonthlyIncome 233 dtype: int64 . cols_with_missing = [&quot;Age&quot;, &quot;TypeofContact&quot;, &quot;DurationOfPitch&quot;, &quot;NumberOfFollowups&quot;, &quot;PreferredPropertyStar&quot;, &quot;NumberOfTrips&quot;, &quot;NumberOfChildrenVisiting&quot;, &quot;MonthlyIncome&quot;] travel[cols_with_missing].describe(include = &quot;all&quot;) . Age TypeofContact DurationOfPitch NumberOfFollowups PreferredPropertyStar NumberOfTrips NumberOfChildrenVisiting MonthlyIncome . count 4653.000000 | 4853 | 4627.000000 | 4833.000000 | 4853.000000 | 4738.000000 | 4812.000000 | 4645.000000 | . unique NaN | 2 | NaN | NaN | NaN | NaN | NaN | NaN | . top NaN | Self Enquiry | NaN | NaN | NaN | NaN | NaN | NaN | . freq NaN | 3440 | NaN | NaN | NaN | NaN | NaN | NaN | . mean 37.621320 | NaN | 15.449319 | 3.708463 | 3.581290 | 3.222035 | 1.187864 | 23600.630786 | . std 9.321348 | NaN | 8.208413 | 1.003123 | 0.798182 | 1.780694 | 0.858059 | 5147.662625 | . min 18.000000 | NaN | 5.000000 | 1.000000 | 3.000000 | 1.000000 | 0.000000 | 16009.000000 | . 25% 31.000000 | NaN | 9.000000 | 3.000000 | 3.000000 | 2.000000 | 1.000000 | 20346.000000 | . 50% 36.000000 | NaN | 13.000000 | 4.000000 | 3.000000 | 3.000000 | 1.000000 | 22351.000000 | . 75% 44.000000 | NaN | 20.000000 | 4.000000 | 4.000000 | 4.000000 | 2.000000 | 25571.000000 | . max 61.000000 | NaN | 36.000000 | 6.000000 | 5.000000 | 8.000000 | 3.000000 | 38677.000000 | . travel.loc[travel[&quot;TypeofContact&quot;].isnull(), [&quot;TypeofContact&quot;]] = &quot;Self Enquiry&quot; . travel[&quot;TypeofContact&quot;].value_counts(dropna = False) . Self Enquiry 3465 Company Invited 1413 Name: TypeofContact, dtype: int64 . Imputing missing values of numerical features will be done after train-test-split of the dataset to avoid any leakage from the test data into the train data | . . Classification . . Model Building . y = travel[&quot;ProdTaken&quot;] X = travel.drop([&quot;CustomerID&quot;, &quot;ProdTaken&quot;], axis =1) print(&quot;The target class is `{}`: n{}&quot;.format(&quot;ProdTaken&quot;, y.unique())) print(&quot; nThe features are : n n{}&quot;.format(X.columns)) . The target class is `ProdTaken`: [1 0] The features are : Index([&#39;Age&#39;, &#39;TypeofContact&#39;, &#39;CityTier&#39;, &#39;DurationOfPitch&#39;, &#39;Occupation&#39;, &#39;Gender&#39;, &#39;NumberOfPersonVisiting&#39;, &#39;NumberOfFollowups&#39;, &#39;ProductPitched&#39;, &#39;PreferredPropertyStar&#39;, &#39;MaritalStatus&#39;, &#39;NumberOfTrips&#39;, &#39;Passport&#39;, &#39;PitchSatisfactionScore&#39;, &#39;OwnCar&#39;, &#39;NumberOfChildrenVisiting&#39;, &#39;Designation&#39;, &#39;MonthlyIncome&#39;], dtype=&#39;object&#39;) . from sklearn.model_selection import train_test_split from sklearn.impute import SimpleImputer from sklearn.preprocessing import OrdinalEncoder from sklearn.preprocessing import OneHotEncoder . Split the data &amp; consider stratification . X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1,stratify = y) print(&quot;Check proper stratification applied: n n&quot;) print(&quot;Proportion of customers who took the product: &quot;,len(y[y == 1]) / len(y)) print(&quot;Proportion of train set where customers took the product: &quot;,len(X_train[y == 1]) / len(X_train)) print(&quot;Proportion of test set where customers took the product: &quot;,len(X_test[y == 1]) / len(X_test)) . Check proper stratification applied: Proportion of customers who took the product: 0.1881918819188192 Proportion of train set where customers took the product: 0.18810866222450026 Proportion of test set where customers took the product: 0.1885245901639344 . &lt;ipython-input-31-5eeae4a3ab14&gt;:4: UserWarning: Boolean Series key will be reindexed to match DataFrame index. print(&#34;Proportion of train set where customers took the product: &#34;,len(X_train[y == 1]) / len(X_train)) &lt;ipython-input-31-5eeae4a3ab14&gt;:5: UserWarning: Boolean Series key will be reindexed to match DataFrame index. print(&#34;Proportion of test set where customers took the product: &#34;,len(X_test[y == 1]) / len(X_test)) . numeric_cols_with_missing = cols_with_missing.copy() numeric_cols_with_missing.remove(&quot;TypeofContact&quot;) . numeric_cols_with_missing . [&#39;Age&#39;, &#39;DurationOfPitch&#39;, &#39;NumberOfFollowups&#39;, &#39;PreferredPropertyStar&#39;, &#39;NumberOfTrips&#39;, &#39;NumberOfChildrenVisiting&#39;, &#39;MonthlyIncome&#39;] . Numeric Imputation &amp; filling missing data . # impute X_train imputer = SimpleImputer(strategy = &quot;median&quot;) imputed_X_train = pd.DataFrame(imputer.fit_transform(X_train[numeric_cols_with_missing])) imputed_X_train.columns = numeric_cols_with_missing imputed_X_train.index = X_train.index imputed_X_train_cols = imputed_X_train.columns print(imputed_X_train.isnull().any()) print(&quot; n&quot;,imputed_X_train_cols) . Age False DurationOfPitch False NumberOfFollowups False PreferredPropertyStar False NumberOfTrips False NumberOfChildrenVisiting False MonthlyIncome False dtype: bool Index([&#39;Age&#39;, &#39;DurationOfPitch&#39;, &#39;NumberOfFollowups&#39;, &#39;PreferredPropertyStar&#39;, &#39;NumberOfTrips&#39;, &#39;NumberOfChildrenVisiting&#39;, &#39;MonthlyIncome&#39;], dtype=&#39;object&#39;) . # impute X_test imputed_X_test = pd.DataFrame(imputer.transform(X_test[numeric_cols_with_missing])) imputed_X_test.columns = numeric_cols_with_missing imputed_X_test.index = X_test.index . imputed_X_test.isnull().any() . Age False DurationOfPitch False NumberOfFollowups False PreferredPropertyStar False NumberOfTrips False NumberOfChildrenVisiting False MonthlyIncome False dtype: bool . unimputed_X_train_cols = [col for col in numeric_cols if col not in imputed_X_train_cols] unimputed_X_train_cols.remove(&quot;CustomerID&quot;) unimputed_X_train_cols.remove(&quot;ProdTaken&quot;) unimputed_X_train = X_train[unimputed_X_train_cols] unimputed_X_test = X_test[unimputed_X_train_cols] . unimputed_X_train.isnull().any() . CityTier False NumberOfPersonVisiting False Passport False PitchSatisfactionScore False OwnCar False dtype: bool . unimputed_X_test.isnull().any() . CityTier False NumberOfPersonVisiting False Passport False PitchSatisfactionScore False OwnCar False dtype: bool . middle_X_train = pd.concat([imputed_X_train, unimputed_X_train], axis = 1) middle_X_test = pd.concat([imputed_X_test, unimputed_X_test], axis = 1) . middle_X_train.isnull().any() . Age False DurationOfPitch False NumberOfFollowups False PreferredPropertyStar False NumberOfTrips False NumberOfChildrenVisiting False MonthlyIncome False CityTier False NumberOfPersonVisiting False Passport False PitchSatisfactionScore False OwnCar False dtype: bool . Ordinal Encoder &amp; One-Hot Encoder . ord_cat = [&quot;Occupation&quot;, &quot;ProductPitched&quot;, &quot;Designation&quot;] oh_cat = [&quot;TypeofContact&quot;, &quot;Gender&quot;, &quot;MaritalStatus&quot;] labelled_X_train = X_train.copy() labelled_X_test = X_test.copy() . ord_encoder = OrdinalEncoder() labelled_X_train[ord_cat] = ord_encoder.fit_transform(X_train[ord_cat]) labelled_X_test[ord_cat] = ord_encoder.transform(X_test[ord_cat]) . oh_encoder = OneHotEncoder(handle_unknown=&#39;ignore&#39;, sparse=False) . oh_X_train = pd.DataFrame(oh_encoder.fit_transform(X_train[oh_cat])) oh_X_test = pd.DataFrame(oh_encoder.transform(X_test[oh_cat])) oh_X_train.index = X_train.index oh_X_test.index = X_test.index . final_X_train = pd.concat([middle_X_train, labelled_X_train[ord_cat], oh_X_train], axis = 1) final_X_test = pd.concat([middle_X_test, labelled_X_test[ord_cat], oh_X_test], axis = 1) . final_X_train . Age DurationOfPitch NumberOfFollowups PreferredPropertyStar NumberOfTrips NumberOfChildrenVisiting MonthlyIncome CityTier NumberOfPersonVisiting Passport ... Occupation ProductPitched Designation 0 1 2 3 4 5 6 . 781 36.0 | 6.0 | 4.0 | 5.0 | 2.0 | 1.0 | 22338.0 | 1 | 2 | 0 | ... | 2.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 0.0 | 0.0 | . 2039 36.0 | 14.0 | 4.0 | 5.0 | 2.0 | 0.0 | 22587.0 | 3 | 3 | 0 | ... | 1.0 | 3.0 | 3.0 | 1.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 1.0 | . 1944 24.0 | 29.0 | 1.0 | 3.0 | 2.0 | 0.0 | 17725.0 | 1 | 2 | 1 | ... | 2.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | . 3099 34.0 | 10.0 | 4.0 | 4.0 | 5.0 | 2.0 | 20955.0 | 2 | 3 | 1 | ... | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | . 346 41.0 | 12.0 | 4.0 | 4.0 | 7.0 | 0.0 | 21032.0 | 1 | 3 | 1 | ... | 2.0 | 1.0 | 2.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 0.0 | 1.0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 2522 38.0 | 14.0 | 4.0 | 3.0 | 6.0 | 1.0 | 32342.0 | 1 | 2 | 0 | ... | 2.0 | 3.0 | 3.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 0.0 | 1.0 | . 931 30.0 | 13.0 | 3.0 | 3.0 | 1.0 | 2.0 | 19695.0 | 1 | 3 | 1 | ... | 2.0 | 1.0 | 2.0 | 1.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 1.0 | . 820 35.0 | 17.0 | 4.0 | 3.0 | 2.0 | 0.0 | 19968.0 | 3 | 2 | 0 | ... | 2.0 | 1.0 | 2.0 | 1.0 | 0.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | . 50 48.0 | 6.0 | 4.0 | 3.0 | 1.0 | 2.0 | 20381.0 | 1 | 3 | 1 | ... | 1.0 | 3.0 | 3.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 0.0 | 1.0 | . 1537 45.0 | 13.0 | 3.0 | 5.0 | 3.0 | 0.0 | 24724.0 | 1 | 2 | 0 | ... | 1.0 | 3.0 | 3.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | . 3902 rows × 22 columns . final_X_train.isnull().any() . Age False DurationOfPitch False NumberOfFollowups False PreferredPropertyStar False NumberOfTrips False NumberOfChildrenVisiting False MonthlyIncome False CityTier False NumberOfPersonVisiting False Passport False PitchSatisfactionScore False OwnCar False Occupation False ProductPitched False Designation False 0 False 1 False 2 False 3 False 4 False 5 False 6 False dtype: bool . Naive Bayes . from sklearn.naive_bayes import GaussianNB from sklearn.naive_bayes import CategoricalNB . gaussian_clf = GaussianNB() gaussian_clf.fit(final_X_train, y_train) pred_labels = gaussian_clf.predict(final_X_test) score = gaussian_clf.score(final_X_test, y_test) print(&#39;Accuracy Score: &#39;, score) . Accuracy Score: 0.8329918032786885 . from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, roc_curve, roc_auc_score print(&quot;Confusion Matrix: n&quot;, confusion_matrix(y_test, pred_labels)) print(&quot; nPrecision Score: &quot;, precision_score(y_test, pred_labels)) print(&quot;Recall Score: &quot;, recall_score(y_test, pred_labels)) print(&quot;F1 Score: &quot;, f1_score(y_test, pred_labels)) . Confusion Matrix: [[728 64] [ 99 85]] Precision Score: 0.5704697986577181 Recall Score: 0.46195652173913043 F1 Score: 0.5105105105105106 . fpr, tpr, thresholds = roc_curve(y_test, pred_labels) . plt.figure(figsize=(8,6)) plt.plot(fpr, tpr, linewidth=2, label=&quot;ROC Curve&quot;) plt.plot([0, 1], [0, 1], &#39;k--&#39;) # dashed diagonal plt.xlim(0,1) plt.ylim(0,1) plt.show() . roc_auc_score(y_test, pred_labels) . 0.6905742204655249 . pred_probas = gaussian_clf.predict_proba(final_X_test) fpr, tpr, thresholds = roc_curve(y_test, pred_probas[:, 1]) . plt.figure(figsize=(8,6)) plt.plot(fpr, tpr, linewidth=2, label=&quot;ROC Curve&quot;) plt.plot([0, 1], [0, 1], &#39;k--&#39;) # dashed diagonal plt.xlim(0,1) plt.ylim(0,1) plt.show() . Cross-Validation . from sklearn.model_selection import cross_val_predict, cross_val_score cv_gaussian_clf = GaussianNB() y_train_scores = cross_val_score(cv_gaussian_clf, final_X_train, y_train, cv=5, scoring = &quot;accuracy&quot;) . print(y_train_scores) print(y_train_scores.mean()) . [0.82586428 0.80921895 0.80769231 0.83974359 0.82179487] 0.8208627991726584 . cat_X_train = pd.concat([labelled_X_train[ord_cat], oh_X_train], axis = 1) cat_X_test = pd.concat([labelled_X_test[ord_cat], oh_X_test], axis = 1) . Mixed Naive Bayes with CategoricalNB &amp; GaussianNB . second_gaussian_clf = GaussianNB() G_clf = second_gaussian_clf.fit(middle_X_train, y_train) # Categorical model for discrete independent variable categorical_clf = CategoricalNB() C_clf = categorical_clf.fit(cat_X_train, y_train) . # Get probability predictions from each model # On training data G_train_probas = second_gaussian_clf.predict_proba(middle_X_train) C_train_probas = categorical_clf.predict_proba(cat_X_train) # On testing data G_test_probas = second_gaussian_clf.predict_proba(middle_X_test) C_test_probas = categorical_clf.predict_proba(cat_X_test) . X_new_train = np.c_[(G_train_probas[:,1], C_train_probas[:,1])] # Train X_new_test = np.c_[(G_test_probas[:,1], C_test_probas[:,1])] # Test . last_gaussian_clf = GaussianNB() last_gaussian_clf.fit(X_new_train, y_train) last_pred_labels = last_gaussian_clf.predict(X_new_test) . new_score = last_gaussian_clf.score(X_new_test, y_test) new_score . 0.8391393442622951 . print(&quot;Confusion Matrix: n&quot;, confusion_matrix(y_test, last_pred_labels)) print(&quot; nPrecision Score: &quot;, precision_score(y_test, last_pred_labels)) print(&quot;Recall Score: &quot;, recall_score(y_test, last_pred_labels)) print(&quot;F1 Score: &quot;, f1_score(y_test, last_pred_labels)) . Confusion Matrix: [[729 63] [ 94 90]] Precision Score: 0.5882352941176471 Recall Score: 0.4891304347826087 F1 Score: 0.5341246290801187 . . Clustering . . X_train_clustering = middle_X_train[[&quot;Age&quot;, &quot;MonthlyIncome&quot;]] X_test_clustering = middle_X_test[[&quot;Age&quot;, &quot;MonthlyIncome&quot;]] . X_train_clustering . Age MonthlyIncome . 781 36.0 | 22338.0 | . 2039 36.0 | 22587.0 | . 1944 24.0 | 17725.0 | . 3099 34.0 | 20955.0 | . 346 41.0 | 21032.0 | . ... ... | ... | . 2522 38.0 | 32342.0 | . 931 30.0 | 19695.0 | . 820 35.0 | 19968.0 | . 50 48.0 | 20381.0 | . 1537 45.0 | 24724.0 | . 3902 rows × 2 columns . X_train_clustering.describe() . Age MonthlyIncome . count 3902.000000 | 3902.000000 | . mean 37.574833 | 23583.390056 | . std 9.158515 | 5106.695893 | . min 18.000000 | 16009.000000 | . 25% 31.000000 | 20514.500000 | . 50% 36.000000 | 22338.000000 | . 75% 43.000000 | 25402.250000 | . max 61.000000 | 38677.000000 | . plt.figure(figsize = (10,8)) plt.scatter(X_train_clustering.iloc[:, 0], X_train_clustering.iloc[:, 1]) plt.xlabel(&quot;Age&quot;) plt.ylabel(&quot;Salary&quot;) plt.show() . We can see the range of the values for each feature is different &amp; this will adversly affect the performance of the KMeans algorithm, so scaling them is necessary. . Features Scaling . from sklearn.preprocessing import MinMaxScaler . scaler = MinMaxScaler() scaled_X_train_clustering = scaler.fit_transform(X_train_clustering) . Model Building . from sklearn.cluster import KMeans from sklearn.metrics import silhouette_samples, silhouette_score . kmeans = KMeans(n_clusters = 3, random_state = 1) kmeans.fit(scaled_X_train_clustering) kmeans.score(scaled_X_train_clustering) . -124.8779525561468 . n_clusters_range = np.arange(5, 15) inertias = [] for n_clusters in n_clusters_range: kmeans = KMeans(n_clusters = n_clusters, random_state = 1) kmeans.fit(scaled_X_train_clustering) inertias.append(kmeans.inertia_) . Elbow Method . plt.figure(figsize = (10,8)) plt.plot(n_clusters_range, inertias) plt.xlabel(&quot;K&quot;, fontsize = 12) plt.ylabel(&quot;Inertia&quot;, fontsize = 12) plt.show() . Silhouettte Score Method . import matplotlib.cm as cm range_n_clusters = [6, 7, 8, 9, 10] for n_clusters in range_n_clusters: fig, (ax1, ax2) = plt.subplots(1, 2) fig.set_size_inches(18, 7) ax1.set_xlim([-1, 1]) ax1.set_ylim([0, len(scaled_X_train_clustering) + (n_clusters + 1) * 10]) clusterer = KMeans(n_clusters=n_clusters, random_state=10) cluster_labels = clusterer.fit_predict(scaled_X_train_clustering) silhouette_avg = silhouette_score(scaled_X_train_clustering, cluster_labels) print(&quot;For n_clusters =&quot;, n_clusters, &quot;The average silhouette_score is :&quot;, silhouette_avg) sample_silhouette_values = silhouette_samples(scaled_X_train_clustering, cluster_labels) y_lower = 10 for i in range(n_clusters): ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i] ith_cluster_silhouette_values.sort() size_cluster_i = ith_cluster_silhouette_values.shape[0] y_upper = y_lower + size_cluster_i color = cm.nipy_spectral(float(i) / n_clusters) ax1.fill_betweenx(np.arange(y_lower, y_upper), 0, ith_cluster_silhouette_values, facecolor=color, edgecolor=color, alpha=0.7) ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i)) y_lower = y_upper + 10 # 10 for the 0 samples ax1.set_title(&quot;The silhouette plot for the various clusters.&quot;) ax1.set_xlabel(&quot;The silhouette coefficient values&quot;) ax1.set_ylabel(&quot;Cluster label&quot;) # The vertical line for average silhouette score of all the values ax1.axvline(x=silhouette_avg, color=&quot;red&quot;, linestyle=&quot;--&quot;) ax1.set_yticks([]) ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1]) # 2nd Plot showing the actual clusters formed colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters) ax2.scatter(scaled_X_train_clustering[:, 0], scaled_X_train_clustering[:, 1], marker=&#39;.&#39;, s=30, lw=0, alpha=0.7, c=colors, edgecolor=&#39;k&#39;) # Labeling the clusters centers = clusterer.cluster_centers_ # Draw white circles at cluster centers ax2.scatter(centers[:, 0], centers[:, 1], marker=&#39;o&#39;, c=&quot;white&quot;, alpha=1, s=200, edgecolor=&#39;k&#39;) for i, c in enumerate(centers): ax2.scatter(c[0], c[1], marker=&#39;$%d$&#39; % i, alpha=1, s=50, edgecolor=&#39;k&#39;) ax2.set_title(&quot;The visualization of the clustered data.&quot;) ax2.set_xlabel(&quot;Feature space for the 1st feature&quot;) ax2.set_ylabel(&quot;Feature space for the 2nd feature&quot;) plt.suptitle((&quot;Silhouette analysis for KMeans clustering on sample data &quot; &quot;with n_clusters = %d&quot; % n_clusters), fontsize=14, fontweight=&#39;bold&#39;) plt.show() . For n_clusters = 6 The average silhouette_score is : 0.3448214634401572 For n_clusters = 7 The average silhouette_score is : 0.3520383009504232 For n_clusters = 8 The average silhouette_score is : 0.35351743531025764 For n_clusters = 9 The average silhouette_score is : 0.3576312842903845 For n_clusters = 10 The average silhouette_score is : 0.3647261439213562 . **The dataset is not suitable for clustring, but we can see that the algorithm can come up with different clusters for the selected features. As there are no clear patterns of clusters, increasing the number of clusters doesnt draw any valuable information.** . **If we compare clustering with classfication on this dataset, we can successfully accomplish a decent result of classifying the customers according to whether they will purchase the product or no, but to find any implicit patterns among customers collected data, the KMeans algorithm doesn perform so well** . scaled_X_test_clustering = scaler.transform(X_test_clustering) . kmeans = KMeans(n_clusters = 6, random_state = 1) kmeans.fit(scaled_X_train_clustering) kmeans.predict(scaled_X_test_clustering)[:10] . array([2, 4, 2, 4, 5, 2, 4, 5, 5, 5]) . Kmodes Clustering for categorical Variables . from kmodes.kmodes import KModes . costs = [] travel_kmodes_clustering = travel.copy() k = range(4,9) for num_clusters in list(k): kmode = KModes(n_clusters=num_clusters, init = &quot;Cao&quot;, n_init = 10, verbose=1) kmode.fit_predict(travel_kmodes_clustering[object_cols]) costs.append(kmode.cost_) plt.plot(k, costs, &#39;bx-&#39;) plt.xlabel(&#39;K&#39;) plt.ylabel(&#39;Cost&#39;) plt.title(&#39;Elbow Method For Optimal k&#39;) plt.show() . Initialization method and algorithm are deterministic. Setting n_init to 1. Init: initializing centroids Init: initializing clusters Starting iterations... Run 1, iteration: 1/100, moves: 812, cost: 8634.0 Initialization method and algorithm are deterministic. Setting n_init to 1. Init: initializing centroids Init: initializing clusters Starting iterations... Run 1, iteration: 1/100, moves: 724, cost: 8196.0 Initialization method and algorithm are deterministic. Setting n_init to 1. Init: initializing centroids Init: initializing clusters Starting iterations... Run 1, iteration: 1/100, moves: 572, cost: 7658.0 Initialization method and algorithm are deterministic. Setting n_init to 1. Init: initializing centroids Init: initializing clusters Starting iterations... Run 1, iteration: 1/100, moves: 572, cost: 6846.0 Initialization method and algorithm are deterministic. Setting n_init to 1. Init: initializing centroids Init: initializing clusters Starting iterations... Run 1, iteration: 1/100, moves: 538, cost: 6720.0 . From the figure above, we can see an elbow at k=7, so we will apply kmodes using this number of clusters . kmode = KModes(n_clusters=7, init = &quot;random&quot;, n_init = 10, verbose=1) clusters = kmode.fit_predict(travel_kmodes_clustering[object_cols]) travel_kmodes_clustering[&quot;clusters&quot;] = clusters clusters . Init: initializing centroids Init: initializing clusters Starting iterations... Run 1, iteration: 1/100, moves: 926, cost: 7031.0 Run 1, iteration: 2/100, moves: 599, cost: 7031.0 Init: initializing centroids Init: initializing clusters Starting iterations... Run 2, iteration: 1/100, moves: 0, cost: 7663.0 Init: initializing centroids Init: initializing clusters Starting iterations... Run 3, iteration: 1/100, moves: 1274, cost: 7253.0 Init: initializing centroids Init: initializing clusters Starting iterations... Run 4, iteration: 1/100, moves: 978, cost: 7527.0 Init: initializing centroids Init: initializing clusters Starting iterations... Run 5, iteration: 1/100, moves: 314, cost: 7817.0 Init: initializing centroids Init: initializing clusters Starting iterations... Run 6, iteration: 1/100, moves: 130, cost: 7711.0 Init: initializing centroids Init: initializing clusters Starting iterations... Run 7, iteration: 1/100, moves: 1194, cost: 8319.0 Run 7, iteration: 2/100, moves: 139, cost: 8319.0 Init: initializing centroids Init: initializing clusters Starting iterations... Run 8, iteration: 1/100, moves: 675, cost: 7645.0 Init: initializing centroids Init: initializing clusters Starting iterations... Run 9, iteration: 1/100, moves: 0, cost: 7661.0 Init: initializing centroids Init: initializing clusters Starting iterations... Run 10, iteration: 1/100, moves: 366, cost: 7059.0 Best run was number 1 . array([3, 1, 2, ..., 0, 4, 2], dtype=uint16) . Visualizing different features counts of k=7 . for col in object_cols: plt.subplots(figsize = (10,5)) sns.countplot(x=&#39;clusters&#39; ,hue=col, data = travel_kmodes_clustering) plt.show() . . Decision Trees . from sklearn.tree import DecisionTreeClassifier,plot_tree . Apply Decision Trees to the X_train dataset without cross-validation . final_X_train.columns . Index([ &#39;Age&#39;, &#39;DurationOfPitch&#39;, &#39;NumberOfFollowups&#39;, &#39;PreferredPropertyStar&#39;, &#39;NumberOfTrips&#39;, &#39;NumberOfChildrenVisiting&#39;, &#39;MonthlyIncome&#39;, &#39;CityTier&#39;, &#39;NumberOfPersonVisiting&#39;, &#39;Passport&#39;, &#39;PitchSatisfactionScore&#39;, &#39;OwnCar&#39;, &#39;Occupation&#39;, &#39;ProductPitched&#39;, &#39;Designation&#39;, 0, 1, 2, 3, 4, 5, 6], dtype=&#39;object&#39;) . final_X_train_dt = final_X_train[[ &#39;Age&#39;, &#39;DurationOfPitch&#39;, &#39;NumberOfFollowups&#39;, &#39;PreferredPropertyStar&#39;, &#39;NumberOfTrips&#39;, &#39;NumberOfChildrenVisiting&#39;, &#39;MonthlyIncome&#39;, &#39;CityTier&#39;, &#39;NumberOfPersonVisiting&#39;, &#39;Passport&#39;, &#39;PitchSatisfactionScore&#39;, &#39;OwnCar&#39;, &#39;Occupation&#39;, &#39;ProductPitched&#39;, &#39;Designation&#39;]] final_X_test_dt = final_X_test[[ &#39;Age&#39;, &#39;DurationOfPitch&#39;, &#39;NumberOfFollowups&#39;, &#39;PreferredPropertyStar&#39;, &#39;NumberOfTrips&#39;, &#39;NumberOfChildrenVisiting&#39;, &#39;MonthlyIncome&#39;, &#39;CityTier&#39;, &#39;NumberOfPersonVisiting&#39;, &#39;Passport&#39;, &#39;PitchSatisfactionScore&#39;, &#39;OwnCar&#39;, &#39;Occupation&#39;, &#39;ProductPitched&#39;, &#39;Designation&#39;]] . Tree of depth = 1 . dt_clf = DecisionTreeClassifier(max_depth=1, random_state=1) dt_clf.fit(final_X_train_dt, y_train) . DecisionTreeClassifier(max_depth=1, random_state=1) . fig = plt.figure(figsize=(8,4)) plot_tree(dt_clf, fontsize = 15,feature_names = final_X_train_dt.columns, class_names = [&quot;Not Taken&quot;, &quot;Taken&quot;], label = &quot;all&quot;, rounded = True ,filled = True) . [Text(223.2, 163.07999999999998, &#39;Passport &lt;= 0.5 ngini = 0.305 nsamples = 3902 nvalue = [3168, 734] nclass = Not Taken&#39;), Text(111.6, 54.360000000000014, &#39;gini = 0.22 nsamples = 2800 nvalue = [2447, 353] nclass = Not Taken&#39;), Text(334.79999999999995, 54.360000000000014, &#39;gini = 0.452 nsamples = 1102 nvalue = [721, 381] nclass = Not Taken&#39;)] . dt_clf.predict_proba(final_X_test_dt) . array([[0.65426497, 0.34573503], [0.87392857, 0.12607143], [0.87392857, 0.12607143], ..., [0.87392857, 0.12607143], [0.87392857, 0.12607143], [0.87392857, 0.12607143]]) . dt_clf.predict(final_X_test_dt) . array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int64) . score = dt_clf.score(final_X_test_dt, y_test) score . 0.8114754098360656 . Tree of depth = 2 . dt_clf_2 = DecisionTreeClassifier(max_depth=2, random_state=1) dt_clf_2.fit(final_X_train_dt, y_train) . DecisionTreeClassifier(max_depth=2, random_state=1) . fig = plt.figure(figsize=(15,6)) plot_tree(dt_clf_2, fontsize = 15,feature_names = final_X_train_dt.columns, class_names = [&quot;Not Taken&quot;, &quot;Taken&quot;], label = &quot;all&quot;, rounded = True ,filled = True) . [Text(418.5, 271.8, &#39;Passport &lt;= 0.5 ngini = 0.305 nsamples = 3902 nvalue = [3168, 734] nclass = Not Taken&#39;), Text(209.25, 163.08, &#39;Age &lt;= 21.5 ngini = 0.22 nsamples = 2800 nvalue = [2447, 353] nclass = Not Taken&#39;), Text(104.625, 54.360000000000014, &#39;gini = 0.499 nsamples = 71 nvalue = [37, 34] nclass = Not Taken&#39;), Text(313.875, 54.360000000000014, &#39;gini = 0.206 nsamples = 2729 nvalue = [2410, 319] nclass = Not Taken&#39;), Text(627.75, 163.08, &#39;ProductPitched &lt;= 0.5 ngini = 0.452 nsamples = 1102 nvalue = [721, 381] nclass = Not Taken&#39;), Text(523.125, 54.360000000000014, &#39;gini = 0.494 nsamples = 440 nvalue = [195, 245] nclass = Taken&#39;), Text(732.375, 54.360000000000014, &#39;gini = 0.326 nsamples = 662 nvalue = [526, 136] nclass = Not Taken&#39;)] . score = dt_clf_2.score(final_X_test_dt, y_test) score . 0.8381147540983607 . Tree of depth = 3 . dt_clf_3 = DecisionTreeClassifier(max_depth=3, random_state=1) dt_clf_3.fit(final_X_train_dt, y_train) . DecisionTreeClassifier(max_depth=3, random_state=1) . fig = plt.figure(figsize=(25,10)) plot_tree(dt_clf_3, fontsize = 15,feature_names = final_X_train_dt.columns, class_names = [&quot;Not Taken&quot;, &quot;Taken&quot;], label = &quot;all&quot;, rounded = True ,filled = True) . [Text(697.5, 475.65000000000003, &#39;Passport &lt;= 0.5 ngini = 0.305 nsamples = 3902 nvalue = [3168, 734] nclass = Not Taken&#39;), Text(348.75, 339.75, &#39;Age &lt;= 21.5 ngini = 0.22 nsamples = 2800 nvalue = [2447, 353] nclass = Not Taken&#39;), Text(174.375, 203.85000000000002, &#39;Occupation &lt;= 0.5 ngini = 0.499 nsamples = 71 nvalue = [37, 34] nclass = Not Taken&#39;), Text(87.1875, 67.94999999999999, &#39;gini = 0.0 nsamples = 11 nvalue = [0, 11] nclass = Taken&#39;), Text(261.5625, 67.94999999999999, &#39;gini = 0.473 nsamples = 60 nvalue = [37, 23] nclass = Not Taken&#39;), Text(523.125, 203.85000000000002, &#39;PreferredPropertyStar &lt;= 4.5 ngini = 0.206 nsamples = 2729 nvalue = [2410, 319] nclass = Not Taken&#39;), Text(435.9375, 67.94999999999999, &#39;gini = 0.176 nsamples = 2215 nvalue = [1999, 216] nclass = Not Taken&#39;), Text(610.3125, 67.94999999999999, &#39;gini = 0.32 nsamples = 514 nvalue = [411, 103] nclass = Not Taken&#39;), Text(1046.25, 339.75, &#39;ProductPitched &lt;= 0.5 ngini = 0.452 nsamples = 1102 nvalue = [721, 381] nclass = Not Taken&#39;), Text(871.875, 203.85000000000002, &#39;Age &lt;= 30.5 ngini = 0.494 nsamples = 440 nvalue = [195, 245] nclass = Taken&#39;), Text(784.6875, 67.94999999999999, &#39;gini = 0.412 nsamples = 172 nvalue = [50, 122] nclass = Taken&#39;), Text(959.0625, 67.94999999999999, &#39;gini = 0.497 nsamples = 268 nvalue = [145, 123] nclass = Not Taken&#39;), Text(1220.625, 203.85000000000002, &#39;CityTier &lt;= 2.5 ngini = 0.326 nsamples = 662 nvalue = [526, 136] nclass = Not Taken&#39;), Text(1133.4375, 67.94999999999999, &#39;gini = 0.179 nsamples = 382 nvalue = [344, 38] nclass = Not Taken&#39;), Text(1307.8125, 67.94999999999999, &#39;gini = 0.455 nsamples = 280 nvalue = [182, 98] nclass = Not Taken&#39;)] . score = dt_clf_3.score(final_X_test_dt, y_test) score . 0.8391393442622951 . Classifying with lower number of features . dt_cols = [&#39;Age&#39;,&#39;PreferredPropertyStar&#39;,&#39;MonthlyIncome&#39;,&#39;CityTier&#39;,&#39;OwnCar&#39;,&#39;Occupation&#39;, &#39;Designation&#39;] X_train_dt = final_X_train[dt_cols] y_train_dt = final_X_train[&#39;ProductPitched&#39;] X_test_dt = final_X_test[dt_cols] y_test_dt = final_X_test[&#39;ProductPitched&#39;] . dt_clf_4 = DecisionTreeClassifier(max_depth=3, random_state=1) dt_clf_4.fit(X_train_dt, y_train_dt) . DecisionTreeClassifier(max_depth=3, random_state=1) . fig = plt.figure(figsize=(25,10)) plot_tree(dt_clf_4, fontsize = 15,feature_names = X_train_dt.columns, class_names = [&quot;Basic&quot;, &quot;Standard&quot;, &quot;Deluxe&quot;, &quot;Super Deluxe&quot;, &quot;King&quot;], label = &quot;all&quot;, rounded = True ,filled = True) . [Text(620.0, 475.65000000000003, &#39;Designation &lt;= 1.5 ngini = 0.703 nsamples = 3902 nvalue = [1476, 1376, 195, 576, 279] nclass = Basic&#39;), Text(310.0, 339.75, &#39;Designation &lt;= 0.5 ngini = 0.267 nsamples = 1755 nvalue = [1476, 0, 0, 0, 279] nclass = Basic&#39;), Text(155.0, 203.85000000000002, &#39;gini = 0.0 nsamples = 279 nvalue = [0, 0, 0, 0, 279] nclass = King&#39;), Text(465.0, 203.85000000000002, &#39;gini = 0.0 nsamples = 1476 nvalue = [1476, 0, 0, 0, 0] nclass = Basic&#39;), Text(930.0, 339.75, &#39;Designation &lt;= 2.5 ngini = 0.509 nsamples = 2147 nvalue = [0, 1376, 195, 576, 0] nclass = Standard&#39;), Text(775.0, 203.85000000000002, &#39;gini = 0.0 nsamples = 1376 nvalue = [0, 1376, 0, 0, 0] nclass = Standard&#39;), Text(1085.0, 203.85000000000002, &#39;Designation &lt;= 3.5 ngini = 0.378 nsamples = 771 nvalue = [0, 0, 195, 576, 0] nclass = Super Deluxe&#39;), Text(930.0, 67.94999999999999, &#39;gini = 0.0 nsamples = 576 nvalue = [0, 0, 0, 576, 0] nclass = Super Deluxe&#39;), Text(1240.0, 67.94999999999999, &#39;gini = 0.0 nsamples = 195 nvalue = [0, 0, 195, 0, 0] nclass = Deluxe&#39;)] . Here the decision tree cant be branched any further as its leaf nodes are totally pure &amp; it seems it overfitted the training data . ord_encoder.categories_ . [array([&#39;Large Business&#39;, &#39;Salaried&#39;, &#39;Small Business&#39;], dtype=object), array([&#39;Basic&#39;, &#39;Deluxe&#39;, &#39;King&#39;, &#39;Standard&#39;, &#39;Super Deluxe&#39;], dtype=object), array([&#39;AVP&#39;, &#39;Executive&#39;, &#39;Manager&#39;, &#39;Senior Manager&#39;, &#39;VP&#39;], dtype=object)] . score = dt_clf_4.score(X_test_dt, y_test) score . 0.29405737704918034 . Limiting max # of features . dt_clf_5 = DecisionTreeClassifier(max_depth=3, max_features = 2, random_state=1) dt_clf_5.fit(X_train_dt, y_train_dt) fig = plt.figure(figsize=(30,10)) plot_tree(dt_clf_5, fontsize = 15,feature_names = X_train_dt.columns, class_names = [&quot;Basic&quot;, &quot;Standard&quot;, &quot;Deluxe&quot;, &quot;Super Deluxe&quot;, &quot;King&quot;], label = &quot;all&quot;, rounded = True ,filled = True) . [Text(837.0, 475.65000000000003, &#39;MonthlyIncome &lt;= 22623.5 ngini = 0.703 nsamples = 3902 nvalue = [1476, 1376, 195, 576, 279] nclass = Basic&#39;), Text(418.5, 339.75, &#39;MonthlyIncome &lt;= 18694.5 ngini = 0.468 nsamples = 2120 nvalue = [1387, 683, 2, 45, 3] nclass = Basic&#39;), Text(209.25, 203.85000000000002, &#39;Age &lt;= 35.5 ngini = 0.127 nsamples = 666 nvalue = [621, 39, 1, 4, 1] nclass = Basic&#39;), Text(104.625, 67.94999999999999, &#39;gini = 0.042 nsamples = 419 nvalue = [410, 7, 1, 1, 0] nclass = Basic&#39;), Text(313.875, 67.94999999999999, &#39;gini = 0.253 nsamples = 247 nvalue = [211, 32, 0, 3, 1] nclass = Basic&#39;), Text(627.75, 203.85000000000002, &#39;CityTier &lt;= 2.5 ngini = 0.525 nsamples = 1454 nvalue = [766, 644, 1, 41, 2] nclass = Basic&#39;), Text(523.125, 67.94999999999999, &#39;gini = 0.479 nsamples = 1035 nvalue = [656, 357, 0, 22, 0] nclass = Basic&#39;), Text(732.375, 67.94999999999999, &#39;gini = 0.46 nsamples = 419 nvalue = [110, 287, 1, 19, 2] nclass = Standard&#39;), Text(1255.5, 339.75, &#39;MonthlyIncome &lt;= 25644.0 ngini = 0.722 nsamples = 1782 nvalue = [89, 693, 193, 531, 276] nclass = Standard&#39;), Text(1046.25, 203.85000000000002, &#39;Designation &lt;= 2.5 ngini = 0.427 nsamples = 870 nvalue = [71, 634, 0, 164, 1] nclass = Standard&#39;), Text(941.625, 67.94999999999999, &#39;gini = 0.183 nsamples = 706 nvalue = [71, 634, 0, 0, 1] nclass = Standard&#39;), Text(1150.875, 67.94999999999999, &#39;gini = 0.0 nsamples = 164 nvalue = [0, 0, 0, 164, 0] nclass = Super Deluxe&#39;), Text(1464.75, 203.85000000000002, &#39;Age &lt;= 39.5 ngini = 0.698 nsamples = 912 nvalue = [18, 59, 193, 367, 275] nclass = Super Deluxe&#39;), Text(1360.125, 67.94999999999999, &#39;gini = 0.414 nsamples = 270 nvalue = [5, 39, 5, 202, 19] nclass = Super Deluxe&#39;), Text(1569.375, 67.94999999999999, &#39;gini = 0.688 nsamples = 642 nvalue = [13, 20, 188, 165, 256] nclass = King&#39;)] . score = dt_clf_5.score(X_test_dt, y_test) score . 0.39549180327868855 . Limiting # of samples in a leaf node . dt_clf_6 = DecisionTreeClassifier(max_depth=3, max_features = 2, min_samples_leaf = 400, random_state=1) dt_clf_6.fit(X_train_dt, y_train_dt) fig = plt.figure(figsize=(30,10)) plot_tree(dt_clf_6, fontsize = 15,feature_names = X_train_dt.columns, class_names = [&quot;Basic&quot;, &quot;Standard&quot;, &quot;Deluxe&quot;, &quot;Super Deluxe&quot;, &quot;King&quot;], label = &quot;all&quot;, rounded = True ,filled = True) . [Text(837.0, 475.65000000000003, &#39;MonthlyIncome &lt;= 22623.5 ngini = 0.703 nsamples = 3902 nvalue = [1476, 1376, 195, 576, 279] nclass = Basic&#39;), Text(334.8, 339.75, &#39;MonthlyIncome &lt;= 18694.5 ngini = 0.468 nsamples = 2120 nvalue = [1387, 683, 2, 45, 3] nclass = Basic&#39;), Text(167.4, 203.85000000000002, &#39;gini = 0.127 nsamples = 666 nvalue = [621, 39, 1, 4, 1] nclass = Basic&#39;), Text(502.20000000000005, 203.85000000000002, &#39;MonthlyIncome &lt;= 20765.5 ngini = 0.525 nsamples = 1454 nvalue = [766, 644, 1, 41, 2] nclass = Basic&#39;), Text(334.8, 67.94999999999999, &#39;gini = 0.424 nsamples = 403 nvalue = [116, 283, 0, 4, 0] nclass = Standard&#39;), Text(669.6, 67.94999999999999, &#39;gini = 0.498 nsamples = 1051 nvalue = [650, 361, 1, 37, 2] nclass = Basic&#39;), Text(1339.2, 339.75, &#39;CityTier &lt;= 2.5 ngini = 0.722 nsamples = 1782 nvalue = [89, 693, 193, 531, 276] nclass = Standard&#39;), Text(1171.8, 203.85000000000002, &#39;MonthlyIncome &lt;= 25644.0 ngini = 0.754 nsamples = 1098 nvalue = [77, 350, 158, 331, 182] nclass = Standard&#39;), Text(1004.4000000000001, 67.94999999999999, &#39;gini = 0.485 nsamples = 488 nvalue = [66, 332, 0, 90, 0] nclass = Standard&#39;), Text(1339.2, 67.94999999999999, &#39;gini = 0.687 nsamples = 610 nvalue = [11, 18, 158, 241, 182] nclass = Super Deluxe&#39;), Text(1506.6000000000001, 203.85000000000002, &#39;gini = 0.641 nsamples = 684 nvalue = [12, 343, 35, 200, 94] nclass = Standard&#39;)] . score = dt_clf_6.score(X_test_dt, y_test) score . 0.41290983606557374 . Applying cross-validation . y_train_scores = cross_val_score(dt_clf_6, X_train_dt, y_train_dt, cv=5, scoring = &quot;accuracy&quot;) print(y_train_scores) print(y_train_scores.mean()) . [0.61331626 0.60819462 0.62564103 0.61153846 0.60769231] 0.6132765356709019 . y_train_scores = cross_val_score(dt_clf_5, X_train_dt, y_train_dt, cv=5, scoring = &quot;accuracy&quot;) print(y_train_scores) print(y_train_scores.mean()) . [0.72599232 0.72215109 0.73205128 0.71025641 0.71538462] 0.7211671427164386 . Its obvious that corss-validation applied on dt_clf_5 &amp; dt_clf_6 increased the training accuracy . y_train_scores = cross_val_score(dt_clf_3, final_X_test_dt, y_test, cv=5, scoring = &quot;accuracy&quot;) print(y_train_scores) print(y_train_scores.mean()) . [0.85714286 0.79487179 0.83589744 0.84102564 0.86666667] 0.8391208791208792 . dt_clf_3_1 = DecisionTreeClassifier(max_depth=3, max_features = 2, random_state=1) dt_clf_3_1.fit(final_X_train_dt, y_train) fig = plt.figure(figsize=(25,10)) plot_tree(dt_clf_3_1, fontsize = 15,feature_names = final_X_train_dt.columns, class_names = [&quot;Not Taken&quot;, &quot;Taken&quot;], label = &quot;all&quot;, rounded = True ,filled = True) . [Text(697.5, 475.65000000000003, &#39;MonthlyIncome &lt;= 21304.5 ngini = 0.305 nsamples = 3902 nvalue = [3168, 734] nclass = Not Taken&#39;), Text(348.75, 339.75, &#39;MonthlyIncome &lt;= 16927.5 ngini = 0.39 nsamples = 1441 nvalue = [1058, 383] nclass = Not Taken&#39;), Text(174.375, 203.85000000000002, &#39;MonthlyIncome &lt;= 16748.0 ngini = 0.497 nsamples = 41 nvalue = [19, 22] nclass = Taken&#39;), Text(87.1875, 67.94999999999999, &#39;gini = 0.496 nsamples = 35 nvalue = [19, 16] nclass = Not Taken&#39;), Text(261.5625, 67.94999999999999, &#39;gini = 0.0 nsamples = 6 nvalue = [0, 6] nclass = Taken&#39;), Text(523.125, 203.85000000000002, &#39;Occupation &lt;= 0.5 ngini = 0.383 nsamples = 1400 nvalue = [1039, 361] nclass = Not Taken&#39;), Text(435.9375, 67.94999999999999, &#39;gini = 0.476 nsamples = 136 nvalue = [83, 53] nclass = Not Taken&#39;), Text(610.3125, 67.94999999999999, &#39;gini = 0.369 nsamples = 1264 nvalue = [956, 308] nclass = Not Taken&#39;), Text(1046.25, 339.75, &#39;NumberOfFollowups &lt;= 5.5 ngini = 0.245 nsamples = 2461 nvalue = [2110, 351] nclass = Not Taken&#39;), Text(871.875, 203.85000000000002, &#39;ProductPitched &lt;= 0.5 ngini = 0.234 nsamples = 2367 nvalue = [2047, 320] nclass = Not Taken&#39;), Text(784.6875, 67.94999999999999, &#39;gini = 0.387 nsamples = 443 nvalue = [327, 116] nclass = Not Taken&#39;), Text(959.0625, 67.94999999999999, &#39;gini = 0.19 nsamples = 1924 nvalue = [1720, 204] nclass = Not Taken&#39;), Text(1220.625, 203.85000000000002, &#39;Age &lt;= 35.5 ngini = 0.442 nsamples = 94 nvalue = [63, 31] nclass = Not Taken&#39;), Text(1133.4375, 67.94999999999999, &#39;gini = 0.494 nsamples = 36 nvalue = [20, 16] nclass = Not Taken&#39;), Text(1307.8125, 67.94999999999999, &#39;gini = 0.383 nsamples = 58 nvalue = [43, 15] nclass = Not Taken&#39;)] . y_train_scores = cross_val_score(dt_clf_3_1, final_X_test_dt, y_test, cv=5, scoring = &quot;accuracy&quot;) print(y_train_scores) print(y_train_scores.mean()) . [0.82142857 0.81538462 0.78974359 0.80512821 0.8 ] 0.8063369963369963 . Cross-validation applied on decision tree of dt_clf_3_1 reduced the accuracy by limiting the number of features in each split, which indicates the model can be generalized to external datasets . Linear Regression &amp; Neural Networks . Applying Logistic Regression without cross-validation . from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score . log_reg = LogisticRegression(solver=&quot;lbfgs&quot;, max_iter=1000, random_state=1) log_reg.fit(middle_X_train, y_train) . LogisticRegression(max_iter=1000, random_state=1) . y_pred = log_reg.predict(middle_X_test) print(&quot;Accuracy:&quot;, accuracy_score(y_test, y_pred)) print(&quot;Precision:&quot;, precision_score(y_test, y_pred)) print(&quot;Recall:&quot;, recall_score(y_test, y_pred)) . Accuracy: 0.8391393442622951 Precision: 0.7454545454545455 Recall: 0.22282608695652173 . The Logistic Classifier can be generalized to new data as it has accuracy of 84% . Apply Softmax Classifier on Multiclass Target . softmax_reg = LogisticRegression(multi_class=&quot;multinomial&quot;, solver=&quot;lbfgs&quot;, random_state=1, max_iter=3000) softmax_reg.fit(middle_X_train, y_train_dt) #using previously choosen decision tree target &quot;y_train_dt&quot; of multiclasses . LogisticRegression(max_iter=3000, multi_class=&#39;multinomial&#39;, random_state=1) . pred_softmax = softmax_reg.predict(middle_X_test) print(&quot;Accuracy:&quot;, accuracy_score(y_test_dt, pred_softmax)) . Accuracy: 0.5522540983606558 . Softmax Classifier seems to be less accurate compared to Binary Logistic Regression Classifier . Applying Logistic Regression with cross-validation . logreg_scores = cross_val_score(log_reg, middle_X_train, y_train, cv=10, scoring = &quot;accuracy&quot;) print(logreg_scores) print(logreg_scores.mean()) . [0.81585678 0.8286445 0.83846154 0.81282051 0.81538462 0.84615385 0.83846154 0.83076923 0.83589744 0.81794872] 0.8280398714669813 . logreg_softmax_scores = cross_val_score(softmax_reg, middle_X_train, y_train_dt, cv=10, scoring = &quot;accuracy&quot;) print(logreg_softmax_scores) print(logreg_softmax_scores.mean()) . C: Users MR anaconda3 lib site-packages sklearn linear_model _logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression n_iter_i = _check_optimize_result( . [0.59079284 0.60102302 0.59487179 0.57179487 0.56923077 0.55897436 0.59487179 0.60512821 0.54358974 0.53846154] 0.576873893370057 . From the experimentation, cross validation didnt improve the Binary Classifier accuracy, but it did improve the Multiclass Classifier accuracy with a small margin . Perceptrons . from sklearn.neural_network import MLPClassifier . Training simple NN with default parameters . mlp = MLPClassifier(max_iter=3000, random_state=1) mlp.fit(middle_X_train, y_train) mlp.score(middle_X_test, y_test) . 0.8114754098360656 . Varying the MLP parameters to check its performance . mlp_2 = MLPClassifier(hidden_layer_sizes = (5,4,3), activation = &quot;logistic&quot;, solver = &quot;sgd&quot;, max_iter=5000,learning_rate_init = 0.01, random_state=1) mlp_2.fit(middle_X_train, y_train) mlp_2.score(middle_X_test, y_test) . 0.8114754098360656 . mlp_3 = MLPClassifier(hidden_layer_sizes = (2,3,2), activation = &quot;tanh&quot;, solver = &quot;sgd&quot;, max_iter=4000,learning_rate_init = 0.01, random_state=1) mlp_3.fit(middle_X_train, y_train) mlp_3.score(middle_X_test, y_test) . 0.8114754098360656 . mlp_4 = MLPClassifier(hidden_layer_sizes = (4,6), activation = &quot;relu&quot;, solver = &quot;adam&quot;, max_iter=6000,learning_rate_init = 0.003, random_state=1) mlp_4.fit(middle_X_train, y_train) mlp_4.score(middle_X_test, y_test) . 0.8114754098360656 . Apply MLP on multiclass target . mlp_5 = MLPClassifier(hidden_layer_sizes = (5,4,3), activation = &quot;logistic&quot;, solver = &quot;sgd&quot;, max_iter=5000,learning_rate_init = 0.01, random_state=1) mlp_5.fit(middle_X_train, y_train_dt) mlp_5.score(middle_X_test, y_test_dt) . 0.36885245901639346 . mlp_6 = MLPClassifier(hidden_layer_sizes = (2,3,5), activation = &quot;tanh&quot;, solver = &quot;sgd&quot;, max_iter=10000,learning_rate_init = 0.001, random_state=1) mlp_6.fit(middle_X_train, y_train_dt) mlp_6.score(middle_X_test, y_test_dt) . 0.36885245901639346 . mlp_7 = MLPClassifier(hidden_layer_sizes = (4,6), activation = &quot;relu&quot;, solver = &quot;adam&quot;, max_iter=6000,learning_rate_init = 0.003, random_state=1) mlp_7.fit(middle_X_train, y_train_dt) mlp_7.score(middle_X_test, y_test_dt) . 0.36885245901639346 . From expermintation, the MLP performance is very close to Binary Logistic Regression Classifier, while its performance is very poor compared to the Multiclass Logistic Regression Classifier. . Also, changing the parameters of the Neural Network didnt change the resulting accuracy, which might be because of the simplicity &amp; the small size of the dataset when its fed to the MLP . Research Question . The Question: . During my experimentation, I encountered a difficulty to apply Clustering on mixed variables of numerical &amp; categorical types. By doing some research, I found a library called Kmodes that applies clustering on categorical variables only, meanwhile sklearn clustering requires numerical input to be able to group similar instances. So, as I have a dataset of mixed variables, I wanted to apply clustering on the full dataset all at once &amp; be able to cluster them combinedly. . **My question is, how to efficiently cluster mixed dataset with categorical &amp; numerical variables ? &amp; how to visualize the resulting clusters in one graph?** . The Answer: . For the first part of the question, the initial idea I thought of is to preprocess one type of the variables &amp; transform it into the other type &amp; combine the result in one-typed dataset. For categorical variables, experimentation can be done to quantify categories in either ordered or unordered manner &amp; find a way to deal with their discrete numerical values to fit in the numerical clusterer, then combine them to the numerical variables &amp; use sklearn to perform clustering. For, numerical variables, we can experiment with different methods of discretization to convert them into discrete values &amp; apply kmodes or a similar library on the final categorical dataset. . For the second part, one way of the representation is to visualize the clusters with counts of each variables in each cluster - wheteher its numerical or categorical - in one graph. Other visualization libraries can be consulted to find a more suitable &amp; effective way of visualizeing the result clusters. . References: . [1] A. Chaturvedi, P.E.Green, and J.D. Carroll (2001). K-modes Clustering. Researchgate [Viewed 2 November 2021]. Available form: https://www.researchgate.net/publication/226946703_K-modes_Clustering . [2] K. Bindra, A. Mishra (2017). &quot;A Detailed Study of Clustering Algorithms&quot;. IEEEXplore [Viewed 4 November 2021]. Available form: https://ieeexplore.ieee.org/document/8342454 . [3] K. Wilamowska, M. Manic (2001). &quot;Unsupervised pattern clustering for data mining&quot;. IEEEXplore [Viewed 4 November 2021]. Available form: https://ieeexplore.ieee.org/document/975574 . [4] Mayra Z. Rodriguez, Cesar H. Comin ,Dalcimar Casanova, Odemir M. Bruno, Diego R. Amancio, Luciano da F. Costa, Francisco A. Rodrigues (2019). &quot;Clustering algorithms: A comparative approach&quot;. Plos One [Viewed 7 November 2021]. Available form: https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0210236 .",
            "url": "https://moraouf.github.io/MoSpace/project/2021/11/28/holiday-package-recommendations-using-different-ml-algorithms.html",
            "relUrl": "/project/2021/11/28/holiday-package-recommendations-using-different-ml-algorithms.html",
            "date": " • Nov 28, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Artificial Neural Networks from Scratch",
            "content": "Intro . import numpy as np import pandas as pd import matplotlib.pyplot as plt import time %matplotlib inline . df = pd.read_csv(&#39;data_banknote_authentication.txt&#39;,names=[&#39;VAR&#39;, &#39;SKEW&#39;, &#39;Curt&#39;, &#39;Entropy&#39;, &#39;Class&#39;]) . df.head() . VAR SKEW Curt Entropy Class . 0 3.62160 | 8.6661 | -2.8073 | -0.44699 | 0 | . 1 4.54590 | 8.1674 | -2.4586 | -1.46210 | 0 | . 2 3.86600 | -2.6383 | 1.9242 | 0.10645 | 0 | . 3 3.45660 | 9.5228 | -4.0112 | -3.59440 | 0 | . 4 0.32924 | -4.4552 | 4.5718 | -0.98880 | 0 | . data = np.array(df) print(&quot;Input Data shape: &quot;, data.shape) print(&quot; n&quot;,data) . Input Data shape: (1372, 5) [[ 3.6216 8.6661 -2.8073 -0.44699 0. ] [ 4.5459 8.1674 -2.4586 -1.4621 0. ] [ 3.866 -2.6383 1.9242 0.10645 0. ] ... [ -3.7503 -13.4586 17.5932 -2.7771 1. ] [ -3.5637 -8.3827 12.393 -1.2823 1. ] [ -2.5419 -0.65804 2.6842 1.1952 1. ]] . X=data[:, :4].T Y = data[:, 4, None].T n_x, m = X.shape print(&quot;X shape: &quot;, X.shape) print(&quot;Y shape: &quot;, Y.shape) . X shape: (4, 1372) Y shape: (1, 1372) . n_h = 5 n_y = 1 . Simple One Hidden Layer Neural Network . def initialize_params(n_x, n_h, n_y): params = {} np.random.seed(1) W1 = np.random.randn(n_h, n_x) * 0.01 b1 = np.zeros((n_h, 1)) W2 = np.random.randn(n_y, n_h) *0.01 b2 = np.zeros((n_y, 1)) params = {&quot;W1&quot;:W1, &quot;b1&quot;: b1, &quot;W2&quot;: W2, &quot;b2&quot;: b2} return params . parameters = initialize_params(n_x , n_h , n_y) parameters . {&#39;W1&#39;: array([[ 0.01624345, -0.00611756, -0.00528172, -0.01072969], [ 0.00865408, -0.02301539, 0.01744812, -0.00761207], [ 0.00319039, -0.0024937 , 0.01462108, -0.02060141], [-0.00322417, -0.00384054, 0.01133769, -0.01099891], [-0.00172428, -0.00877858, 0.00042214, 0.00582815]]), &#39;b1&#39;: array([[0.], [0.], [0.], [0.], [0.]]), &#39;W2&#39;: array([[-0.01100619, 0.01144724, 0.00901591, 0.00502494, 0.00900856]]), &#39;b2&#39;: array([[0.]])} . def sigmoid(Z): return 1/(1+np.exp(-Z)) . # def relu(Z): # return np.maximum(0, Z) . # def tanh(Z): # a = (np.exp(Z) - np.exp(-Z)) / (np.exp(Z) + np.exp(-Z)) # return a . def forward_propagation_one(X, params): W1, b1, W2, b2 = params[&quot;W1&quot;], params[&quot;b1&quot;], params[&quot;W2&quot;], params[&quot;b2&quot;] Z1 = np.dot(W1, X)+b1 A1 = sigmoid(Z1) Z2 = np.dot(W2, A1)+b2 A2 = sigmoid(Z2) forward_cache = {&quot;X&quot;: X, &quot;Z1&quot;:Z1, &quot;A1&quot;: A1, &quot;Z2&quot;:Z2, &quot;A2&quot;: A2} return A2, forward_cache . A2, forward_cache = forward_propagation_one(X, parameters) A2.shape . (1, 1372) . def compute_cost(A, Y): m = Y.shape[1] cost = -(1/m) * np.sum(Y * np.log(A) + (1-Y) * np.log(1-A)) return cost . original_cost = compute_cost(A2, Y) print(&quot;Original cost without any gradient descent applied: {}&quot;.format(original_cost)) . Original cost without any gradient descent applied: 0.6936561936241545 . def sigmoid_derivative(A): return A * (1 - A) . # def relu_derivative(A): # A = np.where(A &lt;= 0, 0, 1) # return A . # def tanh_derivative(A): # return 1 - (A ** 2) . def backward_propagation_one(A2, Y, params, forward_cache): grads = {} X, Z1, A1, Z2 = forward_cache[&quot;X&quot;], forward_cache[&quot;Z1&quot;], forward_cache[&quot;A1&quot;], forward_cache[&quot;Z2&quot;] W1, b1, W2, b2 = params[&quot;W1&quot;], params[&quot;b1&quot;], params[&quot;W2&quot;], params[&quot;b2&quot;] dZ2 = A2 - Y dW2 = 1/m * np.dot(dZ2, A1.T) db2 = 1/m * np.sum(dZ2, axis = 1, keepdims = True) dZ1 = np.dot(W2.T, dZ2) * sigmoid_derivative(A1) dW1 = 1/m * np.dot(dZ1, X.T) db1 = 1/m * np.sum(dZ1, axis = 1, keepdims = True) grads = {&quot;dW1&quot;:dW1, &quot;db1&quot;: db1, &quot;dW2&quot;: dW2, &quot;db2&quot;: db2} return grads . backward_grads = backward_propagation_one(A2, Y, parameters, forward_cache) backward_grads . {&#39;dW1&#39;: array([[-2.88313234e-03, -3.87116314e-03, 6.90812050e-04, 1.23696535e-04], [ 2.97036366e-03, 3.94953380e-03, -6.59826628e-04, -1.25878222e-04], [ 2.35902234e-03, 3.16138365e-03, -5.55768522e-04, -1.02218035e-04], [ 1.31502617e-03, 1.76250744e-03, -3.09975034e-04, -5.69410563e-05], [ 2.35901150e-03, 3.16104986e-03, -5.62605759e-04, -9.93837710e-05]]), &#39;db1&#39;: array([[-1.60439799e-04], [ 1.68086092e-04], [ 1.32150608e-04], [ 7.36549191e-05], [ 1.30971446e-04]]), &#39;dW2&#39;: array([[0.03172731, 0.02240537, 0.02844887, 0.02639144, 0.02554081]]), &#39;db2&#39;: array([[0.05834027]])} . def update_params(grads, params, learning_rate): parameters = params.copy() L = len(grads)//2 for i in range(L): parameters[&quot;W&quot;+str(i+1)] = parameters[&quot;W&quot;+str(i+1)] - learning_rate * grads[&quot;dW&quot;+str(i+1)] parameters[&quot;b&quot;+str(i+1)] = parameters[&quot;b&quot;+str(i+1)] - learning_rate * grads[&quot;db&quot;+str(i+1)] return parameters . updated_parameters = update_params(backward_grads, parameters, 0.01) updated_parameters . {&#39;W1&#39;: array([[ 0.01627228, -0.00607885, -0.00528863, -0.01073092], [ 0.00862437, -0.02305488, 0.01745472, -0.00761081], [ 0.0031668 , -0.00252532, 0.01462664, -0.02060038], [-0.00323732, -0.00385817, 0.01134079, -0.01099834], [-0.00174787, -0.00881019, 0.00042776, 0.00582915]]), &#39;b1&#39;: array([[ 1.60439799e-06], [-1.68086092e-06], [-1.32150608e-06], [-7.36549191e-07], [-1.30971446e-06]]), &#39;W2&#39;: array([[-0.01132346, 0.01122318, 0.00873142, 0.00476103, 0.00875315]]), &#39;b2&#39;: array([[-0.0005834]])} . A2_second, second_foward_cache = forward_propagation_one(X, updated_parameters) first_cost = compute_cost(A2_second, Y) print(&quot;First cost with gradient descent applied: {}&quot;.format(first_cost)) . First cost with gradient descent applied: 0.6935848435446771 . A2_1 = np.where(A2_second&lt;0.5, 0, 1) first_accuracy = np.mean(A2_1 == Y) print(&quot;First accuracy of 1 hidden layer of 5 nodes and 1 iteration is: {}&quot;.format(first_accuracy)) . First accuracy of 1 hidden layer of 5 nodes and 1 iteration is: 0.4446064139941691 . def forward_propagation(A_previous, W, b): Z = np.dot(W, A_previous) + b A = sigmoid(Z) forward_cache = (A_previous,Z) return A, forward_cache . def forward_propagation_for_accuracy(X, parameters): L = len(parameters)//2 caches = [] A = X for i in range(L): A_previous = A A, forward_cache = forward_propagation(A_previous, parameters[&quot;W&quot;+str(i+1)], parameters[&quot;b&quot;+str(i+1)]) caches.append(forward_cache) return A, caches . def compute_accuracy(X, Y, parameters): A, caches = forward_propagation_for_accuracy(X, parameters) A = np.where(A&lt;0.5, 0, 1) acc = np.mean(A == Y) return acc . def plot_cost(costs, accuracy, learning_rate, epochs): plt.figure(figsize = (6,6)) plt.plot(np.squeeze(costs), label = &quot;Costs&quot;) plt.plot(np.squeeze(accuracy), label = &quot;Accuracy&quot;) plt.ylabel(&#39;Cost&#39;) plt.xlabel(&#39;Epochs in hundreds&#39;) plt.title(&quot;Learning rate = &quot; + str(learning_rate) + &quot; &amp; Epochs = &quot; + str(epochs)) plt.legend() plt.show() . def one_layer_model(X, Y, n_x, n_h, n_y, learning_rate, epochs): parameters = initialize_params( n_x, n_h, n_y) costs = [] accuracy = [] print(&quot;Number of Epochs: &quot;, epochs) print(&quot;Learning rate: &quot;, learning_rate) print(&quot;*&quot;*60) t0 = time.time() for i in range(epochs): A2, forward_cache = forward_propagation_one(X, parameters) cost = compute_cost(A2, Y) grads = backward_propagation_one(A2, Y, parameters, forward_cache) parameters = update_params(grads, parameters, learning_rate) if i%100 == 0: costs.append(cost) accuracy.append(compute_accuracy(X, Y, parameters)) print(&quot;Cost after epoch {}: {}&quot;.format(i, cost)) t1 = time.time() print(&quot;*&quot;*60) print(&quot;Elapsed time for the training is: &quot;,t1-t0) A, forward_cache = forward_propagation_one(X, parameters) A = np.where(A&lt;0.5, 0, 1) final_accuracy = np.mean(A == Y) print(&quot;Final Accuracy is: {}%&quot;.format(final_accuracy * 100)) return parameters, costs, accuracy . parameters, costs, accuracy = one_layer_model(X, Y, 4, 5, 1, 0.01, 3000) plot_cost(costs, accuracy, 0.01, 3000) . Number of Epochs: 3000 Learning rate: 0.01 ************************************************************ Cost after epoch 0: 0.6936561936241545 Cost after epoch 100: 0.6891555181919499 Cost after epoch 200: 0.6866492737315638 Cost after epoch 300: 0.6838653133393823 Cost after epoch 400: 0.6798017279749253 Cost after epoch 500: 0.6739129093087606 Cost after epoch 600: 0.6659338773113268 Cost after epoch 700: 0.6556749850152902 Cost after epoch 800: 0.6428892041228172 Cost after epoch 900: 0.6272622327631806 Cost after epoch 1000: 0.6085179665347298 Cost after epoch 1100: 0.586596356299132 Cost after epoch 1200: 0.5617766804997283 Cost after epoch 1300: 0.5346197507727908 Cost after epoch 1400: 0.505791628655597 Cost after epoch 1500: 0.4759732474312634 Cost after epoch 1600: 0.445881389049259 Cost after epoch 1700: 0.41626750777009314 Cost after epoch 1800: 0.38782285706643393 Cost after epoch 1900: 0.36106483902197867 Cost after epoch 2000: 0.3362943928846887 Cost after epoch 2100: 0.3136228916404316 Cost after epoch 2200: 0.293025695719623 Cost after epoch 2300: 0.27439370011090625 Cost after epoch 2400: 0.25757302849981134 Cost after epoch 2500: 0.24239218954464403 Cost after epoch 2600: 0.22867904809295816 Cost after epoch 2700: 0.21627047030015964 Cost after epoch 2800: 0.20501710723669814 Cost after epoch 2900: 0.1947851592063204 ************************************************************ Elapsed time for the training is: 0.5757269859313965 Final Accuracy is: 96.6472303206997% . parameters, costs, accuracy = one_layer_model(X, Y, 4, 5, 1, 0.03, 3000) plot_cost(costs, accuracy, 0.03, 3000) . Number of Epochs: 3000 Learning rate: 0.03 ************************************************************ Cost after epoch 0: 0.6936561936241545 Cost after epoch 100: 0.6838918394741097 Cost after epoch 200: 0.6660965331994124 Cost after epoch 300: 0.6276890856603726 Cost after epoch 400: 0.5625969262112488 Cost after epoch 500: 0.4770645608418843 Cost after epoch 600: 0.3889177534055111 Cost after epoch 700: 0.3145001709889604 Cost after epoch 800: 0.25821846884460986 Cost after epoch 900: 0.21674100533173474 Cost after epoch 1000: 0.185805854165944 Cost after epoch 1100: 0.1621840063716397 Cost after epoch 1200: 0.14370110645756085 Cost after epoch 1300: 0.12892114740910587 Cost after epoch 1400: 0.11688162700916178 Cost after epoch 1500: 0.1069201532807768 Cost after epoch 1600: 0.09856781120696335 Cost after epoch 1700: 0.09148391767868663 Cost after epoch 1800: 0.08541531166519473 Cost after epoch 1900: 0.08017020387369028 Cost after epoch 2000: 0.07560086297166557 Cost after epoch 2100: 0.07159183142824962 Cost after epoch 2200: 0.06805171018972822 Cost after epoch 2300: 0.06490731180066703 Cost after epoch 2400: 0.06209942373987618 Cost after epoch 2500: 0.05957968950607645 Cost after epoch 2600: 0.05730827982279907 Cost after epoch 2700: 0.05525213142578917 Cost after epoch 2800: 0.05338359950940421 Cost after epoch 2900: 0.05167941563264839 ************************************************************ Elapsed time for the training is: 0.5619690418243408 Final Accuracy is: 98.83381924198251% . Two Hidden Layers Model . def initialize_parameters_two(layers_dims): params = {} np.random.seed(1) n_x, n_h_1, n_h_2, n_y = layers_dims[0], layers_dims[1], layers_dims[2], layers_dims[3] W1 = np.random.randn(n_h_1, n_x) b1 = np.zeros((n_h_1, 1)) W2 = np.random.randn(n_h_2, n_h_1) b2 = np.zeros((n_h_2, 1)) W3 = np.random.randn(n_y, n_h_2) b3 = np.zeros((n_y, 1)) params = {&quot;W1&quot;:W1, &quot;b1&quot;: b1, &quot;W2&quot;: W2, &quot;b2&quot;: b2, &quot;W3&quot;: W3, &quot;b3&quot;: b3} return params . def forward_propagation_two(X, params): W1, b1, W2, b2, W3, b3 = params[&quot;W1&quot;], params[&quot;b1&quot;], params[&quot;W2&quot;], params[&quot;b2&quot;], params[&quot;W3&quot;], params[&quot;b3&quot;] Z1 = np.dot(W1, X)+b1 A1 = sigmoid(Z1) Z2 = np.dot(W2, A1)+b2 A2 = sigmoid(Z2) Z3 = np.dot(W3, A2)+b3 A3 = sigmoid(Z3) forward_cache = {&quot;X&quot;: X, &quot;Z1&quot;:Z1, &quot;A1&quot;: A1, &quot;Z2&quot;:Z2, &quot;A2&quot;: A2, &quot;Z3&quot;:Z3, &quot;A3&quot;: A3} return A3, forward_cache . def backward_propagation_two(A3, Y, params, forward_cache): grads = {} X, Z1, A1, Z2, A2, Z3 = forward_cache[&quot;X&quot;], forward_cache[&quot;Z1&quot;], forward_cache[&quot;A1&quot;], forward_cache[&quot;Z2&quot;], forward_cache[&quot;A2&quot;], forward_cache[&quot;Z3&quot;] W1, b1, W2, b2, W3, b3 = params[&quot;W1&quot;], params[&quot;b1&quot;], params[&quot;W2&quot;], params[&quot;b2&quot;], params[&quot;W3&quot;], params[&quot;b3&quot;] dZ3 = A3 - Y dW3 = (1/m) * np.matmul(dZ3, A2.T) db3 = (1/m) * np.sum(dZ3, axis = 1, keepdims = True) dZ2 = np.dot(W3.T, dZ3) * sigmoid_derivative(A2) dW2 = 1/m * np.dot(dZ2, A1.T) db2 = 1/m * np.sum(dZ2, axis = 1, keepdims = True) dZ1 = np.dot(W2.T, dZ2) * sigmoid_derivative(A1) dW1 = 1/m * np.dot(dZ1, X.T) db1 = 1/m * np.sum(dZ1, axis = 1, keepdims = True) grads = {&quot;dW1&quot;: dW1, &quot;db1&quot;: db1, &quot;dW2&quot;: dW2, &quot;db2&quot;: db2, &quot;dW3&quot;: dW3, &quot;db3&quot;: db3} return grads . def two_layers_model(X, Y, layers_dims, learning_rate, epochs): parameters = initialize_parameters_two(layers_dims) costs = [] accuracy = [] print(&quot;Number of Epochs: &quot;, epochs) print(&quot;Learning rate: &quot;, learning_rate) print(&quot;*&quot;*60) t0 = time.time() for i in range(epochs): A3, forward_cache = forward_propagation_two(X, parameters) cost = compute_cost(A3, Y) grads = backward_propagation_two(A3, Y, parameters, forward_cache) parameters = update_params(grads, parameters, learning_rate) if i%100 == 0: costs.append(cost) accuracy.append(compute_accuracy(X, Y, parameters)) print(&quot;Cost after epoch {}: {}&quot;.format(i, cost)) t1 = time.time() print(&quot;*&quot;*60) print(&quot;Elapsed time for the training is: &quot;,t1-t0) A, forward_cache = forward_propagation_two(X, parameters) A = np.where(A&lt;0.5, 0, 1) final_accuracy = np.mean(A == Y) print(&quot;Final Accuracy is: {}%&quot;.format(final_accuracy * 100)) return parameters, costs, accuracy . parameters, costs, accuracy = two_layers_model(X, Y, [4, 10, 7, 1], 0.1, 1000) plot_cost(costs, accuracy, 0.1, 1000) . Number of Epochs: 1000 Learning rate: 0.1 ************************************************************ Cost after epoch 0: 0.7341785722422635 Cost after epoch 100: 0.41804578793955455 Cost after epoch 200: 0.2536391786541635 Cost after epoch 300: 0.16290432223978138 Cost after epoch 400: 0.11279678574236231 Cost after epoch 500: 0.08323844123812278 Cost after epoch 600: 0.06457896658075797 Cost after epoch 700: 0.052136034022113295 Cost after epoch 800: 0.043416750318197556 Cost after epoch 900: 0.03703813172349534 ************************************************************ Elapsed time for the training is: 0.627739667892456 Final Accuracy is: 100.0% . parameters, costs, accuracy = two_layers_model(X, Y, [4, 10, 7, 1], 0.1, 3000) plot_cost(costs, accuracy, 0.1, 3000) . Number of Epochs: 3000 Learning rate: 0.1 ************************************************************ Cost after epoch 0: 0.7341785722422635 Cost after epoch 100: 0.41804578793955455 Cost after epoch 200: 0.2536391786541635 Cost after epoch 300: 0.16290432223978138 Cost after epoch 400: 0.11279678574236231 Cost after epoch 500: 0.08323844123812278 Cost after epoch 600: 0.06457896658075797 Cost after epoch 700: 0.052136034022113295 Cost after epoch 800: 0.043416750318197556 Cost after epoch 900: 0.03703813172349534 Cost after epoch 1000: 0.03219981750232199 Cost after epoch 1100: 0.028416353655911594 Cost after epoch 1200: 0.025381552493223308 Cost after epoch 1300: 0.022895358739410857 Cost after epoch 1400: 0.020822521811838596 Cost after epoch 1500: 0.019068650831998296 Cost after epoch 1600: 0.017565983792854387 Cost after epoch 1700: 0.016264668499844603 Cost after epoch 1800: 0.015127245424454061 Cost after epoch 1900: 0.014125045039189883 Cost after epoch 2000: 0.0132357649849225 Cost after epoch 2100: 0.012441797039876596 Cost after epoch 2200: 0.011729046019771825 Cost after epoch 2300: 0.01108608186847872 Cost after epoch 2400: 0.010503523704498993 Cost after epoch 2500: 0.009973588267700936 Cost after epoch 2600: 0.009489755721197746 Cost after epoch 2700: 0.009046519271195293 Cost after epoch 2800: 0.008639194696894451 Cost after epoch 2900: 0.008263772979248232 ************************************************************ Elapsed time for the training is: 2.1203291416168213 Final Accuracy is: 100.0% . parameters, costs, accuracy = two_layers_model(X, Y, [4, 10, 7, 1], 0.01, 3000) plot_cost(costs, accuracy, 0.01, 3000) . Number of Epochs: 3000 Learning rate: 0.01 ************************************************************ Cost after epoch 0: 0.7341785722422635 Cost after epoch 100: 0.6448888232661 Cost after epoch 200: 0.6114811485661837 Cost after epoch 300: 0.5847225602795906 Cost after epoch 400: 0.5588626928791727 Cost after epoch 500: 0.5335677344762613 Cost after epoch 600: 0.5089708164192742 Cost after epoch 700: 0.48505167154379625 Cost after epoch 800: 0.4618171132294425 Cost after epoch 900: 0.4393419700101513 Cost after epoch 1000: 0.4177176985964929 Cost after epoch 1100: 0.39701800421541655 Cost after epoch 1200: 0.37728840155940685 Cost after epoch 1300: 0.35854748241173423 Cost after epoch 1400: 0.34079258995630013 Cost after epoch 1500: 0.32400626700339125 Cost after epoch 1600: 0.3081611395301733 Cost after epoch 1700: 0.293222596857308 Cost after epoch 1800: 0.2791504742670355 Cost after epoch 1900: 0.2659010708383662 Cost after epoch 2000: 0.25342954064712714 Cost after epoch 2100: 0.2416919089256153 Cost after epoch 2200: 0.23064623180747426 Cost after epoch 2300: 0.220252946157272 Cost after epoch 2400: 0.21047471139561674 Cost after epoch 2500: 0.20127604754864237 Cost after epoch 2600: 0.19262297841338896 Cost after epoch 2700: 0.18448278579029834 Cost after epoch 2800: 0.1768239013398214 Cost after epoch 2900: 0.16961591364746087 ************************************************************ Elapsed time for the training is: 1.9494004249572754 Final Accuracy is: 98.9795918367347% . parameters, costs, accuracy = two_layers_model(X, Y, [4, 10, 7, 1], 0.001, 3000) plot_cost(costs, accuracy, 0.001, 3000) . Number of Epochs: 3000 Learning rate: 0.001 ************************************************************ Cost after epoch 0: 0.7341785722422635 Cost after epoch 100: 0.7181573572658664 Cost after epoch 200: 0.7045395939645949 Cost after epoch 300: 0.692934113784307 Cost after epoch 400: 0.6829996793798759 Cost after epoch 500: 0.6744435445037069 Cost after epoch 600: 0.6670179485990474 Cost after epoch 700: 0.6605154980174369 Cost after epoch 800: 0.6547641264553529 Cost after epoch 900: 0.6496220969495591 Cost after epoch 1000: 0.6449733253699992 Cost after epoch 1100: 0.6407231724115118 Cost after epoch 1200: 0.636794760853255 Cost after epoch 1300: 0.633125817841701 Cost after epoch 1400: 0.6296660089538582 Cost after epoch 1500: 0.6263747141522169 Cost after epoch 1600: 0.6232191896260346 Cost after epoch 1700: 0.6201730597628327 Cost after epoch 1800: 0.6172150872803299 Cost after epoch 1900: 0.6143281750350491 Cost after epoch 2000: 0.6114985590719386 Cost after epoch 2100: 0.6087151584257489 Cost after epoch 2200: 0.6059690526747632 Cost after epoch 2300: 0.60325306311987 Cost after epoch 2400: 0.6005614176724867 Cost after epoch 2500: 0.5978894831051679 Cost after epoch 2600: 0.5952335513047773 Cost after epoch 2700: 0.5925906686406986 Cost after epoch 2800: 0.5899584995941003 Cost after epoch 2900: 0.5873352174604811 ************************************************************ Elapsed time for the training is: 1.997861385345459 Final Accuracy is: 76.60349854227405% . parameters, costs, accuracy = two_layers_model(X, Y, [4, 10, 7, 1], 0.001, 6000) plot_cost(costs, accuracy, 0.001, 6000) . Number of Epochs: 6000 Learning rate: 0.001 ************************************************************ Cost after epoch 0: 0.7341785722422635 Cost after epoch 100: 0.7181573572658664 Cost after epoch 200: 0.7045395939645949 Cost after epoch 300: 0.692934113784307 Cost after epoch 400: 0.6829996793798759 Cost after epoch 500: 0.6744435445037069 Cost after epoch 600: 0.6670179485990474 Cost after epoch 700: 0.6605154980174369 Cost after epoch 800: 0.6547641264553529 Cost after epoch 900: 0.6496220969495591 Cost after epoch 1000: 0.6449733253699992 Cost after epoch 1100: 0.6407231724115118 Cost after epoch 1200: 0.636794760853255 Cost after epoch 1300: 0.633125817841701 Cost after epoch 1400: 0.6296660089538582 Cost after epoch 1500: 0.6263747141522169 Cost after epoch 1600: 0.6232191896260346 Cost after epoch 1700: 0.6201730597628327 Cost after epoch 1800: 0.6172150872803299 Cost after epoch 1900: 0.6143281750350491 Cost after epoch 2000: 0.6114985590719386 Cost after epoch 2100: 0.6087151584257489 Cost after epoch 2200: 0.6059690526747632 Cost after epoch 2300: 0.60325306311987 Cost after epoch 2400: 0.6005614176724867 Cost after epoch 2500: 0.5978894831051679 Cost after epoch 2600: 0.5952335513047773 Cost after epoch 2700: 0.5925906686406986 Cost after epoch 2800: 0.5899584995941003 Cost after epoch 2900: 0.5873352174604811 Cost after epoch 3000: 0.5847194163023651 Cost after epoch 3100: 0.5821100394502469 Cost after epoch 3200: 0.5795063207775528 Cost after epoch 3300: 0.5769077357505552 Cost after epoch 3400: 0.5743139599091506 Cost after epoch 3500: 0.5717248329927316 Cost after epoch 3600: 0.5691403274025002 Cost after epoch 3700: 0.5665605200958242 Cost after epoch 3800: 0.5639855673426818 Cost after epoch 3900: 0.5614156820387404 Cost after epoch 4000: 0.5588511134634823 Cost after epoch 4100: 0.5562921294960175 Cost after epoch 4200: 0.5537390013602476 Cost after epoch 4300: 0.551191990973543 Cost after epoch 4400: 0.5486513409316033 Cost after epoch 4500: 0.5461172670917948 Cost after epoch 4600: 0.543589953633787 Cost after epoch 4700: 0.5410695503942032 Cost after epoch 4800: 0.5385561722027006 Cost after epoch 4900: 0.5360498998979412 Cost after epoch 5000: 0.5335507826766195 Cost after epoch 5100: 0.5310588414267347 Cost after epoch 5200: 0.5285740727145148 Cost after epoch 5300: 0.5260964531280082 Cost after epoch 5400: 0.5236259437239607 Cost after epoch 5500: 0.5211624943730051 Cost after epoch 5600: 0.518706047846992 Cost after epoch 5700: 0.5162565435381629 Cost after epoch 5800: 0.5138139207405653 Cost after epoch 5900: 0.5113781214584459 ************************************************************ Elapsed time for the training is: 4.004393100738525 Final Accuracy is: 88.33819241982506% . L Layer Model . def initialize_parameters_L(layers_dims): parameters = {} np.random.seed(1) L = len(layers_dims) for i in range(1, L): parameters[&quot;W&quot;+str(i)] = np.random.randn(layers_dims[i], layers_dims[i-1]) parameters[&quot;b&quot;+str(i)] = np.zeros((layers_dims[i], 1)) return parameters . def forward_propagation_L(X, parameters): L = len(parameters)//2 caches = [] A = X for i in range(L): A_previous = A A, forward_cache = forward_propagation(A_previous, parameters[&quot;W&quot;+str(i+1)], parameters[&quot;b&quot;+str(i+1)]) caches.append(forward_cache) return A, caches . def backward_propagation_L(A, Y, parameters, caches): grads = {} L = len(parameters)//2 dA = -(Y/A) + ((1-Y)/(1-A)) dZ = A - Y for i in reversed(range(L)): current_cache = caches[i] A, Z = current_cache grads[&quot;dW&quot;+str(i+1)] = (1/m)*np.dot(dZ, A.T) grads[&quot;db&quot;+str(i+1)] = (1/m)*np.sum(dZ, axis = 1, keepdims = True) grads[&quot;dZ&quot;+str(i)] = np.dot(parameters[&quot;W&quot;+str(i+1)].T, dZ) * sigmoid_derivative(A) dZ = grads[&quot;dZ&quot;+str(i)] return grads . def update_params_L(grads, params, learning_rate): parameters = params.copy() L = len(parameters)//2 for i in range(L): parameters[&quot;W&quot;+str(i+1)] = parameters[&quot;W&quot;+str(i+1)] - learning_rate * grads[&quot;dW&quot;+str(i+1)] parameters[&quot;b&quot;+str(i+1)] = parameters[&quot;b&quot;+str(i+1)] - learning_rate * grads[&quot;db&quot;+str(i+1)] return parameters . def L_layers_model(X, Y, layers_dims, learning_rate, epochs): parameters = initialize_parameters_L(layers_dims) costs = [] accuracy = [] print(&quot;Number of Epochs: &quot;, epochs) print(&quot;Learning rate: &quot;, learning_rate) print(&quot;*&quot;*60) t0 = time.time() for i in range(epochs): A, caches = forward_propagation_L(X, parameters) # print(&quot;Number of Caches&quot;,len(caches)) # for i, cache in enumerate(caches): # print(&quot;Cache {} has shape {} : {}&quot;.format(i, cache[0].shape, cache[0])) # print(&quot;Cache {} has shape {} : {}&quot;.format(i, cache[1].shape, cache[1])) cost = compute_cost(A, Y) grads = backward_propagation_L(A, Y, parameters, caches) parameters = update_params_L(grads, parameters, learning_rate) if i%100 == 0: costs.append(cost) accuracy.append(compute_accuracy(X, Y, parameters)) print(&quot;Cost after epoch {}: {}&quot;.format(i, cost)) t1 = time.time() print(&quot;*&quot;*60) print(&quot;Elapsed time for the training is: &quot;,t1-t0) final_accuracy = compute_accuracy(X, Y, parameters) print(&quot;Final Accuracy is: {}%&quot;.format(final_accuracy * 100)) return parameters, costs, accuracy . parameters, costs, accuracy = L_layers_model(X, Y, [4, 10, 7, 5, 1], 0.001, 1000) plot_cost(costs,accuracy, 0.001, 1000) . Number of Epochs: 1000 Learning rate: 0.001 ************************************************************ Cost after epoch 0: 0.9058177931945443 Cost after epoch 100: 0.8787549041993477 Cost after epoch 200: 0.8547431740102313 Cost after epoch 300: 0.8335061181210778 Cost after epoch 400: 0.8147756769903448 Cost after epoch 500: 0.7982963780085447 Cost after epoch 600: 0.7838282813510609 Cost after epoch 700: 0.7711488366590706 Cost after epoch 800: 0.7600538178984678 Cost after epoch 900: 0.7503575176048662 ************************************************************ Elapsed time for the training is: 0.6054956912994385 Final Accuracy is: 44.460641399416915% . parameters, costs, accuracy = L_layers_model(X, Y, [4, 10, 7, 5, 1], 0.001, 5000) plot_cost(costs,accuracy, 0.001, 5000) . Number of Epochs: 5000 Learning rate: 0.001 ************************************************************ Cost after epoch 0: 0.9058177931945443 Cost after epoch 100: 0.8787549041993477 Cost after epoch 200: 0.8547431740102313 Cost after epoch 300: 0.8335061181210778 Cost after epoch 400: 0.8147756769903448 Cost after epoch 500: 0.7982963780085447 Cost after epoch 600: 0.7838282813510609 Cost after epoch 700: 0.7711488366590706 Cost after epoch 800: 0.7600538178984678 Cost after epoch 900: 0.7503575176048662 Cost after epoch 1000: 0.7418923764005263 Cost after epoch 1100: 0.7345082062697952 Cost after epoch 1200: 0.7280711424976305 Cost after epoch 1300: 0.7224624337600349 Cost after epoch 1400: 0.7175771554681254 Cost after epoch 1500: 0.7133229097438804 Cost after epoch 1600: 0.7096185570737341 Cost after epoch 1700: 0.7063930098693144 Cost after epoch 1800: 0.7035841066250511 Cost after epoch 1900: 0.7011375766798135 Cost after epoch 2000: 0.6990060992804858 Cost after epoch 2100: 0.6971484562347166 Cost after epoch 2200: 0.6955287745008499 Cost after epoch 2300: 0.6941158532322328 Cost after epoch 2400: 0.6928825687745086 Cost after epoch 2500: 0.6918053506741462 Cost after epoch 2600: 0.6908637217142981 Cost after epoch 2700: 0.6900398952148747 Cost after epoch 2800: 0.6893184232180706 Cost after epoch 2900: 0.6886858896571354 Cost after epoch 3000: 0.6881306431248971 Cost after epoch 3100: 0.6876425643854394 Cost after epoch 3200: 0.6872128642851072 Cost after epoch 3300: 0.686833908203912 Cost after epoch 3400: 0.6864990636376077 Cost after epoch 3500: 0.6862025679106114 Cost after epoch 3600: 0.6859394133896826 Cost after epoch 3700: 0.6857052478988434 Cost after epoch 3800: 0.6854962883295167 Cost after epoch 3900: 0.6853092456989909 Cost after epoch 4000: 0.6851412601381233 Cost after epoch 4100: 0.6849898444887379 Cost after epoch 4200: 0.6848528353654946 Cost after epoch 4300: 0.6847283506889218 Cost after epoch 4400: 0.6846147528284968 Cost after epoch 4500: 0.6845106166094906 Cost after epoch 4600: 0.6844147015369588 Cost after epoch 4700: 0.6843259276766852 Cost after epoch 4800: 0.6842433547077722 Cost after epoch 4900: 0.6841661637264429 ************************************************************ Elapsed time for the training is: 3.0545618534088135 Final Accuracy is: 55.539358600583085% . parameters, costs, accuracy = L_layers_model(X, Y, [4, 10, 7, 5, 1], 0.001, 20000) plot_cost(costs, accuracy, 0.01, 20000) . Number of Epochs: 20000 Learning rate: 0.001 ************************************************************ Cost after epoch 0: 0.9058177931945443 Cost after epoch 100: 0.8787549041993477 Cost after epoch 200: 0.8547431740102313 Cost after epoch 300: 0.8335061181210778 Cost after epoch 400: 0.8147756769903448 Cost after epoch 500: 0.7982963780085447 Cost after epoch 600: 0.7838282813510609 Cost after epoch 700: 0.7711488366590706 Cost after epoch 800: 0.7600538178984678 Cost after epoch 900: 0.7503575176048662 Cost after epoch 1000: 0.7418923764005263 Cost after epoch 1100: 0.7345082062697952 Cost after epoch 1200: 0.7280711424976305 Cost after epoch 1300: 0.7224624337600349 Cost after epoch 1400: 0.7175771554681254 Cost after epoch 1500: 0.7133229097438804 Cost after epoch 1600: 0.7096185570737341 Cost after epoch 1700: 0.7063930098693144 Cost after epoch 1800: 0.7035841066250511 Cost after epoch 1900: 0.7011375766798135 Cost after epoch 2000: 0.6990060992804858 Cost after epoch 2100: 0.6971484562347166 Cost after epoch 2200: 0.6955287745008499 Cost after epoch 2300: 0.6941158532322328 Cost after epoch 2400: 0.6928825687745086 Cost after epoch 2500: 0.6918053506741462 Cost after epoch 2600: 0.6908637217142981 Cost after epoch 2700: 0.6900398952148747 Cost after epoch 2800: 0.6893184232180706 Cost after epoch 2900: 0.6886858896571354 Cost after epoch 3000: 0.6881306431248971 Cost after epoch 3100: 0.6876425643854394 Cost after epoch 3200: 0.6872128642851072 Cost after epoch 3300: 0.686833908203912 Cost after epoch 3400: 0.6864990636376077 Cost after epoch 3500: 0.6862025679106114 Cost after epoch 3600: 0.6859394133896826 Cost after epoch 3700: 0.6857052478988434 Cost after epoch 3800: 0.6854962883295167 Cost after epoch 3900: 0.6853092456989909 Cost after epoch 4000: 0.6851412601381233 Cost after epoch 4100: 0.6849898444887379 Cost after epoch 4200: 0.6848528353654946 Cost after epoch 4300: 0.6847283506889218 Cost after epoch 4400: 0.6846147528284968 Cost after epoch 4500: 0.6845106166094906 Cost after epoch 4600: 0.6844147015369588 Cost after epoch 4700: 0.6843259276766852 Cost after epoch 4800: 0.6842433547077722 Cost after epoch 4900: 0.6841661637264429 Cost after epoch 5000: 0.6840936414367917 Cost after epoch 5100: 0.6840251664128527 Cost after epoch 5200: 0.6839601971584446 Cost after epoch 5300: 0.6838982617277052 Cost after epoch 5400: 0.6838389487007596 Cost after epoch 5500: 0.6837818993362935 Cost after epoch 5600: 0.6837268007464407 Cost after epoch 5700: 0.6836733799598815 Cost after epoch 5800: 0.6836213987567905 Cost after epoch 5900: 0.6835706491746384 Cost after epoch 6000: 0.6835209495971803 Cost after epoch 6100: 0.6834721413505034 Cost after epoch 6200: 0.6834240857400227 Cost after epoch 6300: 0.6833766614709953 Cost after epoch 6400: 0.6833297624026526 Cost after epoch 6500: 0.6832832955925903 Cost after epoch 6600: 0.6832371795937222 Cost after epoch 6700: 0.6831913429710341 Cost after epoch 6800: 0.683145723009643 Cost after epoch 6900: 0.6831002645893866 Cost after epoch 7000: 0.6830549192043884 Cost after epoch 7100: 0.6830096441088509 Cost after epoch 7200: 0.6829644015727635 Cost after epoch 7300: 0.6829191582333282 Cost after epoch 7400: 0.68287388452975 Cost after epoch 7500: 0.6828285542106365 Cost after epoch 7600: 0.6827831439046467 Cost after epoch 7700: 0.6827376327462361 Cost after epoch 7800: 0.6826920020494045 Cost after epoch 7900: 0.6826462350232622 Cost after epoch 8000: 0.6826003165240365 Cost after epoch 8100: 0.6825542328388279 Cost after epoch 8200: 0.6825079714970336 Cost after epoch 8300: 0.6824615211058802 Cost after epoch 8400: 0.6824148712069695 Cost after epoch 8500: 0.682368012151133 Cost after epoch 8600: 0.6823209349892464 Cost after epoch 8700: 0.682273631376952 Cost after epoch 8800: 0.6822260934915042 Cost after epoch 8900: 0.6821783139591792 Cost after epoch 9000: 0.682130285791894 Cost after epoch 9100: 0.6820820023318527 Cost after epoch 9200: 0.6820334572031856 Cost after epoch 9300: 0.6819846442696889 Cost after epoch 9400: 0.6819355575978749 Cost after epoch 9500: 0.6818861914246567 Cost after epoch 9600: 0.6818365401290667 Cost after epoch 9700: 0.6817865982074945 Cost after epoch 9800: 0.6817363602519885 Cost after epoch 9900: 0.6816858209312291 Cost after epoch 10000: 0.6816349749738285 Cost after epoch 10100: 0.6815838171536583 Cost after epoch 10200: 0.681532342276942 Cost after epoch 10300: 0.6814805451708865 Cost after epoch 10400: 0.6814284206736515 Cost after epoch 10500: 0.6813759636254862 Cost after epoch 10600: 0.6813231688608795 Cost after epoch 10700: 0.6812700312015935 Cost after epoch 10800: 0.6812165454504648 Cost after epoch 10900: 0.6811627063858747 Cost after epoch 11000: 0.6811085087567971 Cost after epoch 11100: 0.6810539472783529 Cost after epoch 11200: 0.6809990166278008 Cost after epoch 11300: 0.6809437114409067 Cost after epoch 11400: 0.6808880263086435 Cost after epoch 11500: 0.6808319557741747 Cost after epoch 11600: 0.6807754943300829 Cost after epoch 11700: 0.6807186364158134 Cost after epoch 11800: 0.6806613764152978 Cost after epoch 11900: 0.6806037086547375 Cost after epoch 12000: 0.6805456274005208 Cost after epoch 12100: 0.6804871268572559 Cost after epoch 12200: 0.6804282011659022 Cost after epoch 12300: 0.6803688444019861 Cost after epoch 12400: 0.6803090505738861 Cost after epoch 12500: 0.6802488136211781 Cost after epoch 12600: 0.6801881274130285 Cost after epoch 12700: 0.6801269857466302 Cost after epoch 12800: 0.6800653823456693 Cost after epoch 12900: 0.6800033108588187 Cost after epoch 13000: 0.6799407648582535 Cost after epoch 13100: 0.6798777378381798 Cost after epoch 13200: 0.6798142232133757 Cost after epoch 13300: 0.679750214317738 Cost after epoch 13400: 0.6796857044028342 Cost after epoch 13500: 0.6796206866364535 Cost after epoch 13600: 0.6795551541011556 Cost after epoch 13700: 0.6794890997928168 Cost after epoch 13800: 0.6794225166191658 Cost after epoch 13900: 0.6793553973983154 Cost after epoch 14000: 0.6792877348572813 Cost after epoch 14100: 0.6792195216304896 Cost after epoch 14200: 0.6791507502582732 Cost after epoch 14300: 0.6790814131853524 Cost after epoch 14400: 0.6790115027593018 Cost after epoch 14500: 0.678941011229001 Cost after epoch 14600: 0.678869930743069 Cost after epoch 14700: 0.6787982533482805 Cost after epoch 14800: 0.6787259709879664 Cost after epoch 14900: 0.6786530755003924 Cost after epoch 15000: 0.6785795586171222 Cost after epoch 15100: 0.6785054119613586 Cost after epoch 15200: 0.6784306270462667 Cost after epoch 15300: 0.6783551952732748 Cost after epoch 15400: 0.6782791079303572 Cost after epoch 15500: 0.6782023561902939 Cost after epoch 15600: 0.67812493110891 Cost after epoch 15700: 0.6780468236232954 Cost after epoch 15800: 0.6779680245499988 Cost after epoch 15900: 0.6778885245832053 Cost after epoch 16000: 0.6778083142928875 Cost after epoch 16100: 0.6777273841229372 Cost after epoch 16200: 0.6776457243892738 Cost after epoch 16300: 0.6775633252779312 Cost after epoch 16400: 0.6774801768431217 Cost after epoch 16500: 0.677396269005277 Cost after epoch 16600: 0.6773115915490696 Cost after epoch 16700: 0.6772261341214078 Cost after epoch 16800: 0.6771398862294105 Cost after epoch 16900: 0.6770528372383605 Cost after epoch 17000: 0.6769649763696333 Cost after epoch 17100: 0.6768762926986043 Cost after epoch 17200: 0.6767867751525354 Cost after epoch 17300: 0.6766964125084369 Cost after epoch 17400: 0.6766051933909103 Cost after epoch 17500: 0.6765131062699663 Cost after epoch 17600: 0.6764201394588243 Cost after epoch 17700: 0.6763262811116885 Cost after epoch 17800: 0.676231519221503 Cost after epoch 17900: 0.676135841617688 Cost after epoch 18000: 0.676039235963852 Cost after epoch 18100: 0.6759416897554879 Cost after epoch 18200: 0.6758431903176457 Cost after epoch 18300: 0.6757437248025878 Cost after epoch 18400: 0.6756432801874246 Cost after epoch 18500: 0.6755418432717311 Cost after epoch 18600: 0.6754394006751467 Cost after epoch 18700: 0.6753359388349561 Cost after epoch 18800: 0.675231444003654 Cost after epoch 18900: 0.6751259022464926 Cost after epoch 19000: 0.6750192994390145 Cost after epoch 19100: 0.67491162126457 Cost after epoch 19200: 0.6748028532118195 Cost after epoch 19300: 0.6746929805722235 Cost after epoch 19400: 0.6745819884375189 Cost after epoch 19500: 0.6744698616971845 Cost after epoch 19600: 0.6743565850358942 Cost after epoch 19700: 0.6742421429309623 Cost after epoch 19800: 0.6741265196497771 Cost after epoch 19900: 0.6740096992472298 ************************************************************ Elapsed time for the training is: 12.11003589630127 Final Accuracy is: 55.539358600583085% . learning_rates = [0.1, 0.03, 0.01, 0.003, 0.001] for lr in learning_rates: parameters, costs, accuracy = L_layers_model(X, Y, [4, 10, 7, 5, 1], lr, 5000) plot_cost(costs, accuracy, lr, 5000) . Number of Epochs: 5000 Learning rate: 0.1 ************************************************************ Cost after epoch 0: 0.9058177931945443 Cost after epoch 100: 0.6816320197542468 Cost after epoch 200: 0.673909212592033 Cost after epoch 300: 0.6522069285568555 Cost after epoch 400: 0.5888618892479032 Cost after epoch 500: 0.4518685625192162 Cost after epoch 600: 0.2796741364147254 Cost after epoch 700: 0.17375741377744647 Cost after epoch 800: 0.11627097451049986 Cost after epoch 900: 0.0837217346892617 Cost after epoch 1000: 0.06373891836540552 Cost after epoch 1100: 0.05061540002131189 Cost after epoch 1200: 0.04151712268665393 Cost after epoch 1300: 0.03492354485515207 Cost after epoch 1400: 0.02996877096079197 Cost after epoch 1500: 0.026133143562655503 Cost after epoch 1600: 0.023089940323251044 Cost after epoch 1700: 0.02062527182878535 Cost after epoch 1800: 0.018594153921656975 Cost after epoch 1900: 0.01689531047598496 Cost after epoch 2000: 0.015456125670167534 Cost after epoch 2100: 0.014223330443845343 Cost after epoch 2200: 0.013157052307864814 Cost after epoch 2300: 0.012226905512098311 Cost after epoch 2400: 0.011409357371449857 Cost after epoch 2500: 0.010685915177303969 Cost after epoch 2600: 0.010041853944958283 Cost after epoch 2700: 0.009465308339388762 Cost after epoch 2800: 0.008946614329084985 Cost after epoch 2900: 0.008477824772305057 Cost after epoch 3000: 0.008052347847131902 Cost after epoch 3100: 0.007664673412625465 Cost after epoch 3200: 0.007310163168105362 Cost after epoch 3300: 0.006984887746312786 Cost after epoch 3400: 0.006685498817459167 Cost after epoch 3500: 0.006409127664881477 Cost after epoch 3600: 0.0061533040302803495 Cost after epoch 3700: 0.00591589065880269 Cost after epoch 3800: 0.0056950301294079065 Cost after epoch 3900: 0.005489101385630174 Cost after epoch 4000: 0.005296683986844817 Cost after epoch 4100: 0.005116528547804123 Cost after epoch 4200: 0.004947532169919341 Cost after epoch 4300: 0.00478871792253632 Cost after epoch 4400: 0.004639217627807828 Cost after epoch 4500: 0.004498257353905016 Cost after epoch 4600: 0.004365145139150257 Cost after epoch 4700: 0.004239260562155862 Cost after epoch 4800: 0.004120045846107555 Cost after epoch 4900: 0.004006998243344211 ************************************************************ Elapsed time for the training is: 3.0467770099639893 Final Accuracy is: 100.0% . Number of Epochs: 5000 Learning rate: 0.03 ************************************************************ Cost after epoch 0: 0.9058177931945443 Cost after epoch 100: 0.6879902811137586 Cost after epoch 200: 0.6835155306434567 Cost after epoch 300: 0.6821292731141158 Cost after epoch 400: 0.6805452381945335 Cost after epoch 500: 0.6785802955136799 Cost after epoch 600: 0.6760421519357027 Cost after epoch 700: 0.6726478806428381 Cost after epoch 800: 0.6679750742153945 Cost after epoch 900: 0.6614052754606485 Cost after epoch 1000: 0.6520883792458579 Cost after epoch 1100: 0.6389878433969111 Cost after epoch 1200: 0.6210541883975376 Cost after epoch 1300: 0.597506712489762 Cost after epoch 1400: 0.5678928862044584 Cost after epoch 1500: 0.531431556110068 Cost after epoch 1600: 0.4865337607706746 Cost after epoch 1700: 0.4304955272318302 Cost after epoch 1800: 0.37394067943288956 Cost after epoch 1900: 0.32340325525249114 Cost after epoch 2000: 0.2786212125398061 Cost after epoch 2100: 0.24025518163986637 Cost after epoch 2200: 0.20799539991279006 Cost after epoch 2300: 0.1811291611601508 Cost after epoch 2400: 0.15884187580832437 Cost after epoch 2500: 0.14033603568522593 Cost after epoch 2600: 0.1249008218185396 Cost after epoch 2700: 0.11194055978764361 Cost after epoch 2800: 0.1009747033212215 Cost after epoch 2900: 0.09162358177308366 Cost after epoch 3000: 0.08358955749612734 Cost after epoch 3100: 0.07663891061854375 Cost after epoch 3200: 0.07058675749085568 Cost after epoch 3300: 0.06528543396692688 Cost after epoch 3400: 0.06061586592392654 Cost after epoch 3500: 0.056481207719319915 Cost after epoch 3600: 0.052802108269355094 Cost after epoch 3700: 0.04951312992934797 Cost after epoch 3800: 0.04655999392748081 Cost after epoch 3900: 0.043897432283548775 Cost after epoch 4000: 0.04148749534310359 Cost after epoch 4100: 0.03929820813039966 Cost after epoch 4200: 0.03730249736860113 Cost after epoch 4300: 0.03547733043722252 Cost after epoch 4400: 0.03380302131716599 Cost after epoch 4500: 0.03226266872096734 Cost after epoch 4600: 0.03084169927023349 Cost after epoch 4700: 0.029527494461679783 Cost after epoch 4800: 0.02830908471296888 Cost after epoch 4900: 0.027176897317367377 ************************************************************ Elapsed time for the training is: 3.0487558841705322 Final Accuracy is: 100.0% . Number of Epochs: 5000 Learning rate: 0.01 ************************************************************ Cost after epoch 0: 0.9058177931945443 Cost after epoch 100: 0.7416651940612006 Cost after epoch 200: 0.6988895558535142 Cost after epoch 300: 0.6880868594940236 Cost after epoch 400: 0.6851264494668848 Cost after epoch 500: 0.6840887630183584 Cost after epoch 600: 0.6835192452830169 Cost after epoch 700: 0.6830541932575773 Cost after epoch 800: 0.6825998950578858 Cost after epoch 900: 0.6821299747718844 Cost after epoch 1000: 0.6816347270113869 Cost after epoch 1100: 0.6811083212439397 Cost after epoch 1200: 0.6805455126102614 Cost after epoch 1300: 0.6799407417682292 Cost after epoch 1400: 0.6792878274496066 Cost after epoch 1500: 0.6785797961681013 Cost after epoch 1600: 0.6778087323816884 Cost after epoch 1700: 0.6769656182772041 Cost after epoch 1800: 0.676040154406026 Cost after epoch 1900: 0.6750205586552179 Cost after epoch 2000: 0.6738933438004958 Cost after epoch 2100: 0.6726430764618709 Cost after epoch 2200: 0.6712521237113623 Cost after epoch 2300: 0.6697003983027446 Cost after epoch 2400: 0.6679651196643991 Cost after epoch 2500: 0.6660206152133459 Cost after epoch 2600: 0.6638381943848862 Cost after epoch 2700: 0.6613861342597688 Cost after epoch 2800: 0.6586298179336766 Cost after epoch 2900: 0.6555320612743084 Cost after epoch 3000: 0.6520536480191028 Cost after epoch 3100: 0.6481540695579566 Cost after epoch 3200: 0.6437924453479001 Cost after epoch 3300: 0.6389286015042772 Cost after epoch 3400: 0.6335243175421393 Cost after epoch 3500: 0.627544774591391 Cost after epoch 3600: 0.6209601321970879 Cost after epoch 3700: 0.6137467900701311 Cost after epoch 3800: 0.6058874306507877 Cost after epoch 3900: 0.597369090579348 Cost after epoch 4000: 0.5881792166354814 Cost after epoch 4100: 0.578300136383801 Cost after epoch 4200: 0.5677041037406363 Cost after epoch 4300: 0.5563520300667314 Cost after epoch 4400: 0.5441953297216566 Cost after epoch 4500: 0.5311775685011918 Cost after epoch 4600: 0.5172329418382869 Cost after epoch 4700: 0.502277128480533 Cost after epoch 4800: 0.4861844333090627 Cost after epoch 4900: 0.46876617739606424 ************************************************************ Elapsed time for the training is: 2.9659767150878906 Final Accuracy is: 93.5131195335277% . Number of Epochs: 5000 Learning rate: 0.003 ************************************************************ Cost after epoch 0: 0.9058177931945443 Cost after epoch 100: 0.8334699800651983 Cost after epoch 200: 0.7837777772736595 Cost after epoch 300: 0.7503057984067155 Cost after epoch 400: 0.7280247692066603 Cost after epoch 500: 0.7132842873454832 Cost after epoch 600: 0.7035533892115141 Cost after epoch 700: 0.6971247687990673 Cost after epoch 800: 0.6928646943119133 Cost after epoch 900: 0.6900266182032148 Cost after epoch 1000: 0.6881208962122407 Cost after epoch 1100: 0.6868268158800607 Cost after epoch 1200: 0.685934286793592 Cost after epoch 1300: 0.6853055576723719 Cost after epoch 1400: 0.6848501903633627 Cost after epoch 1500: 0.6845087221570676 Cost after epoch 1600: 0.6842419969786256 Cost after epoch 1700: 0.6840241904891875 Cost after epoch 1800: 0.6838382431783625 Cost after epoch 1900: 0.6836728652306202 Cost after epoch 2000: 0.6835205690864843 Cost after epoch 2100: 0.6833763751720006 Cost after epoch 2200: 0.6832369593458264 Cost after epoch 2300: 0.6831000906596434 Cost after epoch 2400: 0.6829642601978974 Cost after epoch 2500: 0.682828435840188 Cost after epoch 2600: 0.6826919000964119 Cost after epoch 2700: 0.6825541427953941 Cost after epoch 2800: 0.6824147900206398 Cost after epoch 2900: 0.6822735570125913 Cost after epoch 3000: 0.682130216924924 Cost after epoch 3100: 0.6819845800719997 Cost after epoch 3200: 0.6818364801200866 Cost after epoch 3300: 0.6816857648746767 Cost after epoch 3400: 0.6815322901095168 Cost after epoch 3500: 0.6813759154078426 Cost after epoch 3600: 0.6812165013337025 Cost after epoch 3700: 0.6810539074812925 Cost after epoch 3800: 0.6808879911025955 Cost after epoch 3900: 0.6807186061145482 Cost after epoch 4000: 0.680545602353853 Cost after epoch 4100: 0.6803688249918717 Cost after epoch 4200: 0.6801881140514214 Cost after epoch 4300: 0.6800033039867626 Cost after epoch 4400: 0.6798142233009742 Cost after epoch 4500: 0.6796206941834757 Cost after epoch 4600: 0.6794225321561222 Cost after epoch 4700: 0.6792195457200655 Cost after epoch 4800: 0.6790115359980767 Cost after epoch 4900: 0.678798296368684 ************************************************************ Elapsed time for the training is: 3.0319018363952637 Final Accuracy is: 55.539358600583085% . Number of Epochs: 5000 Learning rate: 0.001 ************************************************************ Cost after epoch 0: 0.9058177931945443 Cost after epoch 100: 0.8787549041993477 Cost after epoch 200: 0.8547431740102313 Cost after epoch 300: 0.8335061181210778 Cost after epoch 400: 0.8147756769903448 Cost after epoch 500: 0.7982963780085447 Cost after epoch 600: 0.7838282813510609 Cost after epoch 700: 0.7711488366590706 Cost after epoch 800: 0.7600538178984678 Cost after epoch 900: 0.7503575176048662 Cost after epoch 1000: 0.7418923764005263 Cost after epoch 1100: 0.7345082062697952 Cost after epoch 1200: 0.7280711424976305 Cost after epoch 1300: 0.7224624337600349 Cost after epoch 1400: 0.7175771554681254 Cost after epoch 1500: 0.7133229097438804 Cost after epoch 1600: 0.7096185570737341 Cost after epoch 1700: 0.7063930098693144 Cost after epoch 1800: 0.7035841066250511 Cost after epoch 1900: 0.7011375766798135 Cost after epoch 2000: 0.6990060992804858 Cost after epoch 2100: 0.6971484562347166 Cost after epoch 2200: 0.6955287745008499 Cost after epoch 2300: 0.6941158532322328 Cost after epoch 2400: 0.6928825687745086 Cost after epoch 2500: 0.6918053506741462 Cost after epoch 2600: 0.6908637217142981 Cost after epoch 2700: 0.6900398952148747 Cost after epoch 2800: 0.6893184232180706 Cost after epoch 2900: 0.6886858896571354 Cost after epoch 3000: 0.6881306431248971 Cost after epoch 3100: 0.6876425643854394 Cost after epoch 3200: 0.6872128642851072 Cost after epoch 3300: 0.686833908203912 Cost after epoch 3400: 0.6864990636376077 Cost after epoch 3500: 0.6862025679106114 Cost after epoch 3600: 0.6859394133896826 Cost after epoch 3700: 0.6857052478988434 Cost after epoch 3800: 0.6854962883295167 Cost after epoch 3900: 0.6853092456989909 Cost after epoch 4000: 0.6851412601381233 Cost after epoch 4100: 0.6849898444887379 Cost after epoch 4200: 0.6848528353654946 Cost after epoch 4300: 0.6847283506889218 Cost after epoch 4400: 0.6846147528284968 Cost after epoch 4500: 0.6845106166094906 Cost after epoch 4600: 0.6844147015369588 Cost after epoch 4700: 0.6843259276766852 Cost after epoch 4800: 0.6842433547077722 Cost after epoch 4900: 0.6841661637264429 ************************************************************ Elapsed time for the training is: 3.013139486312866 Final Accuracy is: 55.539358600583085% . From the above expermimentation, it seems that the best learning rates are 0.01 &amp; 0.003 that dont overfit the training dataset much . parameters, costs, accuracy = L_layers_model(X, Y, [4, 10, 7, 5, 1], 0.003, 20000) plot_cost(costs, accuracy, 0.003, 20000) . Number of Epochs: 20000 Learning rate: 0.003 ************************************************************ Cost after epoch 0: 0.9058177931945443 Cost after epoch 100: 0.8334699800651983 Cost after epoch 200: 0.7837777772736595 Cost after epoch 300: 0.7503057984067155 Cost after epoch 400: 0.7280247692066603 Cost after epoch 500: 0.7132842873454832 Cost after epoch 600: 0.7035533892115141 Cost after epoch 700: 0.6971247687990673 Cost after epoch 800: 0.6928646943119133 Cost after epoch 900: 0.6900266182032148 Cost after epoch 1000: 0.6881208962122407 Cost after epoch 1100: 0.6868268158800607 Cost after epoch 1200: 0.685934286793592 Cost after epoch 1300: 0.6853055576723719 Cost after epoch 1400: 0.6848501903633627 Cost after epoch 1500: 0.6845087221570676 Cost after epoch 1600: 0.6842419969786256 Cost after epoch 1700: 0.6840241904891875 Cost after epoch 1800: 0.6838382431783625 Cost after epoch 1900: 0.6836728652306202 Cost after epoch 2000: 0.6835205690864843 Cost after epoch 2100: 0.6833763751720006 Cost after epoch 2200: 0.6832369593458264 Cost after epoch 2300: 0.6831000906596434 Cost after epoch 2400: 0.6829642601978974 Cost after epoch 2500: 0.682828435840188 Cost after epoch 2600: 0.6826919000964119 Cost after epoch 2700: 0.6825541427953941 Cost after epoch 2800: 0.6824147900206398 Cost after epoch 2900: 0.6822735570125913 Cost after epoch 3000: 0.682130216924924 Cost after epoch 3100: 0.6819845800719997 Cost after epoch 3200: 0.6818364801200866 Cost after epoch 3300: 0.6816857648746767 Cost after epoch 3400: 0.6815322901095168 Cost after epoch 3500: 0.6813759154078426 Cost after epoch 3600: 0.6812165013337025 Cost after epoch 3700: 0.6810539074812925 Cost after epoch 3800: 0.6808879911025955 Cost after epoch 3900: 0.6807186061145482 Cost after epoch 4000: 0.680545602353853 Cost after epoch 4100: 0.6803688249918717 Cost after epoch 4200: 0.6801881140514214 Cost after epoch 4300: 0.6800033039867626 Cost after epoch 4400: 0.6798142233009742 Cost after epoch 4500: 0.6796206941834757 Cost after epoch 4600: 0.6794225321561222 Cost after epoch 4700: 0.6792195457200655 Cost after epoch 4800: 0.6790115359980767 Cost after epoch 4900: 0.678798296368684 Cost after epoch 5000: 0.6785796120895958 Cost after epoch 5100: 0.6783552599086229 Cost after epoch 5200: 0.6781250076608285 Cost after epoch 5300: 0.6778886138509854 Cost after epoch 5400: 0.6776458272206809 Cost after epoch 5500: 0.6773963862996052 Cost after epoch 5600: 0.6771400189407214 Cost after epoch 5700: 0.6768764418391519 Cost after epoch 5800: 0.6766053600347601 Cost after epoch 5900: 0.6763264663985397 Cost after epoch 6000: 0.6760394411030657 Cost after epoch 6100: 0.6757439510774323 Cost after epoch 6200: 0.6754396494472639 Cost after epoch 6300: 0.6751261749605945 Cost after epoch 6400: 0.6748031514006169 Cost after epoch 6500: 0.6744701869865556 Cost after epoch 6600: 0.674126873764178 Cost after epoch 6700: 0.6737727869877627 Cost after epoch 6800: 0.6734074844956619 Cost after epoch 6900: 0.673030506081963 Cost after epoch 7000: 0.6726413728671323 Cost after epoch 7100: 0.6722395866709484 Cost after epoch 7200: 0.6718246293914805 Cost after epoch 7300: 0.671395962394344 Cost after epoch 7400: 0.6709530259169678 Cost after epoch 7500: 0.6704952384931328 Cost after epoch 7600: 0.6700219964035788 Cost after epoch 7700: 0.6695326731590301 Cost after epoch 7800: 0.6690266190225298 Cost after epoch 7900: 0.6685031605785183 Cost after epoch 8000: 0.6679616003565942 Cost after epoch 8100: 0.6674012165183678 Cost after epoch 8200: 0.6668212626162303 Cost after epoch 8300: 0.6662209674331829 Cost after epoch 8400: 0.665599534913104 Cost after epoch 8500: 0.6649561441909264 Cost after epoch 8600: 0.6642899497321436 Cost after epoch 8700: 0.6636000815908305 Cost after epoch 8800: 0.6628856457949323 Cost after epoch 8900: 0.6621457248669158 Cost after epoch 9000: 0.6613793784869919 Cost after epoch 9100: 0.6605856443049719 Cost after epoch 9200: 0.6597635389054632 Cost after epoch 9300: 0.658912058929509 Cost after epoch 9400: 0.6580301823540149 Cost after epoch 9500: 0.6571168699284263 Cost after epoch 9600: 0.6561710667661946 Cost after epoch 9700: 0.6551917040867444 Cost after epoch 9800: 0.654177701102021 Cost after epoch 9900: 0.6531279670404492 Cost after epoch 10000: 0.6520414033004142 Cost after epoch 10100: 0.6509169057253567 Cost after epoch 10200: 0.6497533669934006 Cost after epoch 10300: 0.6485496791161897 Cost after epoch 10400: 0.6473047360443095 Cost after epoch 10500: 0.6460174363801988 Cost after epoch 10600: 0.6446866862035368 Cost after epoch 10700: 0.643311402018259 Cost after epoch 10800: 0.6418905138339267 Cost after epoch 10900: 0.6404229683962269 Cost after epoch 11000: 0.6389077325808188 Cost after epoch 11100: 0.6373437969602519 Cost after epoch 11200: 0.6357301795439034 Cost after epoch 11300: 0.6340659296743908 Cost after epoch 11400: 0.6323501320395124 Cost after epoch 11500: 0.6305819107256266 Cost after epoch 11600: 0.6287604331966461 Cost after epoch 11700: 0.6268849140343483 Cost after epoch 11800: 0.6249546182250658 Cost after epoch 11900: 0.6229688637332627 Cost after epoch 12000: 0.6209270230759691 Cost after epoch 12100: 0.618828523617139 Cost after epoch 12200: 0.6166728463482339 Cost after epoch 12300: 0.6144595230108961 Cost after epoch 12400: 0.612188131532814 Cost after epoch 12500: 0.6098582898546684 Cost after epoch 12600: 0.60746964828286 Cost after epoch 12700: 0.605021880482162 Cost after epoch 12800: 0.6025146731352812 Cost after epoch 12900: 0.5999477142029014 Cost after epoch 13000: 0.5973206797112474 Cost after epoch 13100: 0.594633219154551 Cost after epoch 13200: 0.5918849399381789 Cost after epoch 13300: 0.589075391716544 Cost after epoch 13400: 0.5862040518332248 Cost after epoch 13500: 0.583270313180235 Cost after epoch 13600: 0.5802734755724851 Cost after epoch 13700: 0.5772127412255749 Cost after epoch 13800: 0.5740872142842048 Cost after epoch 13900: 0.5708959037665564 Cost after epoch 14000: 0.5676377289141064 Cost after epoch 14100: 0.5643115258198745 Cost after epoch 14200: 0.560916054310471 Cost after epoch 14300: 0.557450004280325 Cost after epoch 14400: 0.5539120009105624 Cost after epoch 14500: 0.550300608364119 Cost after epoch 14600: 0.5466143315856153 Cost after epoch 14700: 0.5428516157388238 Cost after epoch 14800: 0.5390108426019357 Cost after epoch 14900: 0.5350903229411846 Cost after epoch 15000: 0.5310882835335621 Cost after epoch 15100: 0.5270028471526105 Cost after epoch 15200: 0.5228320035248825 Cost after epoch 15300: 0.5185735690966125 Cost after epoch 15400: 0.5142251335689088 Cost after epoch 15500: 0.5097839918242205 Cost after epoch 15600: 0.5052470615285428 Cost after epoch 15700: 0.5006107901246194 Cost after epoch 15800: 0.49587106139704273 Cost after epoch 15900: 0.4910231232265495 Cost after epoch 16000: 0.48606157717631704 Cost after epoch 16100: 0.48098049986767866 Cost after epoch 16200: 0.47577380626004495 Cost after epoch 16300: 0.4704360086516645 Cost after epoch 16400: 0.4649635460409585 Cost after epoch 16500: 0.4593567959336221 Cost after epoch 16600: 0.4536226396295754 Cost after epoch 16700: 0.44777696573277853 Cost after epoch 16800: 0.4418459023535879 Cost after epoch 16900: 0.4358643840255495 Cost after epoch 17000: 0.42987153797725236 Cost after epoch 17100: 0.4239042040492061 Cost after epoch 17200: 0.41799127702675754 Cost after epoch 17300: 0.41215106555560865 Cost after epoch 17400: 0.4063919298331037 Cost after epoch 17500: 0.4007148755472017 Cost after epoch 17600: 0.3951165296848645 Cost after epoch 17700: 0.38959155222166425 Cost after epoch 17800: 0.38413422597906133 Cost after epoch 17900: 0.37873934458915265 Cost after epoch 18000: 0.3734026211891912 Cost after epoch 18100: 0.3681208114587039 Cost after epoch 18200: 0.3628916832084315 Cost after epoch 18300: 0.3577139119775007 Cost after epoch 18400: 0.3525869466739975 Cost after epoch 18500: 0.3475108682839764 Cost after epoch 18600: 0.34248625315253134 Cost after epoch 18700: 0.33751404638627036 Cost after epoch 18800: 0.3325954479969754 Cost after epoch 18900: 0.3277318130031083 Cost after epoch 19000: 0.32292456599683433 Cost after epoch 19100: 0.3181751302364935 Cost after epoch 19200: 0.313484870940645 Cost after epoch 19300: 0.3088550520789811 Cost after epoch 19400: 0.30428680558853843 Cost after epoch 19500: 0.29978111162985555 Cost after epoch 19600: 0.29533878827536025 Cost after epoch 19700: 0.290960488913185 Cost after epoch 19800: 0.2866467056548412 Cost after epoch 19900: 0.28239777713832287 ************************************************************ Elapsed time for the training is: 12.058766841888428 Final Accuracy is: 98.25072886297376% . Dealing with Overfitting . 1- Splitting the dataset into training set &amp; test set to check for overfitting . from sklearn.model_selection import train_test_split Xt = X.T.copy() Yt = Y.T.copy() X_train, X_test, Y_train, Y_test = train_test_split(Xt, Yt, test_size= 0.2) X_train = X_train.T Y_train = Y_train.T X_test = X_test.T Y_test = Y_test.T print(&quot;X_train shape: &quot;, X_train.shape) print(&quot;Y_train shape: &quot;, Y_train.shape) print(&quot;X_test shape: &quot;, X_test.shape) print(&quot;Y_test shape: &quot;, Y_test.shape) . X_train shape: (4, 1097) Y_train shape: (1, 1097) X_test shape: (4, 275) Y_test shape: (1, 275) . parameters_train, costs_train, accuracy_train = L_layers_model(X_train, Y_train, [4, 10, 7, 5, 1], 0.01, 10000) plot_cost(costs_train, accuracy_train, 0.01, 10000) . Number of Epochs: 10000 Learning rate: 0.01 ************************************************************ Cost after epoch 0: 0.8996300046921928 Cost after epoch 100: 0.7589762455800287 Cost after epoch 200: 0.7102639662371708 Cost after epoch 300: 0.6941070271101197 Cost after epoch 400: 0.6886433590145795 Cost after epoch 500: 0.6866513883792047 Cost after epoch 600: 0.6857892458253425 Cost after epoch 700: 0.6852936283946873 Cost after epoch 800: 0.6849132550377014 Cost after epoch 900: 0.6845642039893428 Cost after epoch 1000: 0.6842175688324243 Cost after epoch 1100: 0.6838625512010701 Cost after epoch 1200: 0.6834942634754908 Cost after epoch 1300: 0.6831096749470337 Cost after epoch 1400: 0.6827062510946886 Cost after epoch 1500: 0.6822814841157961 Cost after epoch 1600: 0.6818327171349454 Cost after epoch 1700: 0.6813570633926912 Cost after epoch 1800: 0.6808513540851164 Cost after epoch 1900: 0.6803120924882912 Cost after epoch 2000: 0.6797354066039267 Cost after epoch 2100: 0.6791169974194085 Cost after epoch 2200: 0.6784520815161496 Cost after epoch 2300: 0.6777353273674961 Cost after epoch 2400: 0.6769607849816925 Cost after epoch 2500: 0.6761218088312947 Cost after epoch 2600: 0.675210974378401 Cost after epoch 2700: 0.674219989017887 Cost after epoch 2800: 0.6731395989667316 Cost after epoch 2900: 0.6719594945668242 Cost after epoch 3000: 0.6706682176675046 Cost after epoch 3100: 0.6692530762090696 Cost after epoch 3200: 0.6677000727816219 Cost after epoch 3300: 0.6659938556389186 Cost after epoch 3400: 0.66411770213303 Cost after epoch 3500: 0.6620535453791484 Cost after epoch 3600: 0.6597820545973013 Cost after epoch 3700: 0.657282777403116 Cost after epoch 3800: 0.6545343479221277 Cost after epoch 3900: 0.6515147581669611 Cost after epoch 4000: 0.6482016829020844 Cost after epoch 4100: 0.6445728427889764 Cost after epoch 4200: 0.6406063902309553 Cost after epoch 4300: 0.6362813090729794 Cost after epoch 4400: 0.6315778309616848 Cost after epoch 4500: 0.6264778783524032 Cost after epoch 4600: 0.6209655281285746 Cost after epoch 4700: 0.6150274257848615 Cost after epoch 4800: 0.6086529687557505 Cost after epoch 4900: 0.6018339969832763 Cost after epoch 5000: 0.5945637759721256 Cost after epoch 5100: 0.5868352411429562 Cost after epoch 5200: 0.578638875503358 Cost after epoch 5300: 0.5699611028325786 Cost after epoch 5400: 0.5607838945454025 Cost after epoch 5500: 0.5510853198510877 Cost after epoch 5600: 0.5408402109674776 Cost after epoch 5700: 0.5300202608925986 Cost after epoch 5800: 0.5185928209786137 Cost after epoch 5900: 0.5065171187231338 Cost after epoch 6000: 0.4937365425878291 Cost after epoch 6100: 0.4801690040836301 Cost after epoch 6200: 0.4657133423024436 Cost after epoch 6300: 0.4503329368731093 Cost after epoch 6400: 0.4342771514727816 Cost after epoch 6500: 0.4181625540421013 Cost after epoch 6600: 0.4025105816758375 Cost after epoch 6700: 0.38742809468704004 Cost after epoch 6800: 0.37282729328062736 Cost after epoch 6900: 0.3586276849963431 Cost after epoch 7000: 0.3447976131479042 Cost after epoch 7100: 0.3313417782361371 Cost after epoch 7200: 0.31828370994402094 Cost after epoch 7300: 0.3056525247912511 Cost after epoch 7400: 0.29347442474778707 Cost after epoch 7500: 0.2817682349949066 Cost after epoch 7600: 0.2705440620596553 Cost after epoch 7700: 0.25980396729403554 Cost after epoch 7800: 0.24954358524511377 Cost after epoch 7900: 0.23975390023523266 Cost after epoch 8000: 0.23042275576444307 Cost after epoch 8100: 0.2215359618643844 Cost after epoch 8200: 0.21307803587400173 Cost after epoch 8300: 0.20503267786409712 Cost after epoch 8400: 0.1973830822354849 Cost after epoch 8500: 0.19011215959366543 Cost after epoch 8600: 0.18320271262571083 Cost after epoch 8700: 0.1766375871067885 Cost after epoch 8800: 0.17039980586786937 Cost after epoch 8900: 0.16447268725014835 Cost after epoch 9000: 0.1588399474115041 Cost after epoch 9100: 0.1534857857145806 Cost after epoch 9200: 0.14839495305271871 Cost after epoch 9300: 0.14355280372184626 Cost after epoch 9400: 0.1389453320562143 Cost after epoch 9500: 0.13455919543789677 Cost after epoch 9600: 0.13038172547643867 Cost after epoch 9700: 0.1264009291811539 Cost after epoch 9800: 0.12260548186562022 Cost after epoch 9900: 0.11898471337725819 ************************************************************ Elapsed time for the training is: 5.211627721786499 Final Accuracy is: 99.54421148587056% . parameters_test, costs_test, accuracy_test = L_layers_model(X_test, Y_test, [4, 10, 7, 5, 1], 0.01, 10000) plot_cost(costs_test, accuracy_test, 0.01, 10000) . Number of Epochs: 10000 Learning rate: 0.01 ************************************************************ Cost after epoch 0: 0.9305014440566515 Cost after epoch 100: 0.8714573561543504 Cost after epoch 200: 0.8253897454787105 Cost after epoch 300: 0.7898416011177477 Cost after epoch 400: 0.7626304784124494 Cost after epoch 500: 0.7419153956169339 Cost after epoch 600: 0.7262000246064937 Cost after epoch 700: 0.7142998774103417 Cost after epoch 800: 0.7052944950771561 Cost after epoch 900: 0.6984775975786219 Cost after epoch 1000: 0.6933117990456957 Cost after epoch 1100: 0.6893904190700164 Cost after epoch 1200: 0.6864066858203244 Cost after epoch 1300: 0.6841295901797287 Cost after epoch 1400: 0.6823852937226864 Cost after epoch 1500: 0.6810429778239475 Cost after epoch 1600: 0.6800041508600071 Cost after epoch 1700: 0.6791946033738617 Cost after epoch 1800: 0.6785583696692791 Cost after epoch 1900: 0.6780531999793133 Cost after epoch 2000: 0.6776471656847919 Cost after epoch 2100: 0.6773161128186123 Cost after epoch 2200: 0.6770417502551791 Cost after epoch 2300: 0.6768102128571588 Cost after epoch 2400: 0.6766109803048819 Cost after epoch 2500: 0.6764360625663305 Cost after epoch 2600: 0.6762793855040073 Cost after epoch 2700: 0.6761363269012644 Cost after epoch 2800: 0.6760033656937059 Cost after epoch 2900: 0.6758778165109461 Cost after epoch 3000: 0.6757576285889513 Cost after epoch 3100: 0.6756412333108806 Cost after epoch 3200: 0.6755274285248556 Cost after epoch 3300: 0.6754152907038614 Cost after epoch 3400: 0.6753041082032646 Cost after epoch 3500: 0.6751933305187012 Cost after epoch 3600: 0.6750825296878215 Cost after epoch 3700: 0.6749713709151618 Cost after epoch 3800: 0.6748595902061495 Cost after epoch 3900: 0.6747469773305707 Cost after epoch 4000: 0.6746333628402871 Cost after epoch 4100: 0.674518608172401 Cost after epoch 4200: 0.6744025981014228 Cost after epoch 4300: 0.674285234980339 Cost after epoch 4400: 0.6741664343443844 Cost after epoch 4500: 0.6740461215530883 Cost after epoch 4600: 0.6739242292235423 Cost after epoch 4700: 0.6738006952666894 Cost after epoch 4800: 0.6736754613832372 Cost after epoch 4900: 0.6735484719099012 Cost after epoch 5000: 0.673419672932657 Cost after epoch 5100: 0.673289011603475 Cost after epoch 5200: 0.6731564356120875 Cost after epoch 5300: 0.673021892775839 Cost after epoch 5400: 0.67288533071943 Cost after epoch 5500: 0.6727466966230521 Cost after epoch 5600: 0.6726059370225069 Cost after epoch 5700: 0.6724629976487926 Cost after epoch 5800: 0.6723178232976065 Cost after epoch 5900: 0.6721703577214744 Cost after epoch 6000: 0.6720205435389512 Cost after epoch 6100: 0.6718683221566456 Cost after epoch 6200: 0.6717136337008378 Cost after epoch 6300: 0.671556416956223 Cost after epoch 6400: 0.6713966093098981 Cost after epoch 6500: 0.6712341466991635 Cost after epoch 6600: 0.6710689635620476 Cost after epoch 6700: 0.6709009927897303 Cost after epoch 6800: 0.6707301656802337 Cost after epoch 6900: 0.6705564118929126 Cost after epoch 7000: 0.6703796594033848 Cost after epoch 7100: 0.6701998344586392 Cost after epoch 7200: 0.6700168615321239 Cost after epoch 7300: 0.6698306632786777 Cost after epoch 7400: 0.6696411604892014 Cost after epoch 7500: 0.6694482720450078 Cost after epoch 7600: 0.6692519148718069 Cost after epoch 7700: 0.6690520038933125 Cost after epoch 7800: 0.6688484519844637 Cost after epoch 7900: 0.6686411699242792 Cost after epoch 8000: 0.6684300663483662 Cost after epoch 8100: 0.6682150477011213 Cost after epoch 8200: 0.6679960181876667 Cost after epoch 8300: 0.6677728797255781 Cost after epoch 8400: 0.6675455318964608 Cost after epoch 8500: 0.6673138718974494 Cost after epoch 8600: 0.6670777944927013 Cost after epoch 8700: 0.6668371919649716 Cost after epoch 8800: 0.6665919540673585 Cost after epoch 8900: 0.6663419679753171 Cost after epoch 9000: 0.666087118239046 Cost after epoch 9100: 0.6658272867363594 Cost after epoch 9200: 0.6655623526261626 Cost after epoch 9300: 0.6652921923026571 Cost after epoch 9400: 0.6650166793504085 Cost after epoch 9500: 0.6647356845004159 Cost after epoch 9600: 0.6644490755873304 Cost after epoch 9700: 0.6641567175079749 Cost after epoch 9800: 0.6638584721813235 Cost after epoch 9900: 0.6635541985101067 ************************************************************ Elapsed time for the training is: 2.405806541442871 Final Accuracy is: 57.81818181818181% . parameters_train, costs_train, accuracy_train = L_layers_model(X_train, Y_train, [4, 10, 6, 1], 0.01, 10000) plot_cost(costs_train, accuracy_train, 0.01, 10000) . Number of Epochs: 10000 Learning rate: 0.01 ************************************************************ Cost after epoch 0: 0.7399319745162903 Cost after epoch 100: 0.6924390475741036 Cost after epoch 200: 0.6700828013819284 Cost after epoch 300: 0.6537267508821771 Cost after epoch 400: 0.6390546761445141 Cost after epoch 500: 0.6248785156614701 Cost after epoch 600: 0.6107179612276016 Cost after epoch 700: 0.596293353990341 Cost after epoch 800: 0.5814122974811856 Cost after epoch 900: 0.5659483347068631 Cost after epoch 1000: 0.5498468964910254 Cost after epoch 1100: 0.5331317058859114 Cost after epoch 1200: 0.5158895547492424 Cost after epoch 1300: 0.49823527456240013 Cost after epoch 1400: 0.48028446747839126 Cost after epoch 1500: 0.46214646466454695 Cost after epoch 1600: 0.4439257530836232 Cost after epoch 1700: 0.4257225957806516 Cost after epoch 1800: 0.40763348554361206 Cost after epoch 1900: 0.38975285680518706 Cost after epoch 2000: 0.37217465471278866 Cost after epoch 2100: 0.35499173885810925 Cost after epoch 2200: 0.33829221453472014 Cost after epoch 2300: 0.3221536963735968 Cost after epoch 2400: 0.30663828824123834 Cost after epoch 2500: 0.2917906806227268 Cost after epoch 2600: 0.2776393178689533 Cost after epoch 2700: 0.2641988946982586 Cost after epoch 2800: 0.25147274212511606 Cost after epoch 2900: 0.23945468455156665 Cost after epoch 3000: 0.2281305385548698 Cost after epoch 3100: 0.21747949833688388 Cost after epoch 3200: 0.20747552300600108 Cost after epoch 3300: 0.19808871667819905 Cost after epoch 3400: 0.18928663473749846 Cost after epoch 3500: 0.18103544708949515 Cost after epoch 3600: 0.17330091387580046 Cost after epoch 3700: 0.16604915886584962 Cost after epoch 3800: 0.1592472491916494 Cost after epoch 3900: 0.15286360414914002 Cost after epoch 4000: 0.14686826179227566 Cost after epoch 4100: 0.14123303264995488 Cost after epoch 4200: 0.13593156747089366 Cost after epoch 4300: 0.13093936208276621 Cost after epoch 4400: 0.12623371823588525 Cost after epoch 4500: 0.12179367525251275 Cost after epoch 4600: 0.11759992371104952 Cost after epoch 4700: 0.11363470937566038 Cost after epoch 4800: 0.10988173314260384 Cost after epoch 4900: 0.1063260508598966 Cost after epoch 5000: 0.10295397540796186 Cost after epoch 5100: 0.09975298232764919 Cost after epoch 5200: 0.09671161948534075 Cost after epoch 5300: 0.09381942072823564 Cost after epoch 5400: 0.09106682317653167 Cost after epoch 5500: 0.08844508769962399 Cost after epoch 5600: 0.0859462222036699 Cost after epoch 5700: 0.08356290757971604 Cost after epoch 5800: 0.08128842647220128 Cost after epoch 5900: 0.07911659536174885 Cost after epoch 6000: 0.07704170074324956 Cost after epoch 6100: 0.07505844035533385 Cost after epoch 6200: 0.07316187043315507 Cost after epoch 6300: 0.0713473597945972 Cost after epoch 6400: 0.06961055124803674 Cost after epoch 6500: 0.06794733038004051 Cost after epoch 6600: 0.06635380132047956 Cost after epoch 6700: 0.06482626867264302 Cost after epoch 6800: 0.0633612245039052 Cost after epoch 6900: 0.061955339154436226 Cost after epoch 7000: 0.06060545463765838 Cost after epoch 7100: 0.05930857954708637 Cost after epoch 7200: 0.0580618846034488 Cost after epoch 7300: 0.056862698224602236 Cost after epoch 7400: 0.05570850173822724 Cost after epoch 7500: 0.05459692405708506 Cost after epoch 7600: 0.05352573578656478 Cost after epoch 7700: 0.0524928428337126 Cost after epoch 7800: 0.05149627964255738 Cost after epoch 7900: 0.05053420220238318 Cost after epoch 8000: 0.04960488097412161 Cost after epoch 8100: 0.048706693864431445 Cost after epoch 8200: 0.04783811935442836 Cost after epoch 8300: 0.04699772986541201 Cost after epoch 8400: 0.046184185420443474 Cost after epoch 8500: 0.04539622763997111 Cost after epoch 8600: 0.04463267409258222 Cost after epoch 8700: 0.043892413008459114 Cost after epoch 8800: 0.043174398352933935 Cost after epoch 8900: 0.04247764525021753 Cost after epoch 9000: 0.041801225742396315 Cost after epoch 9100: 0.04114426486566177 Cost after epoch 9200: 0.0405059370240164 Cost after epoch 9300: 0.039885462640021935 Cost after epoch 9400: 0.03928210506221963 Cost after epoch 9500: 0.038695167709428054 Cost after epoch 9600: 0.03812399143303111 Cost after epoch 9700: 0.03756795207947605 Cost after epoch 9800: 0.037026458236410154 Cost after epoch 9900: 0.03649894914712865 ************************************************************ Elapsed time for the training is: 3.9827988147735596 Final Accuracy is: 100.0% . parameters_test, costs_test, accuracy_test = L_layers_model(X_test, Y_test, [4, 10, 6, 1], 0.01, 10000) plot_cost(costs_test, accuracy_test, 0.01, 10000) . Number of Epochs: 10000 Learning rate: 0.01 ************************************************************ Cost after epoch 0: 0.7407218996768389 Cost after epoch 100: 0.7195591113379605 Cost after epoch 200: 0.7041275862531886 Cost after epoch 300: 0.6926176067321209 Cost after epoch 400: 0.6837750217324892 Cost after epoch 500: 0.6767431907099846 Cost after epoch 600: 0.6709416156861331 Cost after epoch 700: 0.6659783231887184 Cost after epoch 800: 0.6615887700535014 Cost after epoch 900: 0.6575940835149235 Cost after epoch 1000: 0.6538728384094368 Cost after epoch 1100: 0.6503421021055003 Cost after epoch 1200: 0.6469447451185957 Cost after epoch 1300: 0.6436409588703522 Cost after epoch 1400: 0.640402588358482 Cost after epoch 1500: 0.637209345096768 Cost after epoch 1600: 0.6340462752486352 Cost after epoch 1700: 0.6309020656544869 Cost after epoch 1800: 0.6277679093756127 Cost after epoch 1900: 0.6246367451135316 Cost after epoch 2000: 0.6215027467385508 Cost after epoch 2100: 0.6183609804436302 Cost after epoch 2200: 0.6152071745812467 Cost after epoch 2300: 0.6120375656071118 Cost after epoch 2400: 0.6088487957861349 Cost after epoch 2500: 0.6056378464386633 Cost after epoch 2600: 0.6024019958684901 Cost after epoch 2700: 0.5991387946185188 Cost after epoch 2800: 0.5958460529470017 Cost after epoch 2900: 0.5925218368125756 Cost after epoch 3000: 0.5891644694785251 Cost after epoch 3100: 0.5857725362946109 Cost after epoch 3200: 0.5823448904388666 Cost after epoch 3300: 0.5788806575237917 Cost after epoch 3400: 0.5753792370947913 Cost after epoch 3500: 0.5718402992604403 Cost after epoch 3600: 0.5682637750593852 Cost after epoch 3700: 0.5646498397208676 Cost after epoch 3800: 0.5609988887049145 Cost after epoch 3900: 0.55731150725305 Cost after epoch 4000: 0.5535884350310935 Cost after epoch 4100: 0.5498305281609651 Cost after epoch 4200: 0.5460387213779144 Cost after epoch 4300: 0.5422139931133506 Cost after epoch 4400: 0.538357335965883 Cost after epoch 4500: 0.5344697343462455 Cost after epoch 4600: 0.530552150200307 Cost after epoch 4700: 0.5266055167977208 Cost after epoch 4800: 0.5226307397794349 Cost after epoch 4900: 0.5186287040933234 Cost after epoch 5000: 0.5146002851557318 Cost after epoch 5100: 0.510546362539088 Cost after epoch 5200: 0.5064678346424499 Cost after epoch 5300: 0.5023656330766832 Cost after epoch 5400: 0.4982407358161314 Cost after epoch 5500: 0.49409417847731096 Cost after epoch 5600: 0.48992706334622366 Cost after epoch 5700: 0.4857405659732034 Cost after epoch 5800: 0.48153593928801475 Cost after epoch 5900: 0.47731451526960034 Cost after epoch 6000: 0.4730777042521559 Cost after epoch 6100: 0.46882699198104133 Cost after epoch 6200: 0.4645639345647525 Cost after epoch 6300: 0.4602901515131666 Cost after epoch 6400: 0.45600731711042425 Cost after epoch 6500: 0.4517171504384489 Cost after epoch 6600: 0.44742140443369893 Cost after epoch 6700: 0.4431218544120258 Cost after epoch 6800: 0.4388202865220267 Cost after epoch 6900: 0.4345184865777923 Cost after epoch 7000: 0.43021822967547446 Cost after epoch 7100: 0.425921270919359 Cost after epoch 7200: 0.4216293374822618 Cost after epoch 7300: 0.4173441221152446 Cost after epoch 7400: 0.41306727811617605 Cost after epoch 7500: 0.4088004156765789 Cost after epoch 7600: 0.40454509945869427 Cost after epoch 7700: 0.40030284721259995 Cost after epoch 7800: 0.3960751292255595 Cost after epoch 7900: 0.3918633683987316 Cost after epoch 8000: 0.3876689407646535 Cost after epoch 8100: 0.3834931762868126 Cost after epoch 8200: 0.3793373598150058 Cost after epoch 8300: 0.3752027321028712 Cost after epoch 8400: 0.37109049082398654 Cost after epoch 8500: 0.367001791548448 Cost after epoch 8600: 0.3629377486619576 Cost after epoch 8700: 0.358899436223998 Cost after epoch 8800: 0.35488788877093624 Cost after epoch 8900: 0.3509041020744612 Cost after epoch 9000: 0.3469490338663186 Cost after epoch 9100: 0.34302360453764624 Cost after epoch 9200: 0.3391286978160941 Cost after epoch 9300: 0.3352651614171317 Cost after epoch 9400: 0.3314338076582761 Cost after epoch 9500: 0.3276354140172451 Cost after epoch 9600: 0.32387072360807906 Cost after epoch 9700: 0.3201404455439525 Cost after epoch 9800: 0.31644525515255006 Cost after epoch 9900: 0.3127857940102363 ************************************************************ Elapsed time for the training is: 1.8522896766662598 Final Accuracy is: 99.63636363636364% . . This last example will be compared to the same model with the same hyperparameters but with dropout applied to see if we will succeed to overcome overfitting of the trainig set . . parameters_train, costs_train, accuracy_train = L_layers_model(X_train, Y_train, [4, 4, 5, 1], 0.01, 10000) plot_cost(costs_train, accuracy_train, 0.01, 10000) . Number of Epochs: 10000 Learning rate: 0.01 ************************************************************ Cost after epoch 0: 0.8230434382594065 Cost after epoch 100: 0.7730753314080838 Cost after epoch 200: 0.7484390149364067 Cost after epoch 300: 0.7341418486558031 Cost after epoch 400: 0.7243001641184288 Cost after epoch 500: 0.7165950135072124 Cost after epoch 600: 0.7100529920767523 Cost after epoch 700: 0.7042048149392519 Cost after epoch 800: 0.6987678653171386 Cost after epoch 900: 0.6935275760934737 Cost after epoch 1000: 0.6883072022566008 Cost after epoch 1100: 0.6829987964548083 Cost after epoch 1200: 0.6776235129719038 Cost after epoch 1300: 0.6723233483905824 Cost after epoch 1400: 0.667245199282351 Cost after epoch 1500: 0.6624526531365461 Cost after epoch 1600: 0.6579367142446559 Cost after epoch 1700: 0.6536541082515633 Cost after epoch 1800: 0.6495484892709283 Cost after epoch 1900: 0.6455599482647718 Cost after epoch 2000: 0.6416315044286579 Cost after epoch 2100: 0.6377136573977471 Cost after epoch 2200: 0.6337661026150797 Cost after epoch 2300: 0.6297569126305718 Cost after epoch 2400: 0.6256604369074017 Cost after epoch 2500: 0.6214551224576801 Cost after epoch 2600: 0.6171218880420994 Cost after epoch 2700: 0.6126431423938786 Cost after epoch 2800: 0.6080022613601656 Cost after epoch 2900: 0.6031833000499169 Cost after epoch 3000: 0.5981707914539679 Cost after epoch 3100: 0.5929495758976915 Cost after epoch 3200: 0.5875046682709535 Cost after epoch 3300: 0.5818211922470711 Cost after epoch 3400: 0.5758844129107459 Cost after epoch 3500: 0.5696799202045177 Cost after epoch 3600: 0.5631940833778243 Cost after epoch 3700: 0.5564149842163161 Cost after epoch 3800: 0.5493340224022907 Cost after epoch 3900: 0.541948067502748 Cost after epoch 4000: 0.5342613248914376 Cost after epoch 4100: 0.5262855201353299 Cost after epoch 4200: 0.5180379266038236 Cost after epoch 4300: 0.5095395072395026 Cost after epoch 4400: 0.5008167759651931 Cost after epoch 4500: 0.4919072604748985 Cost after epoch 4600: 0.48286267001523636 Cost after epoch 4700: 0.4737436963536191 Cost after epoch 4800: 0.46460782967956765 Cost after epoch 4900: 0.45549873004002495 Cost after epoch 5000: 0.4464437279926654 Cost after epoch 5100: 0.43745800537260054 Cost after epoch 5200: 0.4285501768858519 Cost after epoch 5300: 0.41972615790351026 Cost after epoch 5400: 0.4109910989389908 Cost after epoch 5500: 0.40235017288584274 Cost after epoch 5600: 0.393808847415578 Cost after epoch 5700: 0.38537295182906134 Cost after epoch 5800: 0.37704865918066 Cost after epoch 5900: 0.36884241989543703 Cost after epoch 6000: 0.36076084024745925 Cost after epoch 6100: 0.3528104775817867 Cost after epoch 6200: 0.3449975297261829 Cost after epoch 6300: 0.33732743299397 Cost after epoch 6400: 0.32980443600936615 Cost after epoch 6500: 0.32243125137705114 Cost after epoch 6600: 0.3152088754690801 Cost after epoch 6700: 0.3081366110295763 Cost after epoch 6800: 0.3012122612982025 Cost after epoch 6900: 0.29443242405276954 Cost after epoch 7000: 0.28779281213511027 Cost after epoch 7100: 0.2812885516184396 Cost after epoch 7200: 0.27491443942596727 Cost after epoch 7300: 0.2686651648645929 Cost after epoch 7400: 0.262535510036657 Cost after epoch 7500: 0.25652054435242133 Cost after epoch 7600: 0.2506158217721548 Cost after epoch 7700: 0.2448175786282376 Cost after epoch 7800: 0.2391229169829537 Cost after epoch 7900: 0.23352994605480276 Cost after epoch 8000: 0.228037846224061 Cost after epoch 8100: 0.22264682130828747 Cost after epoch 8200: 0.2173579187361512 Cost after epoch 8300: 0.21217272325471068 Cost after epoch 8400: 0.20709296101519323 Cost after epoch 8500: 0.20212007549143685 Cost after epoch 8600: 0.19725484289481365 Cost after epoch 8700: 0.19249707720907397 Cost after epoch 8800: 0.18784543870614845 Cost after epoch 8900: 0.18329731904813365 Cost after epoch 9000: 0.17884874947112822 Cost after epoch 9100: 0.17449428507799367 Cost after epoch 9200: 0.17022687768828806 Cost after epoch 9300: 0.16603788781327558 Cost after epoch 9400: 0.16191763130999068 Cost after epoch 9500: 0.15785715921567395 Cost after epoch 9600: 0.1538519183157566 Cost after epoch 9700: 0.1499065005113642 Cost after epoch 9800: 0.14603669087015678 Cost after epoch 9900: 0.14226473842294074 ************************************************************ Elapsed time for the training is: 2.948514699935913 Final Accuracy is: 97.90337283500456% . parameters_test, costs_test, accuracy_test = L_layers_model(X_test, Y_test, [4, 4, 5, 1], 0.01, 10000) plot_cost(costs_test, accuracy_test, 0.01, 10000) . Number of Epochs: 10000 Learning rate: 0.01 ************************************************************ Cost after epoch 0: 0.8292199627993843 Cost after epoch 100: 0.8091168122635036 Cost after epoch 200: 0.792866843980141 Cost after epoch 300: 0.7796896270652756 Cost after epoch 400: 0.7689525971790507 Cost after epoch 500: 0.7601484853200209 Cost after epoch 600: 0.7528736255368409 Cost after epoch 700: 0.746808535840789 Cost after epoch 800: 0.7417012909040721 Cost after epoch 900: 0.7373536999760733 Cost after epoch 1000: 0.7336100470824676 Cost after epoch 1100: 0.7303480466974648 Cost after epoch 1200: 0.7274716499233467 Cost after epoch 1300: 0.7249053613619677 Cost after epoch 1400: 0.7225897702418947 Cost after epoch 1500: 0.7204780471717971 Cost after epoch 1600: 0.7185332031969469 Cost after epoch 1700: 0.7167259476668785 Cost after epoch 1800: 0.7150330149445682 Cost after epoch 1900: 0.7134358574354823 Cost after epoch 2000: 0.7119196244868686 Cost after epoch 2100: 0.7104723642426349 Cost after epoch 2200: 0.7090843993560475 Cost after epoch 2300: 0.7077478382900769 Cost after epoch 2400: 0.7064561923899908 Cost after epoch 2500: 0.7052040755001241 Cost after epoch 2600: 0.7039869680229055 Cost after epoch 2700: 0.7028010313053769 Cost after epoch 2800: 0.7016429613399819 Cost after epoch 2900: 0.7005098731804298 Cost after epoch 3000: 0.6993992093543359 Cost after epoch 3100: 0.6983086670222662 Cost after epoch 3200: 0.6972361397812622 Cost after epoch 3300: 0.696179670912863 Cost after epoch 3400: 0.6951374155882021 Cost after epoch 3500: 0.6941076101110103 Cost after epoch 3600: 0.6930885467392744 Cost after epoch 3700: 0.6920785530069287 Cost after epoch 3800: 0.691075974792276 Cost after epoch 3900: 0.69007916266986 Cost after epoch 4000: 0.6890864613541009 Cost after epoch 4100: 0.6880962023099634 Cost after epoch 4200: 0.6871066998783691 Cost after epoch 4300: 0.6861162515467278 Cost after epoch 4400: 0.6851231432842785 Cost after epoch 4500: 0.6841256611412819 Cost after epoch 4600: 0.6831221105447226 Cost after epoch 4700: 0.6821108448488188 Cost after epoch 4800: 0.6810903046216771 Cost after epoch 4900: 0.680059068741993 Cost after epoch 5000: 0.6790159174924324 Cost after epoch 5100: 0.677959906331608 Cost after epoch 5200: 0.6768904468419054 Cost after epoch 5300: 0.6758073885919139 Cost after epoch 5400: 0.6747110926982296 Cost after epoch 5500: 0.6736024854357433 Cost after epoch 5600: 0.6724830793212188 Cost after epoch 5700: 0.6713549507041747 Cost after epoch 5800: 0.6702206676471697 Cost after epoch 5900: 0.6690831694541618 Cost after epoch 6000: 0.6679456081199845 Cost after epoch 6100: 0.6668111698176354 Cost after epoch 6200: 0.6656828988293328 Cost after epoch 6300: 0.6645635455832413 Cost after epoch 6400: 0.6634554549221284 Cost after epoch 6500: 0.6623605022656245 Cost after epoch 6600: 0.6612800765535719 Cost after epoch 6700: 0.6602151020450219 Cost after epoch 6800: 0.659166087370494 Cost after epoch 6900: 0.6581331896611411 Cost after epoch 7000: 0.6571162832561349 Cost after epoch 7100: 0.6561150253003468 Cost after epoch 7200: 0.6551289135288472 Cost after epoch 7300: 0.6541573340855498 Cost after epoch 7400: 0.6531995990835158 Cost after epoch 7500: 0.6522549747764603 Cost after epoch 7600: 0.6513227017993248 Cost after epoch 7700: 0.6504020091161553 Cost after epoch 7800: 0.6494921232399417 Cost after epoch 7900: 0.6485922740814474 Cost after epoch 8000: 0.6477016985248605 Cost after epoch 8100: 0.6468196425680999 Cost after epoch 8200: 0.6459453626323836 Cost after epoch 8300: 0.6450781264512083 Cost after epoch 8400: 0.6442172137958917 Cost after epoch 8500: 0.6433619171806281 Cost after epoch 8600: 0.6425115426094886 Cost after epoch 8700: 0.6416654103748453 Cost after epoch 8800: 0.6408228558852704 Cost after epoch 8900: 0.6399832304854902 Cost after epoch 9000: 0.639145902226678 Cost after epoch 9100: 0.6383102565483609 Cost after epoch 9200: 0.6374756968403691 Cost after epoch 9300: 0.6366416448623609 Cost after epoch 9400: 0.635807541007856 Cost after epoch 9500: 0.6349728444084168 Cost after epoch 9600: 0.6341370328810133 Cost after epoch 9700: 0.6332996027274593 Cost after epoch 9800: 0.6324600683990649 Cost after epoch 9900: 0.6316179620424353 ************************************************************ Elapsed time for the training is: 1.5505356788635254 Final Accuracy is: 62.18181818181818% . . From the above edperimentation, its very obvious that the model is overfitting the training set as the accuracy on it is very high compared to the test set. . . 2- Dropout Regularization . L Layer Model with Dropout . def forward_propagation_L_dropout(X, parameters, keep_rate): L = len(parameters)//2 caches = [] A = X for i, k in zip(range(L), keep_rate): A_previous = A keep_rate = k np.random.seed(1) D = np.random.rand(A.shape[0], A.shape[1]) D = D &lt; keep_rate A_previous = A_previous * D A_previous = A_previous / keep_rate A, forward_cache = forward_propagation(A_previous, parameters[&quot;W&quot;+str(i+1)], parameters[&quot;b&quot;+str(i+1)]) caches.append((forward_cache, D)) return A, caches . def backward_propagation_L_dropout(A, Y, parameters, caches, keep_rate): grads = {} L = len(parameters)//2 current_cache = caches[L-1] forward_cache, D = current_cache dA = -(Y/A) + ((1-Y)/(1-A)) dZ = A - Y for i, k in zip(reversed(range(L)), reversed(keep_rate)): current_cache = caches[i] forward_cache, D = current_cache A, Z = forward_cache keep_rate = k grads[&quot;dW&quot;+str(i+1)] = (1/m)*np.dot(dZ, A.T) grads[&quot;db&quot;+str(i+1)] = (1/m)*np.sum(dZ, axis = 1, keepdims = True) grads[&quot;dA&quot;+str(i)] = np.dot(parameters[&quot;W&quot;+str(i+1)].T, dZ) grads[&quot;dA&quot;+str(i)] = grads[&quot;dA&quot;+str(i)] * D grads[&quot;dA&quot;+str(i)] = grads[&quot;dA&quot;+str(i)] / keep_rate grads[&quot;dZ&quot;+str(i)] = grads[&quot;dA&quot;+str(i)] * sigmoid_derivative(A) dZ = grads[&quot;dZ&quot;+str(i)] return grads . def L_layers_model_dropout(X, Y, layers_dims, learning_rate, epochs, keep_rate): parameters = initialize_parameters_L(layers_dims) costs = [] accuracy = [] print(&quot;Number of Epochs: &quot;, epochs) print(&quot;Learning rate: &quot;, learning_rate) print(&quot;*&quot;*60) t0 = time.time() for i in range(epochs): A, caches = forward_propagation_L_dropout(X, parameters, keep_rate) cost = compute_cost(A, Y) grads = backward_propagation_L_dropout(A, Y, parameters, caches, keep_rate) parameters = update_params_L(grads, parameters, learning_rate) if i%100 == 0: costs.append(cost) accuracy.append(compute_accuracy(X, Y, parameters)) print(&quot;Cost after epoch {}: {}&quot;.format(i, cost)) t1 = time.time() print(&quot;*&quot;*60) print(&quot;Elapsed time for the training is: &quot;,t1-t0) final_accuracy = compute_accuracy(X, Y, parameters) print(&quot;Final Accuracy is: {}%&quot;.format(final_accuracy * 100)) return parameters, costs, accuracy . parameters, costs, accuracy = L_layers_model_dropout(X, Y, [4, 10, 7, 5, 1], 0.003, 10000, [1, 0.7, 0.5, 0.8]) plot_cost(costs, accuracy, 0.003, 10000) . Number of Epochs: 10000 Learning rate: 0.003 ************************************************************ Cost after epoch 0: 0.9163835691760398 Cost after epoch 100: 0.8452166215925239 Cost after epoch 200: 0.7944852215167835 Cost after epoch 300: 0.7594672098645816 Cost after epoch 400: 0.735846168960295 Cost after epoch 500: 0.7201425611735257 Cost after epoch 600: 0.7097809479383744 Cost after epoch 700: 0.702957127659818 Cost after epoch 800: 0.6984498022960527 Cost after epoch 900: 0.6954492544897947 Cost after epoch 1000: 0.6934251540055715 Cost after epoch 1100: 0.6920325231935958 Cost after epoch 1200: 0.6910477465324621 Cost after epoch 1300: 0.6903260998710389 Cost after epoch 1400: 0.6897739546165346 Cost after epoch 1500: 0.6893307225600451 Cost after epoch 1600: 0.6889571698465814 Cost after epoch 1700: 0.6886278617467434 Cost after epoch 1800: 0.6883262765002832 Cost after epoch 1900: 0.688041644577009 Cost after epoch 2000: 0.6877669105059085 Cost after epoch 2100: 0.6874974347874416 Cost after epoch 2200: 0.6872301895330475 Cost after epoch 2300: 0.6869632759673049 Cost after epoch 2400: 0.6866956244391672 Cost after epoch 2500: 0.6864267579936453 Cost after epoch 2600: 0.6861565423556856 Cost after epoch 2700: 0.6858849098560402 Cost after epoch 2800: 0.6856116014337282 Cost after epoch 2900: 0.6853360040820026 Cost after epoch 3000: 0.6850571642700899 Cost after epoch 3100: 0.6847740191336513 Cost after epoch 3200: 0.6844858150698114 Cost after epoch 3300: 0.6841925758938845 Cost after epoch 3400: 0.68389535948756 Cost after epoch 3500: 0.6835960238632751 Cost after epoch 3600: 0.6832964657742026 Cost after epoch 3700: 0.6829977617580486 Cost after epoch 3800: 0.6826998619112221 Cost after epoch 3900: 0.6824020378842803 Cost after epoch 4000: 0.6821035699146663 Cost after epoch 4100: 0.6818039299192752 Cost after epoch 4200: 0.6815023167125741 Cost after epoch 4300: 0.6811971321904242 Cost after epoch 4400: 0.6808859702631105 Cost after epoch 4500: 0.680566194529671 Cost after epoch 4600: 0.6802355400376059 Cost after epoch 4700: 0.6798918789172875 Cost after epoch 4800: 0.6795326176352593 Cost after epoch 4900: 0.6791561273348659 Cost after epoch 5000: 0.6787673906869401 Cost after epoch 5100: 0.6783820950549274 Cost after epoch 5200: 0.6780147133402169 Cost after epoch 5300: 0.6776687408022856 Cost after epoch 5400: 0.6773438199783035 Cost after epoch 5500: 0.6770410362098955 Cost after epoch 5600: 0.6767625456269885 Cost after epoch 5700: 0.6765086648162947 Cost after epoch 5800: 0.6762762182058322 Cost after epoch 5900: 0.6760610701755437 Cost after epoch 6000: 0.6758598839360831 Cost after epoch 6100: 0.6756702832269227 Cost after epoch 6200: 0.675490602701399 Cost after epoch 6300: 0.6753196659487016 Cost after epoch 6400: 0.6751566450612627 Cost after epoch 6500: 0.6750009774251214 Cost after epoch 6600: 0.6748523101131773 Cost after epoch 6700: 0.6747104528946858 Cost after epoch 6800: 0.6745753345070458 Cost after epoch 6900: 0.6744469661490684 Cost after epoch 7000: 0.674325417823824 Cost after epoch 7100: 0.674210808427835 Cost after epoch 7200: 0.6741033035515902 Cost after epoch 7300: 0.6740031107581114 Cost after epoch 7400: 0.6739104635746233 Cost after epoch 7500: 0.6738255916262264 Cost after epoch 7600: 0.6737486814903102 Cost after epoch 7700: 0.673679837352983 Cost after epoch 7800: 0.6736190509205974 Cost after epoch 7900: 0.673566186861324 Cost after epoch 8000: 0.6735209851097836 Cost after epoch 8100: 0.6734830768011439 Cost after epoch 8200: 0.6734520080358309 Cost after epoch 8300: 0.6734272655797071 Cost after epoch 8400: 0.6734083003030867 Cost after epoch 8500: 0.6733945464572791 Cost after epoch 8600: 0.673385436756718 Cost after epoch 8700: 0.6733804141997113 Cost after epoch 8800: 0.6733789416737702 Cost after epoch 8900: 0.6733805100049348 Cost after epoch 9000: 0.6733846446173654 Cost after epoch 9100: 0.6733909106319879 Cost after epoch 9200: 0.6733989161364485 Cost after epoch 9300: 0.6734083134579014 Cost after epoch 9400: 0.6734187984677674 Cost after epoch 9500: 0.6734301081533863 Cost after epoch 9600: 0.6734420168472464 Cost after epoch 9700: 0.6734543315831477 Cost after epoch 9800: 0.6734668870452335 Cost after epoch 9900: 0.673479540501155 ************************************************************ Elapsed time for the training is: 10.37084436416626 Final Accuracy is: 55.539358600583085% . parameters, costs, accuracy = L_layers_model_dropout(X, Y, [4, 10, 7, 5, 1], 0.003, 20000, [1, 0.7, 0.5, 0.8]) plot_cost(costs, accuracy, 0.003, 20000) . Number of Epochs: 20000 Learning rate: 0.003 ************************************************************ Cost after epoch 0: 0.9163835691760398 Cost after epoch 100: 0.8452166215925239 Cost after epoch 200: 0.7944852215167835 Cost after epoch 300: 0.7594672098645816 Cost after epoch 400: 0.735846168960295 Cost after epoch 500: 0.7201425611735257 Cost after epoch 600: 0.7097809479383744 Cost after epoch 700: 0.702957127659818 Cost after epoch 800: 0.6984498022960527 Cost after epoch 900: 0.6954492544897947 Cost after epoch 1000: 0.6934251540055715 Cost after epoch 1100: 0.6920325231935958 Cost after epoch 1200: 0.6910477465324621 Cost after epoch 1300: 0.6903260998710389 Cost after epoch 1400: 0.6897739546165346 Cost after epoch 1500: 0.6893307225600451 Cost after epoch 1600: 0.6889571698465814 Cost after epoch 1700: 0.6886278617467434 Cost after epoch 1800: 0.6883262765002832 Cost after epoch 1900: 0.688041644577009 Cost after epoch 2000: 0.6877669105059085 Cost after epoch 2100: 0.6874974347874416 Cost after epoch 2200: 0.6872301895330475 Cost after epoch 2300: 0.6869632759673049 Cost after epoch 2400: 0.6866956244391672 Cost after epoch 2500: 0.6864267579936453 Cost after epoch 2600: 0.6861565423556856 Cost after epoch 2700: 0.6858849098560402 Cost after epoch 2800: 0.6856116014337282 Cost after epoch 2900: 0.6853360040820026 Cost after epoch 3000: 0.6850571642700899 Cost after epoch 3100: 0.6847740191336513 Cost after epoch 3200: 0.6844858150698114 Cost after epoch 3300: 0.6841925758938845 Cost after epoch 3400: 0.68389535948756 Cost after epoch 3500: 0.6835960238632751 Cost after epoch 3600: 0.6832964657742026 Cost after epoch 3700: 0.6829977617580486 Cost after epoch 3800: 0.6826998619112221 Cost after epoch 3900: 0.6824020378842803 Cost after epoch 4000: 0.6821035699146663 Cost after epoch 4100: 0.6818039299192752 Cost after epoch 4200: 0.6815023167125741 Cost after epoch 4300: 0.6811971321904242 Cost after epoch 4400: 0.6808859702631105 Cost after epoch 4500: 0.680566194529671 Cost after epoch 4600: 0.6802355400376059 Cost after epoch 4700: 0.6798918789172875 Cost after epoch 4800: 0.6795326176352593 Cost after epoch 4900: 0.6791561273348659 Cost after epoch 5000: 0.6787673906869401 Cost after epoch 5100: 0.6783820950549274 Cost after epoch 5200: 0.6780147133402169 Cost after epoch 5300: 0.6776687408022856 Cost after epoch 5400: 0.6773438199783035 Cost after epoch 5500: 0.6770410362098955 Cost after epoch 5600: 0.6767625456269885 Cost after epoch 5700: 0.6765086648162947 Cost after epoch 5800: 0.6762762182058322 Cost after epoch 5900: 0.6760610701755437 Cost after epoch 6000: 0.6758598839360831 Cost after epoch 6100: 0.6756702832269227 Cost after epoch 6200: 0.675490602701399 Cost after epoch 6300: 0.6753196659487016 Cost after epoch 6400: 0.6751566450612627 Cost after epoch 6500: 0.6750009774251214 Cost after epoch 6600: 0.6748523101131773 Cost after epoch 6700: 0.6747104528946858 Cost after epoch 6800: 0.6745753345070458 Cost after epoch 6900: 0.6744469661490684 Cost after epoch 7000: 0.674325417823824 Cost after epoch 7100: 0.674210808427835 Cost after epoch 7200: 0.6741033035515902 Cost after epoch 7300: 0.6740031107581114 Cost after epoch 7400: 0.6739104635746233 Cost after epoch 7500: 0.6738255916262264 Cost after epoch 7600: 0.6737486814903102 Cost after epoch 7700: 0.673679837352983 Cost after epoch 7800: 0.6736190509205974 Cost after epoch 7900: 0.673566186861324 Cost after epoch 8000: 0.6735209851097836 Cost after epoch 8100: 0.6734830768011439 Cost after epoch 8200: 0.6734520080358309 Cost after epoch 8300: 0.6734272655797071 Cost after epoch 8400: 0.6734083003030867 Cost after epoch 8500: 0.6733945464572791 Cost after epoch 8600: 0.673385436756718 Cost after epoch 8700: 0.6733804141997113 Cost after epoch 8800: 0.6733789416737702 Cost after epoch 8900: 0.6733805100049348 Cost after epoch 9000: 0.6733846446173654 Cost after epoch 9100: 0.6733909106319879 Cost after epoch 9200: 0.6733989161364485 Cost after epoch 9300: 0.6734083134579014 Cost after epoch 9400: 0.6734187984677674 Cost after epoch 9500: 0.6734301081533863 Cost after epoch 9600: 0.6734420168472464 Cost after epoch 9700: 0.6734543315831477 Cost after epoch 9800: 0.6734668870452335 Cost after epoch 9900: 0.673479540501155 Cost after epoch 10000: 0.6734921669901623 Cost after epoch 10100: 0.6735046549101539 Cost after epoch 10200: 0.6735169020629111 Cost after epoch 10300: 0.6735288122175467 Cost after epoch 10400: 0.6735402923575811 Cost after epoch 10500: 0.6735512509599878 Cost after epoch 10600: 0.6735615978299829 Cost after epoch 10700: 0.6735712460560074 Cost after epoch 10800: 0.6735801164478026 Cost after epoch 10900: 0.6735881443817578 Cost after epoch 11000: 0.6735952884663491 Cost after epoch 11100: 0.6736015400977611 Cost after epoch 11200: 0.6736069329408029 Cost after epoch 11300: 0.6736115515726446 Cost after epoch 11400: 0.6736155387644065 Cost after epoch 11500: 0.6736191009700845 Cost after epoch 11600: 0.673622511437234 Cost after epoch 11700: 0.6736261098585241 Cost after epoch 11800: 0.6736302966131442 Cost after epoch 11900: 0.6736355187044483 Cost after epoch 12000: 0.673642244559292 Cost after epoch 12100: 0.6736509277723678 Cost after epoch 12200: 0.6736619665103132 Cost after epoch 12300: 0.6736756715388877 Cost after epoch 12400: 0.6736922522391328 Cost after epoch 12500: 0.673711813808372 Cost after epoch 12600: 0.6737343479114307 Cost after epoch 12700: 0.673759714582534 Cost after epoch 12800: 0.6737876416054137 Cost after epoch 12900: 0.6738177683032798 Cost after epoch 13000: 0.6738497246073363 Cost after epoch 13100: 0.6738832038891609 Cost after epoch 13200: 0.6739179937910474 Cost after epoch 13300: 0.6739539605369339 Cost after epoch 13400: 0.6739910064907908 Cost after epoch 13500: 0.6740290248391426 Cost after epoch 13600: 0.6740678666709375 Cost after epoch 13700: 0.6741073253199695 Cost after epoch 13800: 0.6741471360886372 Cost after epoch 13900: 0.6741869868664553 Cost after epoch 14000: 0.6742265349739592 Cost after epoch 14100: 0.6742654261935012 Cost after epoch 14200: 0.6743033128753213 Cost after epoch 14300: 0.6743398692466931 Cost after epoch 14400: 0.674374803292205 Cost after epoch 14500: 0.6744078652747563 Cost after epoch 14600: 0.6744388530198997 Cost after epoch 14700: 0.6744676139255468 Cost after epoch 14800: 0.6744940437394968 Cost after epoch 14900: 0.674518082521963 Cost after epoch 15000: 0.6745397086166133 Cost after epoch 15100: 0.6745589316403164 Cost after epoch 15200: 0.6745757854131654 Cost after epoch 15300: 0.6745903214874721 Cost after epoch 15400: 0.6746026036333974 Cost after epoch 15500: 0.6746127033911213 Cost after epoch 15600: 0.6746206966372427 Cost after epoch 15700: 0.674626661029471 Cost after epoch 15800: 0.6746306741660949 Cost after epoch 15900: 0.6746328123022391 Cost after epoch 16000: 0.6746331494861645 Cost after epoch 16100: 0.6746317570049193 Cost after epoch 16200: 0.67462870305362 Cost after epoch 16300: 0.674624052563977 Cost after epoch 16400: 0.6746178671447292 Cost after epoch 16500: 0.6746102050996847 Cost after epoch 16600: 0.6746011214987541 Cost after epoch 16700: 0.6745906682844168 Cost after epoch 16800: 0.6745788944011416 Cost after epoch 16900: 0.6745658459389141 Cost after epoch 17000: 0.6745515662846235 Cost after epoch 17100: 0.6745360962769097 Cost after epoch 17200: 0.6745194743614141 Cost after epoch 17300: 0.6745017367443267 Cost after epoch 17400: 0.6744829175428178 Cost after epoch 17500: 0.6744630489314373 Cost after epoch 17600: 0.674442161283908 Cost after epoch 17700: 0.6744202833099899 Cost after epoch 17800: 0.6743974421872425 Cost after epoch 17900: 0.6743736636876209 Cost after epoch 18000: 0.6743489722988861 Cost after epoch 18100: 0.6743233913408415 Cost after epoch 18200: 0.674296943076401 Cost after epoch 18300: 0.6742696488174877 Cost after epoch 18400: 0.674241529025746 Cost after epoch 18500: 0.6742126034080367 Cost after epoch 18600: 0.674182891006677 Cost after epoch 18700: 0.6741524102843975 Cost after epoch 18800: 0.6741211792039999 Cost after epoch 18900: 0.6740892153027487 Cost after epoch 19000: 0.6740565357615635 Cost after epoch 19100: 0.6740231574691687 Cost after epoch 19200: 0.6739890970814185 Cost after epoch 19300: 0.6739543710761227 Cost after epoch 19400: 0.6739189958037958 Cost after epoch 19500: 0.6738829875348522 Cost after epoch 19600: 0.6738463625038834 Cost after epoch 19700: 0.673809136951751 Cost after epoch 19800: 0.673771327166328 Cost after epoch 19900: 0.6737329495228075 ************************************************************ Elapsed time for the training is: 20.851077556610107 Final Accuracy is: 55.539358600583085% . parameters, costs, accuracy = L_layers_model_dropout(X, Y, [4, 10, 7, 5, 1], 0.01, 10000, [1, 0.7, 0.5, 0.8]) plot_cost(costs, accuracy, 0.01, 10000) . Number of Epochs: 10000 Learning rate: 0.01 ************************************************************ Cost after epoch 0: 0.9163835691760398 Cost after epoch 100: 0.7503431804269972 Cost after epoch 200: 0.7048311347426974 Cost after epoch 300: 0.6933912390028243 Cost after epoch 400: 0.6901159440724898 Cost after epoch 500: 0.6887307047872974 Cost after epoch 600: 0.6877661676025677 Cost after epoch 700: 0.6868743400161007 Cost after epoch 800: 0.6859762951944409 Cost after epoch 900: 0.6850583560641338 Cost after epoch 1000: 0.6840957720595733 Cost after epoch 1100: 0.6830996467591726 Cost after epoch 1200: 0.6821064288541362 Cost after epoch 1300: 0.6810978054522956 Cost after epoch 1400: 0.68001293498189 Cost after epoch 1500: 0.6787739481566913 Cost after epoch 1600: 0.6775632813630946 Cost after epoch 1700: 0.6765939989221508 Cost after epoch 1800: 0.6758619521410361 Cost after epoch 1900: 0.675265895196295 Cost after epoch 2000: 0.6747577914221431 Cost after epoch 2100: 0.674325407556817 Cost after epoch 2200: 0.6739702318298254 Cost after epoch 2300: 0.6736993402662211 Cost after epoch 2400: 0.673517087946735 Cost after epoch 2500: 0.6734154539835131 Cost after epoch 2600: 0.673376281962632 Cost after epoch 2700: 0.6733791500488295 Cost after epoch 2800: 0.6734063675820808 Cost after epoch 2900: 0.6734451545792269 Cost after epoch 3000: 0.6734874705325166 Cost after epoch 3100: 0.6735283423531555 Cost after epoch 3200: 0.673564113026381 Cost after epoch 3300: 0.6735916360866214 Cost after epoch 3400: 0.6736095890715408 Cost after epoch 3500: 0.6736216649314908 Cost after epoch 3600: 0.6736389354384226 Cost after epoch 3700: 0.6736771938563265 Cost after epoch 3800: 0.6737468435709224 Cost after epoch 3900: 0.6738453584019537 Cost after epoch 4000: 0.673961737478154 Cost after epoch 4100: 0.6740896321097527 Cost after epoch 4200: 0.6742221699289208 Cost after epoch 4300: 0.6743476698159889 Cost after epoch 4400: 0.6744547127154201 Cost after epoch 4500: 0.674536641473232 Cost after epoch 4600: 0.6745920544746693 Cost after epoch 4700: 0.674622674493505 Cost after epoch 4800: 0.6746312502929351 Cost after epoch 4900: 0.6746205197371956 Cost after epoch 5000: 0.6745928756473021 Cost after epoch 5100: 0.6745503144471385 Cost after epoch 5200: 0.6744944746330193 Cost after epoch 5300: 0.6744266972751082 Cost after epoch 5400: 0.6743480870332796 Cost after epoch 5500: 0.6742595675455582 Cost after epoch 5600: 0.6741619298612178 Cost after epoch 5700: 0.674055873741099 Cost after epoch 5800: 0.6739420417564405 Cost after epoch 5900: 0.6738210463957107 Cost after epoch 6000: 0.6736934912838499 Cost after epoch 6100: 0.6735599889949121 Cost after epoch 6200: 0.6734211793584283 Cost after epoch 6300: 0.6732777532503494 Cost after epoch 6400: 0.6731304874997817 Cost after epoch 6500: 0.6729802966830587 Cost after epoch 6600: 0.6728283067631442 Cost after epoch 6700: 0.6726759522500497 Cost after epoch 6800: 0.6725250895217735 Cost after epoch 6900: 0.6723780989637229 Cost after epoch 7000: 0.6722379127322451 Cost after epoch 7100: 0.6721078546740522 Cost after epoch 7200: 0.6719911295136116 Cost after epoch 7300: 0.6718897912053456 Cost after epoch 7400: 0.6718031918311366 Cost after epoch 7500: 0.671726559054273 Cost after epoch 7600: 0.6716512566086288 Cost after epoch 7700: 0.6715676673129589 Cost after epoch 7800: 0.6714689392009937 Cost after epoch 7900: 0.6713527410162196 Cost after epoch 8000: 0.6712204507333995 Cost after epoch 8100: 0.6710753312803095 Cost after epoch 8200: 0.670921044679362 Cost after epoch 8300: 0.6707608129486458 Cost after epoch 8400: 0.6705970330547333 Cost after epoch 8500: 0.6704311649499776 Cost after epoch 8600: 0.6702638443352439 Cost after epoch 8700: 0.6700951961696926 Cost after epoch 8800: 0.6699252216304028 Cost after epoch 8900: 0.6697540808699329 Cost after epoch 9000: 0.6695822240794802 Cost after epoch 9100: 0.6694104820925814 Cost after epoch 9200: 0.6692402410617789 Cost after epoch 9300: 0.6690737396642806 Cost after epoch 9400: 0.6689144422963429 Cost after epoch 9500: 0.6687673469379548 Cost after epoch 9600: 0.6686389232531543 Cost after epoch 9700: 0.6685362065854783 Cost after epoch 9800: 0.6684646749462853 Cost after epoch 9900: 0.6684253422199302 ************************************************************ Elapsed time for the training is: 10.349528074264526 Final Accuracy is: 55.539358600583085% . parameters, costs, accuracy = L_layers_model_dropout(X, Y, [4, 14, 10, 6, 1], 0.01, 10000, [1, 0.7, 0.5, 0.8]) plot_cost(costs, accuracy, 0.01, 10000) . Number of Epochs: 10000 Learning rate: 0.01 ************************************************************ Cost after epoch 0: 0.8884564843017544 Cost after epoch 100: 0.7861143784667658 Cost after epoch 200: 0.749249305076672 Cost after epoch 300: 0.7369676568928601 Cost after epoch 400: 0.7331872443833815 Cost after epoch 500: 0.7318313540597957 Cost after epoch 600: 0.7314822893153313 Cost after epoch 700: 0.7370370275549597 Cost after epoch 800: 0.7436862244994701 Cost after epoch 900: 0.7481224788301187 Cost after epoch 1000: 0.7505808024111353 Cost after epoch 1100: 0.7511197953911235 Cost after epoch 1200: 0.7503220280303022 Cost after epoch 1300: 0.7485665109908302 Cost after epoch 1400: 0.746136347543621 Cost after epoch 1500: 0.7432851289524816 Cost after epoch 1600: 0.7402297344873537 Cost after epoch 1700: 0.7370878006574682 Cost after epoch 1800: 0.7338677797546795 Cost after epoch 1900: 0.7306196481670908 Cost after epoch 2000: 0.727255045260981 Cost after epoch 2100: 0.723766733705112 Cost after epoch 2200: 0.7203163783171995 Cost after epoch 2300: 0.7169497597801625 Cost after epoch 2400: 0.713668019318686 Cost after epoch 2500: 0.7105977315021181 Cost after epoch 2600: 0.7078155711953356 Cost after epoch 2700: 0.7053364710718822 Cost after epoch 2800: 0.7031800648527065 Cost after epoch 2900: 0.7013158347183425 Cost after epoch 3000: 0.6996948481162067 Cost after epoch 3100: 0.6982746863247999 Cost after epoch 3200: 0.6969855740953866 Cost after epoch 3300: 0.6957905739679923 Cost after epoch 3400: 0.6946749904199989 Cost after epoch 3500: 0.6936312766586199 Cost after epoch 3600: 0.692655121143362 Cost after epoch 3700: 0.6917449860751651 Cost after epoch 3800: 0.6909015289519607 Cost after epoch 3900: 0.6901263207744914 Cost after epoch 4000: 0.6894205254646425 Cost after epoch 4100: 0.6887840942846175 Cost after epoch 4200: 0.6882154891298234 Cost after epoch 4300: 0.6877116832999106 Cost after epoch 4400: 0.6872680986931957 Cost after epoch 4500: 0.6868783100382988 Cost after epoch 4600: 0.6865344296298004 Cost after epoch 4700: 0.6862296605493782 Cost after epoch 4800: 0.6859616504953702 Cost after epoch 4900: 0.6857313287718699 Cost after epoch 5000: 0.6855335743775544 Cost after epoch 5100: 0.685351402837529 Cost after epoch 5200: 0.6851842570635371 Cost after epoch 5300: 0.6850404323746727 Cost after epoch 5400: 0.6848553950866357 Cost after epoch 5500: 0.6846072714826761 Cost after epoch 5600: 0.684332499026824 Cost after epoch 5700: 0.6840274968605844 Cost after epoch 5800: 0.6836909462765856 Cost after epoch 5900: 0.683359765832128 Cost after epoch 6000: 0.6830427270763811 Cost after epoch 6100: 0.6827143477407956 Cost after epoch 6200: 0.6823561200524082 Cost after epoch 6300: 0.681973541348577 Cost after epoch 6400: 0.6815867124941073 Cost after epoch 6500: 0.6812048119286287 Cost after epoch 6600: 0.6808336892890411 Cost after epoch 6700: 0.6804852692121325 Cost after epoch 6800: 0.6801677087271901 Cost after epoch 6900: 0.6798781879009781 Cost after epoch 7000: 0.6796079308662869 Cost after epoch 7100: 0.6793482244028656 Cost after epoch 7200: 0.6790932740721422 Cost after epoch 7300: 0.6788402952363594 Cost after epoch 7400: 0.6785884850240251 Cost after epoch 7500: 0.678337923796602 Cost after epoch 7600: 0.678088894362313 Cost after epoch 7700: 0.6778416689388124 Cost after epoch 7800: 0.6775965330572012 Cost after epoch 7900: 0.6773538449558847 Cost after epoch 8000: 0.67711405403217 Cost after epoch 8100: 0.6768776813017767 Cost after epoch 8200: 0.67664526896776 Cost after epoch 8300: 0.6764173093725512 Cost after epoch 8400: 0.6761941825223554 Cost after epoch 8500: 0.6759761071758623 Cost after epoch 8600: 0.6757630578853333 Cost after epoch 8700: 0.675554621196386 Cost after epoch 8800: 0.6753498647562597 Cost after epoch 8900: 0.6751473332729571 Cost after epoch 9000: 0.674945199143897 Cost after epoch 9100: 0.6747414883438433 Cost after epoch 9200: 0.6745342824099138 Cost after epoch 9300: 0.6743218554737642 Cost after epoch 9400: 0.6741027667803512 Cost after epoch 9500: 0.6738759425918079 Cost after epoch 9600: 0.6736407508668572 Cost after epoch 9700: 0.6733970391803118 Cost after epoch 9800: 0.6731451059375158 Cost after epoch 9900: 0.6728856034007293 ************************************************************ Elapsed time for the training is: 12.911050081253052 Final Accuracy is: 55.539358600583085% . Checking Dropout over train &amp; test sets . parameters_train, costs_train, accuracy_train = L_layers_model_dropout(X_train, Y_train, [4, 4, 5, 1], 0.01, 10000, [1, 0.7, 0.5]) plot_cost(costs_train, accuracy_train, 0.01, 10000) . Number of Epochs: 10000 Learning rate: 0.01 ************************************************************ Cost after epoch 0: 0.9546995251618293 Cost after epoch 100: 0.8989616787226404 Cost after epoch 200: 0.8728276532930982 Cost after epoch 300: 0.8569666749314313 Cost after epoch 400: 0.8442237893569077 Cost after epoch 500: 0.8297304533032444 Cost after epoch 600: 0.8110807533753162 Cost after epoch 700: 0.7868120444863264 Cost after epoch 800: 0.7633531748998451 Cost after epoch 900: 0.7501339188467193 Cost after epoch 1000: 0.7393713768110134 Cost after epoch 1100: 0.7287667184187708 Cost after epoch 1200: 0.7173251203715487 Cost after epoch 1300: 0.7052856967253646 Cost after epoch 1400: 0.6959505183239167 Cost after epoch 1500: 0.6914411623101798 Cost after epoch 1600: 0.6885005783984556 Cost after epoch 1700: 0.6862557822667534 Cost after epoch 1800: 0.6844722357105958 Cost after epoch 1900: 0.6830223638486413 Cost after epoch 2000: 0.6818277480147721 Cost after epoch 2100: 0.6808333258164482 Cost after epoch 2200: 0.6799963182495413 Cost after epoch 2300: 0.6792827288275762 Cost after epoch 2400: 0.6786660486167665 Cost after epoch 2500: 0.6781257326140785 Cost after epoch 2600: 0.6776445969157902 Cost after epoch 2700: 0.6772055872639556 Cost after epoch 2800: 0.6767913247081336 Cost after epoch 2900: 0.676387028268617 Cost after epoch 3000: 0.6759816339127921 Cost after epoch 3100: 0.6755707677728161 Cost after epoch 3200: 0.6751618965260346 Cost after epoch 3300: 0.6747586941526931 Cost after epoch 3400: 0.6743644119987741 Cost after epoch 3500: 0.6739875443552549 Cost after epoch 3600: 0.6735951844476016 Cost after epoch 3700: 0.6731349991320379 Cost after epoch 3800: 0.6725952398578828 Cost after epoch 3900: 0.6719417680701966 Cost after epoch 4000: 0.6710760461837292 Cost after epoch 4100: 0.669982377582085 Cost after epoch 4200: 0.6687730556172836 Cost after epoch 4300: 0.6674454315876583 Cost after epoch 4400: 0.6658100654564713 Cost after epoch 4500: 0.6640136630141962 Cost after epoch 4600: 0.6622304660407866 Cost after epoch 4700: 0.6605303506895934 Cost after epoch 4800: 0.6589698920404065 Cost after epoch 4900: 0.6575795783381848 Cost after epoch 5000: 0.6563636743863674 Cost after epoch 5100: 0.6553175403790787 Cost after epoch 5200: 0.654438452501494 Cost after epoch 5300: 0.6537306192293608 Cost after epoch 5400: 0.6531855889243711 Cost after epoch 5500: 0.6527452241507884 Cost after epoch 5600: 0.6522889364470716 Cost after epoch 5700: 0.6516463389078776 Cost after epoch 5800: 0.6506007152337102 Cost after epoch 5900: 0.6488974681515278 Cost after epoch 6000: 0.6467782451074395 Cost after epoch 6100: 0.6464677703729296 Cost after epoch 6200: 0.6483637632125464 Cost after epoch 6300: 0.6498738314735658 Cost after epoch 6400: 0.6505651369845148 Cost after epoch 6500: 0.6508315331518851 Cost after epoch 6600: 0.6509310351853862 Cost after epoch 6700: 0.6509853988322348 Cost after epoch 6800: 0.6510527456867901 Cost after epoch 6900: 0.6511544347817229 Cost after epoch 7000: 0.6512837708286743 Cost after epoch 7100: 0.6514180042366083 Cost after epoch 7200: 0.6515273189137516 Cost after epoch 7300: 0.6515869311294957 Cost after epoch 7400: 0.6515868175197429 Cost after epoch 7500: 0.6515208776966639 Cost after epoch 7600: 0.6513730107378937 Cost after epoch 7700: 0.6511183645195125 Cost after epoch 7800: 0.6507402566022146 Cost after epoch 7900: 0.6502482031346513 Cost after epoch 8000: 0.6496818559372978 Cost after epoch 8100: 0.6490785499525427 Cost after epoch 8200: 0.6484446243379552 Cost after epoch 8300: 0.6477713228252499 Cost after epoch 8400: 0.6470580526132493 Cost after epoch 8500: 0.6462990564985089 Cost after epoch 8600: 0.6454604227432406 Cost after epoch 8700: 0.6445028291055983 Cost after epoch 8800: 0.6434173725364623 Cost after epoch 8900: 0.6422225456703409 Cost after epoch 9000: 0.640944708835939 Cost after epoch 9100: 0.6396031477984537 Cost after epoch 9200: 0.6382048883426504 Cost after epoch 9300: 0.6367474514506392 Cost after epoch 9400: 0.6352218631162437 Cost after epoch 9500: 0.6336080100090965 Cost after epoch 9600: 0.6318694193148084 Cost after epoch 9700: 0.6299714534596752 Cost after epoch 9800: 0.6279132159627012 Cost after epoch 9900: 0.625727350087584 ************************************************************ Elapsed time for the training is: 4.873733758926392 Final Accuracy is: 80.03646308113036% . parameters_test, costs_test, accuracy_test = L_layers_model_dropout(X_test, Y_test, [4, 4, 5, 1], 0.01, 10000, [1, 0.7, 0.5]) plot_cost(costs_test, accuracy_test, 0.01, 10000) . Number of Epochs: 10000 Learning rate: 0.01 ************************************************************ Cost after epoch 0: 0.9510749792801051 Cost after epoch 100: 0.9414397803636251 Cost after epoch 200: 0.9312296261665969 Cost after epoch 300: 0.9217693229302586 Cost after epoch 400: 0.9139912033463444 Cost after epoch 500: 0.9080605147258527 Cost after epoch 600: 0.9036317662850696 Cost after epoch 700: 0.9002381804356252 Cost after epoch 800: 0.8973213608206313 Cost after epoch 900: 0.8942527860837431 Cost after epoch 1000: 0.890521736515452 Cost after epoch 1100: 0.8859168282961396 Cost after epoch 1200: 0.8805006881887317 Cost after epoch 1300: 0.8744488295744905 Cost after epoch 1400: 0.8679161212813785 Cost after epoch 1500: 0.8610052792286955 Cost after epoch 1600: 0.853800272773862 Cost after epoch 1700: 0.8464046087237695 Cost after epoch 1800: 0.8389554081329569 Cost after epoch 1900: 0.8316129921761897 Cost after epoch 2000: 0.8245358898169772 Cost after epoch 2100: 0.8178557092584258 Cost after epoch 2200: 0.811665960285966 Cost after epoch 2300: 0.8060297589627715 Cost after epoch 2400: 0.801003424767536 Cost after epoch 2500: 0.796662862073679 Cost after epoch 2600: 0.7930463055822238 Cost after epoch 2700: 0.7897488327142551 Cost after epoch 2800: 0.7857957871654428 Cost after epoch 2900: 0.7810205691780421 Cost after epoch 3000: 0.7756831789776126 Cost after epoch 3100: 0.769934088595464 Cost after epoch 3200: 0.7647012440871064 Cost after epoch 3300: 0.7607923448566232 Cost after epoch 3400: 0.7580715094228548 Cost after epoch 3500: 0.7561279196650429 Cost after epoch 3600: 0.7546517618588845 Cost after epoch 3700: 0.7534149745034492 Cost after epoch 3800: 0.7521992637983471 Cost after epoch 3900: 0.7508034132611408 Cost after epoch 4000: 0.7491122293237124 Cost after epoch 4100: 0.7471195916921084 Cost after epoch 4200: 0.7448806394620361 Cost after epoch 4300: 0.742460010946896 Cost after epoch 4400: 0.7399114600175742 Cost after epoch 4500: 0.7372775272720303 Cost after epoch 4600: 0.734593528783889 Cost after epoch 4700: 0.7318900625255581 Cost after epoch 4800: 0.7291937769192152 Cost after epoch 4900: 0.7265273709913886 Cost after epoch 5000: 0.7239095338668337 Cost after epoch 5100: 0.7213550860031587 Cost after epoch 5200: 0.7188753175310074 Cost after epoch 5300: 0.7164784349325538 Cost after epoch 5400: 0.7141700358556792 Cost after epoch 5500: 0.7119535629717949 Cost after epoch 5600: 0.7098307130526419 Cost after epoch 5700: 0.7078017926862679 Cost after epoch 5800: 0.7058660202315628 Cost after epoch 5900: 0.7040217777237713 Cost after epoch 6000: 0.7022668182837707 Cost after epoch 6100: 0.7005984351276004 Cost after epoch 6200: 0.6990135980904659 Cost after epoch 6300: 0.6975090630320343 Cost after epoch 6400: 0.6960814588039037 Cost after epoch 6500: 0.6947273557662652 Cost after epoch 6600: 0.6934433192035119 Cost after epoch 6700: 0.6922259504315733 Cost after epoch 6800: 0.6910719179149395 Cost after epoch 6900: 0.6899779803109871 Cost after epoch 7000: 0.6889410030231848 Cost after epoch 7100: 0.6879579695637704 Cost after epoch 7200: 0.6870259887937696 Cost after epoch 7300: 0.6861422989198935 Cost after epoch 7400: 0.6853042689830326 Cost after epoch 7500: 0.6845093984736198 Cost after epoch 7600: 0.6837553156586568 Cost after epoch 7700: 0.6830397752069836 Cost after epoch 7800: 0.6823606557520421 Cost after epoch 7900: 0.6817159581221429 Cost after epoch 8000: 0.681103805064017 Cost after epoch 8100: 0.680522443327235 Cost after epoch 8200: 0.6799702488899675 Cost after epoch 8300: 0.6794457358496404 Cost after epoch 8400: 0.6789475691930021 Cost after epoch 8500: 0.6784745818128382 Cost after epoch 8600: 0.6780257980443181 Cost after epoch 8700: 0.6776004722482815 Cost after epoch 8800: 0.6771981658685507 Cost after epoch 8900: 0.6768189140853136 Cost after epoch 9000: 0.6764635641208125 Cost after epoch 9100: 0.6761343317662051 Cost after epoch 9200: 0.6758353399848956 Cost after epoch 9300: 0.6755723193694708 Cost after epoch 9400: 0.6753505859739192 Cost after epoch 9500: 0.6751720366904359 Cost after epoch 9600: 0.6750336386271888 Cost after epoch 9700: 0.6749286337739314 Cost after epoch 9800: 0.6748490320573768 Cost after epoch 9900: 0.6747875596107115 ************************************************************ Elapsed time for the training is: 2.494713544845581 Final Accuracy is: 57.81818181818181% . parameters_train, costs_train, accuracy_train = L_layers_model_dropout(X_train, Y_train, [4, 10, 6, 1], 0.01, 10000, [1, 0.7, 0.5]) plot_cost(costs_train, accuracy_train, 0.01, 10000) . Number of Epochs: 10000 Learning rate: 0.01 ************************************************************ Cost after epoch 0: 0.9371750358170076 Cost after epoch 100: 0.8901788661577312 Cost after epoch 200: 0.8619526828124259 Cost after epoch 300: 0.838447021204934 Cost after epoch 400: 0.8197686999753737 Cost after epoch 500: 0.8031344519990478 Cost after epoch 600: 0.7852352027654261 Cost after epoch 700: 0.7651403048320857 Cost after epoch 800: 0.7453716177888862 Cost after epoch 900: 0.7283838394625726 Cost after epoch 1000: 0.7149128300909405 Cost after epoch 1100: 0.7046585994149723 Cost after epoch 1200: 0.6970385989829607 Cost after epoch 1300: 0.6914799959272218 Cost after epoch 1400: 0.687492367671897 Cost after epoch 1500: 0.6846783774421041 Cost after epoch 1600: 0.6827495516209965 Cost after epoch 1700: 0.6815184754867163 Cost after epoch 1800: 0.680846309412683 Cost after epoch 1900: 0.6805977517304455 Cost after epoch 2000: 0.6806300564425535 Cost after epoch 2100: 0.6808009709636245 Cost after epoch 2200: 0.6810827921923288 Cost after epoch 2300: 0.6815551132148815 Cost after epoch 2400: 0.6820868913845232 Cost after epoch 2500: 0.6825960164144903 Cost after epoch 2600: 0.6830820434776602 Cost after epoch 2700: 0.6834883568288846 Cost after epoch 2800: 0.6836968666597574 Cost after epoch 2900: 0.6836961994369902 Cost after epoch 3000: 0.6835672245389953 Cost after epoch 3100: 0.6833959510511821 Cost after epoch 3200: 0.6832465216219807 Cost after epoch 3300: 0.6831540195187995 Cost after epoch 3400: 0.6831237412681944 Cost after epoch 3500: 0.6831415026914113 Cost after epoch 3600: 0.6831875934058279 Cost after epoch 3700: 0.6832460328276103 Cost after epoch 3800: 0.6833058318447632 Cost after epoch 3900: 0.6833592420619548 Cost after epoch 4000: 0.6834036163876568 Cost after epoch 4100: 0.6834493781143742 Cost after epoch 4200: 0.6835253001304892 Cost after epoch 4300: 0.6836590111393578 Cost after epoch 4400: 0.6838481575091594 Cost after epoch 4500: 0.684073486311353 Cost after epoch 4600: 0.6843215889156314 Cost after epoch 4700: 0.6845850351056676 Cost after epoch 4800: 0.6848566277654208 Cost after epoch 4900: 0.6851260387931427 Cost after epoch 5000: 0.6853786520121391 Cost after epoch 5100: 0.6855965333742631 Cost after epoch 5200: 0.6857614020707933 Cost after epoch 5300: 0.6858586289600386 Cost after epoch 5400: 0.6858806653886743 Cost after epoch 5500: 0.685828665068673 Cost after epoch 5600: 0.6857121167863123 Cost after epoch 5700: 0.6855458612906093 Cost after epoch 5800: 0.6853447365467413 Cost after epoch 5900: 0.6851210190245134 Cost after epoch 6000: 0.6848864867765648 Cost after epoch 6100: 0.6846535489609474 Cost after epoch 6200: 0.6844327750900348 Cost after epoch 6300: 0.6842314030873541 Cost after epoch 6400: 0.684055222230181 Cost after epoch 6500: 0.6839096874146336 Cost after epoch 6600: 0.6837990686933446 Cost after epoch 6700: 0.6837276003197608 Cost after epoch 6800: 0.6837022127828064 Cost after epoch 6900: 0.6837315133229022 Cost after epoch 7000: 0.6838045174543466 Cost after epoch 7100: 0.6838502601400941 Cost after epoch 7200: 0.683811308316823 Cost after epoch 7300: 0.6837257040031363 Cost after epoch 7400: 0.6836333043964063 Cost after epoch 7500: 0.6835449492720898 Cost after epoch 7600: 0.6834601977756131 Cost after epoch 7700: 0.6833768238612252 Cost after epoch 7800: 0.6832931621994318 Cost after epoch 7900: 0.6832082521700854 Cost after epoch 8000: 0.6831215794649768 Cost after epoch 8100: 0.6830328784456314 Cost after epoch 8200: 0.6829420223296944 Cost after epoch 8300: 0.6828489336196083 Cost after epoch 8400: 0.6827534879409981 Cost after epoch 8500: 0.6826554327306744 Cost after epoch 8600: 0.6825543441982819 Cost after epoch 8700: 0.6824496209190858 Cost after epoch 8800: 0.6823404997154104 Cost after epoch 8900: 0.6822260863898421 Cost after epoch 9000: 0.6821053956258514 Cost after epoch 9100: 0.6819773846251133 Cost after epoch 9200: 0.6818409637657232 Cost after epoch 9300: 0.681694983485072 Cost after epoch 9400: 0.6815382215151565 Cost after epoch 9500: 0.6813694080689656 Cost after epoch 9600: 0.6811873129089961 Cost after epoch 9700: 0.6809908904637618 Cost after epoch 9800: 0.6807794548016595 Cost after epoch 9900: 0.6805528310986931 ************************************************************ Elapsed time for the training is: 6.752296686172485 Final Accuracy is: 54.96809480401094% . parameters_test, costs_test, accuracy_test = L_layers_model_dropout(X_test, Y_test, [4, 10, 6, 1], 0.01, 10000, [1, 0.7, 0.5]) plot_cost(costs_test, accuracy_test, 0.01, 10000) . Number of Epochs: 10000 Learning rate: 0.01 ************************************************************ Cost after epoch 0: 0.9480403969904232 Cost after epoch 100: 0.9362237746822375 Cost after epoch 200: 0.9200938497267922 Cost after epoch 300: 0.903343937573097 Cost after epoch 400: 0.8940118246994508 Cost after epoch 500: 0.8907869381808411 Cost after epoch 600: 0.8886412688711486 Cost after epoch 700: 0.883147346063512 Cost after epoch 800: 0.8769772640048323 Cost after epoch 900: 0.8708887887701804 Cost after epoch 1000: 0.8647998259864911 Cost after epoch 1100: 0.8585222287945455 Cost after epoch 1200: 0.8520027876557966 Cost after epoch 1300: 0.8453233638152013 Cost after epoch 1400: 0.8385459841466859 Cost after epoch 1500: 0.8316835111651655 Cost after epoch 1600: 0.8247174011339953 Cost after epoch 1700: 0.8176315158989654 Cost after epoch 1800: 0.8104570160326438 Cost after epoch 1900: 0.8032652211959737 Cost after epoch 2000: 0.7961355729963056 Cost after epoch 2100: 0.789136583272275 Cost after epoch 2200: 0.7823112677961138 Cost after epoch 2300: 0.775672408352858 Cost after epoch 2400: 0.7692160518151918 Cost after epoch 2500: 0.7629412238838339 Cost after epoch 2600: 0.7568604281116301 Cost after epoch 2700: 0.7509972123530644 Cost after epoch 2800: 0.7453767681208928 Cost after epoch 2900: 0.7400187035221023 Cost after epoch 3000: 0.7349347611712702 Cost after epoch 3100: 0.730129367898223 Cost after epoch 3200: 0.725601027816927 Cost after epoch 3300: 0.7213437991582616 Cost after epoch 3400: 0.7173486494038583 Cost after epoch 3500: 0.7136045843917408 Cost after epoch 3600: 0.7100994808108275 Cost after epoch 3700: 0.7068206290443805 Cost after epoch 3800: 0.7037550652543418 Cost after epoch 3900: 0.700889787369363 Cost after epoch 4000: 0.6982119195684978 Cost after epoch 4100: 0.6957088518065728 Cost after epoch 4200: 0.6933683577849269 Cost after epoch 4300: 0.6911786877173168 Cost after epoch 4400: 0.6891286335859681 Cost after epoch 4500: 0.6872075678617828 Cost after epoch 4600: 0.6854054590502929 Cost after epoch 4700: 0.683712868375158 Cost after epoch 4800: 0.6821209316355645 Cost after epoch 4900: 0.6806213291218632 Cost after epoch 5000: 0.6792062448574632 Cost after epoch 5100: 0.6778683148658381 Cost after epoch 5200: 0.6766005632832309 Cost after epoch 5300: 0.6753963257932145 Cost after epoch 5400: 0.6742491632328644 Cost after epoch 5500: 0.6731527761104577 Cost after epoch 5600: 0.6721009459670979 Cost after epoch 5700: 0.6710875554769917 Cost after epoch 5800: 0.6701067756931479 Cost after epoch 5900: 0.6691535351159804 Cost after epoch 6000: 0.6682243248619312 Cost after epoch 6100: 0.6673181144122052 Cost after epoch 6200: 0.6664366873303731 Cost after epoch 6300: 0.6655836741706704 Cost after epoch 6400: 0.6647626331238312 Cost after epoch 6500: 0.6639756639685012 Cost after epoch 6600: 0.6632234242731055 Cost after epoch 6700: 0.6625058645359988 Cost after epoch 6800: 0.6618225888552545 Cost after epoch 6900: 0.6611725329792152 Cost after epoch 7000: 0.6605534213599379 Cost after epoch 7100: 0.6599615419171059 Cost after epoch 7200: 0.6593919856837961 Cost after epoch 7300: 0.6588392076262589 Cost after epoch 7400: 0.6582977974319442 Cost after epoch 7500: 0.6577634851557503 Cost after epoch 7600: 0.6572342998598006 Cost after epoch 7700: 0.6567112997447401 Cost after epoch 7800: 0.6561979977747271 Cost after epoch 7900: 0.6556985405736396 Cost after epoch 8000: 0.6552159905404351 Cost after epoch 8100: 0.6547517315398682 Cost after epoch 8200: 0.6543057765052426 Cost after epoch 8300: 0.6538773211378137 Cost after epoch 8400: 0.6534651786165192 Cost after epoch 8500: 0.6530680341023023 Cost after epoch 8600: 0.6526845710916861 Cost after epoch 8700: 0.6523135283781506 Cost after epoch 8800: 0.6519537262395217 Cost after epoch 8900: 0.6516040824779347 Cost after epoch 9000: 0.651263627378285 Cost after epoch 9100: 0.6509315193190078 Cost after epoch 9200: 0.6506070572083932 Cost after epoch 9300: 0.6502896806837162 Cost after epoch 9400: 0.6499789434550117 Cost after epoch 9500: 0.6496744387186555 Cost after epoch 9600: 0.6493756468740025 Cost after epoch 9700: 0.6490816631340898 Cost after epoch 9800: 0.6487907480421508 Cost after epoch 9900: 0.6484996428743001 ************************************************************ Elapsed time for the training is: 3.2573578357696533 Final Accuracy is: 57.81818181818181% . . We can see from this last example compared to the same one trained without dropout regularization that dropout helped reduce the overfitting of the training set as the accuracy here on test set is higher than the train set, but in the previous model without dropout the accuracy of the train set was a lot higher than in the test set . . parameters_train, costs_train, accuracy_train = L_layers_model_dropout(X_train, Y_train, [4, 10, 6, 1], 0.1, 10000, [1, 0.7, 0.5]) plot_cost(costs_train, accuracy_train, 0.1, 10000) . Number of Epochs: 10000 Learning rate: 0.1 ************************************************************ Cost after epoch 0: 0.9371750358170076 Cost after epoch 100: 0.7148449351877749 Cost after epoch 200: 0.6806591719968571 Cost after epoch 300: 0.683627520592218 Cost after epoch 400: 0.683306874108081 Cost after epoch 500: 0.6852215835186607 Cost after epoch 600: 0.6849644102928755 Cost after epoch 700: 0.6836512085341881 Cost after epoch 800: 0.682989000225858 Cost after epoch 900: 0.6817952025595082 Cost after epoch 1000: 0.6798348255477928 Cost after epoch 1100: 0.6773090079985971 Cost after epoch 1200: 0.6779162870758444 Cost after epoch 1300: 0.6722676948801636 Cost after epoch 1400: 0.661442829008715 Cost after epoch 1500: 0.6493602923077556 Cost after epoch 1600: 0.6363590624713782 Cost after epoch 1700: 0.6225662462414588 Cost after epoch 1800: 0.6125459862876222 Cost after epoch 1900: 0.6055462391376081 Cost after epoch 2000: 0.5989754086835076 Cost after epoch 2100: 0.5922602975116197 Cost after epoch 2200: 0.5853631144652075 Cost after epoch 2300: 0.5778488489482049 Cost after epoch 2400: 0.569948913615145 Cost after epoch 2500: 0.5616643474083453 Cost after epoch 2600: 0.5518486463133966 Cost after epoch 2700: 0.5414264948178018 Cost after epoch 2800: 0.5344312992336694 Cost after epoch 2900: 0.5299257080441242 Cost after epoch 3000: 0.5275792232468987 Cost after epoch 3100: 0.5267664728453205 Cost after epoch 3200: 0.5273544129535863 Cost after epoch 3300: 0.524542928453572 Cost after epoch 3400: 0.521090868365496 Cost after epoch 3500: 0.518360527818668 Cost after epoch 3600: 0.5168722631922018 Cost after epoch 3700: 0.5157819054192158 Cost after epoch 3800: 0.5159566061790889 Cost after epoch 3900: 0.5213128594159316 Cost after epoch 4000: 0.5254045353916642 Cost after epoch 4100: 0.5248746301508157 Cost after epoch 4200: 0.5115464440226909 Cost after epoch 4300: 0.48068800706782494 Cost after epoch 4400: 0.47573495259704257 Cost after epoch 4500: 0.4794009970788233 Cost after epoch 4600: 0.48204039300936735 Cost after epoch 4700: 0.4816140805329928 Cost after epoch 4800: 0.47978271350554763 Cost after epoch 4900: 0.4770560072895941 Cost after epoch 5000: 0.47359034033290487 Cost after epoch 5100: 0.4694805487548386 Cost after epoch 5200: 0.46518850496024694 Cost after epoch 5300: 0.4600975417580903 Cost after epoch 5400: 0.4543908863892356 Cost after epoch 5500: 0.44876644866376325 Cost after epoch 5600: 0.4431502702419958 Cost after epoch 5700: 0.43688248526522205 Cost after epoch 5800: 0.43012904525783513 Cost after epoch 5900: 0.4232206115781827 Cost after epoch 6000: 0.4159193549533355 Cost after epoch 6100: 0.4081606906008312 Cost after epoch 6200: 0.40046767165797403 Cost after epoch 6300: 0.39335103813115435 Cost after epoch 6400: 0.38819332773385673 Cost after epoch 6500: 0.38621489418748467 Cost after epoch 6600: 0.38419519876602687 Cost after epoch 6700: 0.38279087356991603 Cost after epoch 6800: 0.39009584537715464 Cost after epoch 6900: 0.38963648267248174 Cost after epoch 7000: 0.36619388203821696 Cost after epoch 7100: 0.3604506438451917 Cost after epoch 7200: 0.3540315974199042 Cost after epoch 7300: 0.34987729648659144 Cost after epoch 7400: 0.3456225840913732 Cost after epoch 7500: 0.33984948126997067 Cost after epoch 7600: 0.33241641483346246 Cost after epoch 7700: 0.3323883011279831 Cost after epoch 7800: 0.3257498732625113 Cost after epoch 7900: 0.31906545600960085 Cost after epoch 8000: 0.3144163370684355 Cost after epoch 8100: 0.31008024656269306 Cost after epoch 8200: 0.3065982402130161 Cost after epoch 8300: 0.30549339203923437 Cost after epoch 8400: 0.3022755352243474 Cost after epoch 8500: 0.2986581687118634 Cost after epoch 8600: 0.29437125661862423 Cost after epoch 8700: 0.289936591583961 Cost after epoch 8800: 0.28477461207361576 Cost after epoch 8900: 0.28098875515059396 Cost after epoch 9000: 0.27709194571247336 Cost after epoch 9100: 0.2742249443232413 Cost after epoch 9200: 0.2725427475317404 Cost after epoch 9300: 0.2713432557728788 Cost after epoch 9400: 0.27098448488141386 Cost after epoch 9500: 0.2706055515145624 Cost after epoch 9600: 0.2703230021998321 Cost after epoch 9700: 0.26985639051786103 Cost after epoch 9800: 0.26995412921999784 Cost after epoch 9900: 0.27048799046063576 ************************************************************ Elapsed time for the training is: 6.8307976722717285 Final Accuracy is: 97.90337283500456% . parameters_test, costs_test, accuracy_test = L_layers_model_dropout(X_test, Y_test, [4, 10, 6, 1], 0.1, 10000, [1, 0.7, 0.5]) plot_cost(costs_test, accuracy_test, 0.1, 10000) . Number of Epochs: 10000 Learning rate: 0.1 ************************************************************ Cost after epoch 0: 0.9480403969904232 Cost after epoch 100: 0.8646935339570276 Cost after epoch 200: 0.7960898963600662 Cost after epoch 300: 0.734895192349115 Cost after epoch 400: 0.6981339644487111 Cost after epoch 500: 0.6791158177226333 Cost after epoch 600: 0.6680931709398115 Cost after epoch 700: 0.6604269359473756 Cost after epoch 800: 0.6550968152604175 Cost after epoch 900: 0.6511660792942181 Cost after epoch 1000: 0.6481273010354601 Cost after epoch 1100: 0.6431097336126508 Cost after epoch 1200: 0.6404669210014711 Cost after epoch 1300: 0.6369591641947605 Cost after epoch 1400: 0.6341345910072915 Cost after epoch 1500: 0.632817535809138 Cost after epoch 1600: 0.6328593865847539 Cost after epoch 1700: 0.6336587584832982 Cost after epoch 1800: 0.6348588505774121 Cost after epoch 1900: 0.6366395852107821 Cost after epoch 2000: 0.6384113058457282 Cost after epoch 2100: 0.6396066581250579 Cost after epoch 2200: 0.6404562361236253 Cost after epoch 2300: 0.6411455531094967 Cost after epoch 2400: 0.6415405922151448 Cost after epoch 2500: 0.6413687464717178 Cost after epoch 2600: 0.6408787763362073 Cost after epoch 2700: 0.6403172863165718 Cost after epoch 2800: 0.6389642004728698 Cost after epoch 2900: 0.636923156701831 Cost after epoch 3000: 0.6335142680453358 Cost after epoch 3100: 0.6307369584609976 Cost after epoch 3200: 0.6290578175532052 Cost after epoch 3300: 0.6280449235666302 Cost after epoch 3400: 0.627492939428992 Cost after epoch 3500: 0.627032974816736 Cost after epoch 3600: 0.6264489156373656 Cost after epoch 3700: 0.625629538938457 Cost after epoch 3800: 0.6244669342705479 Cost after epoch 3900: 0.6231973457206892 Cost after epoch 4000: 0.6217968494596748 Cost after epoch 4100: 0.6203304188069166 Cost after epoch 4200: 0.6187282335861783 Cost after epoch 4300: 0.6169064801861278 Cost after epoch 4400: 0.6147690343325826 Cost after epoch 4500: 0.6122382163860398 Cost after epoch 4600: 0.6094600698079156 Cost after epoch 4700: 0.6065334562956082 Cost after epoch 4800: 0.6031242708479989 Cost after epoch 4900: 0.6001327204308363 Cost after epoch 5000: 0.5976594462577978 Cost after epoch 5100: 0.5950065111725362 Cost after epoch 5200: 0.5924698495797281 Cost after epoch 5300: 0.5898134806375049 Cost after epoch 5400: 0.586897383726315 Cost after epoch 5500: 0.5835823234765828 Cost after epoch 5600: 0.5798642131463283 Cost after epoch 5700: 0.5759376977145831 Cost after epoch 5800: 0.57196471650224 Cost after epoch 5900: 0.568049001515673 Cost after epoch 6000: 0.5642187464092653 Cost after epoch 6100: 0.5604562193356489 Cost after epoch 6200: 0.5567251444993542 Cost after epoch 6300: 0.5529836333757007 Cost after epoch 6400: 0.5491889234559553 Cost after epoch 6500: 0.5453029332166303 Cost after epoch 6600: 0.5413008081152235 Cost after epoch 6700: 0.5371759420190961 Cost after epoch 6800: 0.5329368989966621 Cost after epoch 6900: 0.5286013808381438 Cost after epoch 7000: 0.5241925801601747 Cost after epoch 7100: 0.5197353598063372 Cost after epoch 7200: 0.5152489831661822 Cost after epoch 7300: 0.5107406930404151 Cost after epoch 7400: 0.5062052406542676 Cost after epoch 7500: 0.5016292138267233 Cost after epoch 7600: 0.49699786825044373 Cost after epoch 7700: 0.49230529096836007 Cost after epoch 7800: 0.48756934428473714 Cost after epoch 7900: 0.48284341275367443 Cost after epoch 8000: 0.4782042911053055 Cost after epoch 8100: 0.4737287379748771 Cost after epoch 8200: 0.46949286860391204 Cost after epoch 8300: 0.46555260701680595 Cost after epoch 8400: 0.46191109887246534 Cost after epoch 8500: 0.4585328065544015 Cost after epoch 8600: 0.45537244861393755 Cost after epoch 8700: 0.45238850367989863 Cost after epoch 8800: 0.4495461427363094 Cost after epoch 8900: 0.4468166235156407 Cost after epoch 9000: 0.4441759600625595 Cost after epoch 9100: 0.4416038332312867 Cost after epoch 9200: 0.439083003989954 Cost after epoch 9300: 0.4365990808812663 Cost after epoch 9400: 0.43414035641019455 Cost after epoch 9500: 0.43169756214478544 Cost after epoch 9600: 0.42926357011709215 Cost after epoch 9700: 0.42683312280230956 Cost after epoch 9800: 0.42440263063437866 Cost after epoch 9900: 0.4219700125007899 ************************************************************ Elapsed time for the training is: 3.1022698879241943 Final Accuracy is: 98.18181818181819% . . In this example also the testing accuracy is higher than the training accuracy . . Using Hellinger Distance as Cost Function . def hellinger(A, Y): m = Y.shape[1] h = (1/ (m * np.sqrt(2))) * np.sum( np.power((np.sqrt(Y) - np.sqrt(A)), 2) ) return h . def backward_propagation_L_hellinger(A, Y, parameters, caches): grads = {} L = len(parameters)//2 dZ = ( (np.sqrt(A) - np.sqrt(Y)) / (np.sqrt(2) * np.sqrt(A)) ) * (A * (1-A)) for i in reversed(range(L)): current_cache = caches[i] A, Z = current_cache grads[&quot;dW&quot;+str(i+1)] = (1/m)*np.dot(dZ, A.T) grads[&quot;db&quot;+str(i+1)] = (1/m)*np.sum(dZ, axis = 1, keepdims = True) grads[&quot;dZ&quot;+str(i)] = np.dot(parameters[&quot;W&quot;+str(i+1)].T, dZ) * sigmoid_derivative(A) dZ = grads[&quot;dZ&quot;+str(i)] return grads . L Layers Model with Hellinger Distance as Cost Function . def L_layers_model_hellinger(X, Y, layers_dims, learning_rate, epochs): parameters = initialize_parameters_L(layers_dims) costs = [] accuracy = [] print(&quot;Number of Epochs: &quot;, epochs) print(&quot;Learning rate: &quot;, learning_rate) print(&quot;*&quot;*60) t0 = time.time() for i in range(epochs): A, caches = forward_propagation_L(X, parameters) cost = hellinger(A, Y) grads = backward_propagation_L_hellinger(A, Y, parameters, caches) parameters = update_params_L(grads, parameters, learning_rate) if i%100 == 0: costs.append(cost) accuracy.append(compute_accuracy(X, Y, parameters)) print(&quot;Cost after epoch {}: {}&quot;.format(i, cost)) t1 = time.time() print(&quot;*&quot;*60) print(&quot;Elapsed time for the training is: &quot;,t1-t0) final_accuracy = compute_accuracy(X, Y, parameters) print(&quot;Final Accuracy is: {}%&quot;.format(final_accuracy * 100)) return parameters, costs, accuracy . parameters, costs, accuracy = L_layers_model_hellinger(X, Y, [4, 10, 7, 5, 1], 0.001, 1000) plot_cost(costs,accuracy, 0.001, 1000) . Number of Epochs: 1000 Learning rate: 0.001 ************************************************************ Cost after epoch 0: 0.30145026684143195 Cost after epoch 100: 0.30022234820340604 Cost after epoch 200: 0.2989784601351985 Cost after epoch 300: 0.29771875986518137 Cost after epoch 400: 0.2964434313370276 Cost after epoch 500: 0.29515268614156465 Cost after epoch 600: 0.29384676439709306 Cost after epoch 700: 0.29252593556905926 Cost after epoch 800: 0.2911904992196735 Cost after epoch 900: 0.2898407856778482 ************************************************************ Elapsed time for the training is: 0.6422405242919922 Final Accuracy is: 44.460641399416915% . parameters, costs, accuracy = L_layers_model_hellinger(X, Y, [4, 10, 7, 5, 1], 0.001, 5000) plot_cost(costs,accuracy, 0.001, 5000) . Number of Epochs: 5000 Learning rate: 0.001 ************************************************************ Cost after epoch 0: 0.30145026684143195 Cost after epoch 100: 0.30022234820340604 Cost after epoch 200: 0.2989784601351985 Cost after epoch 300: 0.29771875986518137 Cost after epoch 400: 0.2964434313370276 Cost after epoch 500: 0.29515268614156465 Cost after epoch 600: 0.29384676439709306 Cost after epoch 700: 0.29252593556905926 Cost after epoch 800: 0.2911904992196735 Cost after epoch 900: 0.2898407856778482 Cost after epoch 1000: 0.2884771566197002 Cost after epoch 1100: 0.2871000055498193 Cost after epoch 1200: 0.2857097581735815 Cost after epoch 1300: 0.28430687265096993 Cost after epoch 1400: 0.2828918397226795 Cost after epoch 1500: 0.28146518269973125 Cost after epoch 1600: 0.2800274573084075 Cost after epoch 1700: 0.2785792513830481 Cost after epoch 1800: 0.2771211844001265 Cost after epoch 1900: 0.27565390684803964 Cost after epoch 2000: 0.27417809942821214 Cost after epoch 2100: 0.27269447208440184 Cost after epoch 2200: 0.2712037628585183 Cost after epoch 2300: 0.2697067365727909 Cost after epoch 2400: 0.2682041833397474 Cost after epoch 2500: 0.26669691690316316 Cost after epoch 2600: 0.26518577281489236 Cost after epoch 2700: 0.26367160645426957 Cost after epoch 2800: 0.2621552908985557 Cost after epoch 2900: 0.26063771465465113 Cost after epoch 3000: 0.259119779263998 Cost after epoch 3100: 0.257602396794201 Cost after epoch 3200: 0.2560864872323925 Cost after epoch 3300: 0.25457297579671045 Cost after epoch 3400: 0.25306279018343897 Cost after epoch 3500: 0.2515568577683378 Cost after epoch 3600: 0.25005610278145024 Cost after epoch 3700: 0.24856144347520884 Cost after epoch 3800: 0.24707378930593815 Cost after epoch 3900: 0.24559403814888198 Cost after epoch 4000: 0.24412307356664867 Cost after epoch 4100: 0.24266176215048332 Cost after epoch 4200: 0.24121095095303868 Cost after epoch 4300: 0.23977146503034463 Cost after epoch 4400: 0.23834410510948634 Cost after epoch 4500: 0.23692964539711234 Cost after epoch 4600: 0.23552883154233373 Cost after epoch 4700: 0.23414237876586733 Cost after epoch 4800: 0.23277097016546036 Cost after epoch 4900: 0.23141525520572612 ************************************************************ Elapsed time for the training is: 3.2761058807373047 Final Accuracy is: 44.460641399416915% . parameters, costs, accuracy = L_layers_model_hellinger(X, Y, [4, 4, 5, 1], 0.001, 5000) plot_cost(costs,accuracy, 0.001, 5000) . Number of Epochs: 5000 Learning rate: 0.001 ************************************************************ Cost after epoch 0: 0.2716714692731854 Cost after epoch 100: 0.27055870351833267 Cost after epoch 200: 0.26944491191404424 Cost after epoch 300: 0.26833046872219707 Cost after epoch 400: 0.26721575168930495 Cost after epoch 500: 0.2661011414345968 Cost after epoch 600: 0.26498702082268755 Cost after epoch 700: 0.26387377432311754 Cost after epoch 800: 0.26276178735914324 Cost after epoch 900: 0.26165144564825366 Cost after epoch 1000: 0.26054313453696304 Cost after epoch 1100: 0.25943723833248783 Cost after epoch 1200: 0.25833413963395413 Cost after epoch 1300: 0.2572342186658073 Cost after epoch 1400: 0.2561378526160947 Cost after epoch 1500: 0.2550454149822757 Cost after epoch 1600: 0.2539572749271824 Cost after epoch 1700: 0.25287379664769255 Cost after epoch 1800: 0.25179533875861226 Cost after epoch 1900: 0.2507222536941683 Cost after epoch 2000: 0.24965488712940817 Cost after epoch 2100: 0.24859357742368246 Cost after epoch 2200: 0.24753865508824446 Cost after epoch 2300: 0.24649044227985634 Cost after epoch 2400: 0.24544925232212605 Cost after epoch 2500: 0.2444153892561294 Cost after epoch 2600: 0.24338914742169143 Cost after epoch 2700: 0.24237081107051478 Cost after epoch 2800: 0.2413606540121511 Cost after epoch 2900: 0.2403589392936193 Cost after epoch 3000: 0.2393659189132761 Cost after epoch 3100: 0.23838183356935178 Cost after epoch 3200: 0.23740691244337117 Cost after epoch 3300: 0.23644137301848983 Cost after epoch 3400: 0.2354854209325942 Cost after epoch 3500: 0.23453924986583496 Cost after epoch 3600: 0.2336030414620974 Cost after epoch 3700: 0.23267696528374987 Cost after epoch 3800: 0.2317611787988657 Cost after epoch 3900: 0.23085582739997182 Cost after epoch 4000: 0.2299610444532537 Cost after epoch 4100: 0.2290769513770304 Cost after epoch 4200: 0.22820365774821286 Cost after epoch 4300: 0.22734126143536834 Cost after epoch 4400: 0.2264898487569388 Cost after epoch 4500: 0.22564949466309925 Cost after epoch 4600: 0.22482026293968946 Cost after epoch 4700: 0.2240022064326164 Cost after epoch 4800: 0.22319536729109898 Cost after epoch 4900: 0.22239977722811294 ************************************************************ Elapsed time for the training is: 1.7793834209442139 Final Accuracy is: 42.3469387755102% . Using Hellinger as a cost function worked to lower the cost, but the weird behaviour of the accuracy &amp; the decreasing cost starting around 0.3 needs further experimentation &amp; analysis .",
            "url": "https://moraouf.github.io/MoSpace/project/2021/10/14/artificial-neural-networks-from-scratch.html",
            "relUrl": "/project/2021/10/14/artificial-neural-networks-from-scratch.html",
            "date": " • Oct 14, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Visualizing Earnings Based On College Majors",
            "content": "import pandas as pd import numpy as np import matplotlib.pyplot as plt %matplotlib inline . recent_grads = pd.read_csv(&#39;recent-grads.csv&#39;) recent_grads.iloc[0] . Rank 1 Major_code 2419 Major PETROLEUM ENGINEERING Total 2339 Men 2057 Women 282 Major_category Engineering ShareWomen 0.120564 Sample_size 36 Employed 1976 Full_time 1849 Part_time 270 Full_time_year_round 1207 Unemployed 37 Unemployment_rate 0.0183805 Median 110000 P25th 95000 P75th 125000 College_jobs 1534 Non_college_jobs 364 Low_wage_jobs 193 Name: 0, dtype: object . recent_grads.head() . Rank Major_code Major Total Men Women Major_category ShareWomen Sample_size Employed ... Part_time Full_time_year_round Unemployed Unemployment_rate Median P25th P75th College_jobs Non_college_jobs Low_wage_jobs . 0 1 | 2419 | PETROLEUM ENGINEERING | 2339.0 | 2057.0 | 282.0 | Engineering | 0.120564 | 36 | 1976 | ... | 270 | 1207 | 37 | 0.018381 | 110000 | 95000 | 125000 | 1534 | 364 | 193 | . 1 2 | 2416 | MINING AND MINERAL ENGINEERING | 756.0 | 679.0 | 77.0 | Engineering | 0.101852 | 7 | 640 | ... | 170 | 388 | 85 | 0.117241 | 75000 | 55000 | 90000 | 350 | 257 | 50 | . 2 3 | 2415 | METALLURGICAL ENGINEERING | 856.0 | 725.0 | 131.0 | Engineering | 0.153037 | 3 | 648 | ... | 133 | 340 | 16 | 0.024096 | 73000 | 50000 | 105000 | 456 | 176 | 0 | . 3 4 | 2417 | NAVAL ARCHITECTURE AND MARINE ENGINEERING | 1258.0 | 1123.0 | 135.0 | Engineering | 0.107313 | 16 | 758 | ... | 150 | 692 | 40 | 0.050125 | 70000 | 43000 | 80000 | 529 | 102 | 0 | . 4 5 | 2405 | CHEMICAL ENGINEERING | 32260.0 | 21239.0 | 11021.0 | Engineering | 0.341631 | 289 | 25694 | ... | 5180 | 16697 | 1672 | 0.061098 | 65000 | 50000 | 75000 | 18314 | 4440 | 972 | . 5 rows × 21 columns . recent_grads.tail() . Rank Major_code Major Total Men Women Major_category ShareWomen Sample_size Employed ... Part_time Full_time_year_round Unemployed Unemployment_rate Median P25th P75th College_jobs Non_college_jobs Low_wage_jobs . 168 169 | 3609 | ZOOLOGY | 8409.0 | 3050.0 | 5359.0 | Biology &amp; Life Science | 0.637293 | 47 | 6259 | ... | 2190 | 3602 | 304 | 0.046320 | 26000 | 20000 | 39000 | 2771 | 2947 | 743 | . 169 170 | 5201 | EDUCATIONAL PSYCHOLOGY | 2854.0 | 522.0 | 2332.0 | Psychology &amp; Social Work | 0.817099 | 7 | 2125 | ... | 572 | 1211 | 148 | 0.065112 | 25000 | 24000 | 34000 | 1488 | 615 | 82 | . 170 171 | 5202 | CLINICAL PSYCHOLOGY | 2838.0 | 568.0 | 2270.0 | Psychology &amp; Social Work | 0.799859 | 13 | 2101 | ... | 648 | 1293 | 368 | 0.149048 | 25000 | 25000 | 40000 | 986 | 870 | 622 | . 171 172 | 5203 | COUNSELING PSYCHOLOGY | 4626.0 | 931.0 | 3695.0 | Psychology &amp; Social Work | 0.798746 | 21 | 3777 | ... | 965 | 2738 | 214 | 0.053621 | 23400 | 19200 | 26000 | 2403 | 1245 | 308 | . 172 173 | 3501 | LIBRARY SCIENCE | 1098.0 | 134.0 | 964.0 | Education | 0.877960 | 2 | 742 | ... | 237 | 410 | 87 | 0.104946 | 22000 | 20000 | 22000 | 288 | 338 | 192 | . 5 rows × 21 columns . recent_grads.describe() . Rank Major_code Total Men Women ShareWomen Sample_size Employed Full_time Part_time Full_time_year_round Unemployed Unemployment_rate Median P25th P75th College_jobs Non_college_jobs Low_wage_jobs . count 173.000000 | 173.000000 | 172.000000 | 172.000000 | 172.000000 | 172.000000 | 173.000000 | 173.000000 | 173.000000 | 173.000000 | 173.000000 | 173.000000 | 173.000000 | 173.000000 | 173.000000 | 173.000000 | 173.000000 | 173.000000 | 173.000000 | . mean 87.000000 | 3879.815029 | 39370.081395 | 16723.406977 | 22646.674419 | 0.522223 | 356.080925 | 31192.763006 | 26029.306358 | 8832.398844 | 19694.427746 | 2416.329480 | 0.068191 | 40151.445087 | 29501.445087 | 51494.219653 | 12322.635838 | 13284.497110 | 3859.017341 | . std 50.084928 | 1687.753140 | 63483.491009 | 28122.433474 | 41057.330740 | 0.231205 | 618.361022 | 50675.002241 | 42869.655092 | 14648.179473 | 33160.941514 | 4112.803148 | 0.030331 | 11470.181802 | 9166.005235 | 14906.279740 | 21299.868863 | 23789.655363 | 6944.998579 | . min 1.000000 | 1100.000000 | 124.000000 | 119.000000 | 0.000000 | 0.000000 | 2.000000 | 0.000000 | 111.000000 | 0.000000 | 111.000000 | 0.000000 | 0.000000 | 22000.000000 | 18500.000000 | 22000.000000 | 0.000000 | 0.000000 | 0.000000 | . 25% 44.000000 | 2403.000000 | 4549.750000 | 2177.500000 | 1778.250000 | 0.336026 | 39.000000 | 3608.000000 | 3154.000000 | 1030.000000 | 2453.000000 | 304.000000 | 0.050306 | 33000.000000 | 24000.000000 | 42000.000000 | 1675.000000 | 1591.000000 | 340.000000 | . 50% 87.000000 | 3608.000000 | 15104.000000 | 5434.000000 | 8386.500000 | 0.534024 | 130.000000 | 11797.000000 | 10048.000000 | 3299.000000 | 7413.000000 | 893.000000 | 0.067961 | 36000.000000 | 27000.000000 | 47000.000000 | 4390.000000 | 4595.000000 | 1231.000000 | . 75% 130.000000 | 5503.000000 | 38909.750000 | 14631.000000 | 22553.750000 | 0.703299 | 338.000000 | 31433.000000 | 25147.000000 | 9948.000000 | 16891.000000 | 2393.000000 | 0.087557 | 45000.000000 | 33000.000000 | 60000.000000 | 14444.000000 | 11783.000000 | 3466.000000 | . max 173.000000 | 6403.000000 | 393735.000000 | 173809.000000 | 307087.000000 | 0.968954 | 4212.000000 | 307933.000000 | 251540.000000 | 115172.000000 | 199897.000000 | 28169.000000 | 0.177226 | 110000.000000 | 95000.000000 | 125000.000000 | 151643.000000 | 148395.000000 | 48207.000000 | . recent_grads.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 173 entries, 0 to 172 Data columns (total 21 columns): # Column Non-Null Count Dtype -- -- 0 Rank 173 non-null int64 1 Major_code 173 non-null int64 2 Major 173 non-null object 3 Total 172 non-null float64 4 Men 172 non-null float64 5 Women 172 non-null float64 6 Major_category 173 non-null object 7 ShareWomen 172 non-null float64 8 Sample_size 173 non-null int64 9 Employed 173 non-null int64 10 Full_time 173 non-null int64 11 Part_time 173 non-null int64 12 Full_time_year_round 173 non-null int64 13 Unemployed 173 non-null int64 14 Unemployment_rate 173 non-null float64 15 Median 173 non-null int64 16 P25th 173 non-null int64 17 P75th 173 non-null int64 18 College_jobs 173 non-null int64 19 Non_college_jobs 173 non-null int64 20 Low_wage_jobs 173 non-null int64 dtypes: float64(5), int64(14), object(2) memory usage: 28.5+ KB . print(&#39;Number of Rows Before :&#39;, len(recent_grads)) recent_grads = recent_grads.dropna() print(&#39;Number of Rows After :&#39;, len(recent_grads)) . Number of Rows Before : 173 Number of Rows After : 172 . p1 = recent_grads.plot(x = &#39;Sample_size&#39;, y = &#39;Median&#39;, kind = &#39;scatter&#39;) p2 = recent_grads.plot(x = &#39;Sample_size&#39;, y = &#39;Unemployment_rate&#39;, kind = &#39;scatter&#39;) p3 = recent_grads.plot(x = &#39;Full_time&#39;, y = &#39;Median&#39;, kind = &#39;scatter&#39;) p4 = recent_grads.plot(x = &#39;ShareWomen&#39;, y = &#39;Unemployment_rate&#39;, kind = &#39;scatter&#39;) p5 = recent_grads.plot(x = &#39;Men&#39;, y = &#39;Median&#39;, kind = &#39;scatter&#39;) p6 = recent_grads.plot(x = &#39;Women&#39;, y = &#39;Median&#39;, kind = &#39;scatter&#39;) . h1 = recent_grads[&#39;Sample_size&#39;].hist(bins = 10, range = (0,4500)) h1.set_title(&#39;Sample_size&#39;) . Text(0.5, 1.0, &#39;Sample_size&#39;) . h2 = recent_grads[&#39;Median&#39;].hist(bins = 20, range = (22000,110000)) h2.set_title(&#39;Median&#39;) . Text(0.5, 1.0, &#39;Median&#39;) . h3 = recent_grads[&#39;Employed&#39;].hist(bins = 10, range = (0,300000)) h3.set_title(&#39;Employed&#39;) . Text(0.5, 1.0, &#39;Employed&#39;) . h4 = recent_grads[&#39;Full_time&#39;].hist(bins = 10, range = (0,250000)) h4.set_title(&#39;Full_time&#39;) . Text(0.5, 1.0, &#39;Full_time&#39;) . h5 = recent_grads[&#39;ShareWomen&#39;].hist(bins = 20, range = (0,1)) h5.set_title(&#39;Share Women&#39;) . Text(0.5, 1.0, &#39;Share Women&#39;) . h6 = recent_grads[&#39;Men&#39;].hist(bins = 10, range = (110,175000)) h6.set_title(&#39;Men&#39;) . Text(0.5, 1.0, &#39;Men&#39;) . h7 = recent_grads[&#39;Women&#39;].hist(bins = 10, range = (0,300000)) h7.set_title(&#39;Women&#39;) . Text(0.5, 1.0, &#39;Women&#39;) . from pandas.plotting import scatter_matrix matrix1 = scatter_matrix(recent_grads[[&#39;Sample_size&#39;, &#39;Median&#39;]]) matrix2 = scatter_matrix(recent_grads[[&#39;Sample_size&#39;, &#39;Median&#39;, &#39;Unemployment_rate&#39;]]) . recent_grads[&#39;ShareWomen&#39;][:10].plot(kind = &#39;bar&#39;) . &lt;AxesSubplot:&gt; . recent_grads[&#39;ShareWomen&#39;][-10:-1].plot(kind = &#39;bar&#39;) . &lt;AxesSubplot:&gt; . recent_grads[:10].plot.bar(x=&#39;Major_category&#39;, y=&#39;Unemployment_rate&#39;) # OR # recent_grads[&#39;Unemployment_rate&#39;][:10].plot(kind = &#39;bar&#39;) . &lt;AxesSubplot:xlabel=&#39;Major_category&#39;&gt; . recent_grads[-10:-1].plot.bar(x=&#39;Major_category&#39;, y=&#39;Unemployment_rate&#39;) . &lt;AxesSubplot:xlabel=&#39;Major_category&#39;&gt; .",
            "url": "https://moraouf.github.io/MoSpace/project/2021/05/12/visualizing-earnings.html",
            "relUrl": "/project/2021/05/12/visualizing-earnings.html",
            "date": " • May 12, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Neo4j in handling IMDB dataset",
            "content": "Data Loading Codes . :auto using periodic commit . load csv with headers from “file:///actors.csv” as row fieldterminator “;” . with row, (case row.sex when “M” then “Male” when “F” then “Female” else “Other” end) as sex, split(row.name, “,”) as name . create (a:Actor) . set a.actorId = toInteger(row.actorid), a.name = (ltrim(name[1])+ “ “ + name[0]), a.sex = sex . . 1- Create Constraint on Movies Ids: . create constraint on (m:Movie) ASSERT m.movieId is unique . . 2- Load Movies . :auto using periodic commit . load csv with headers from “file:///movies.csv” as row fieldterminator “;” . create (m:Movie) . set m.movieId = toInteger(row.movieid), m.title = trim(split(row.title, “(“)[0]), m.year = toInteger(row.year) . . 3- Load Directors: . :auto using periodic commit . load csv with headers from “file:///directors.csv” as row fieldterminator “;” . create (d:Director) . set d.directorId = toInteger(row.directorid), d.name = row.name . . 4- Load Actors: . :auto using periodic commit . load csv with headers from “file:///actors.csv” as row fieldterminator “;” . with row, (case row.sex when “M” then “Male” when “F” then “Female” else “Other” end) as sex . create (a:Actor) . set a.actorId = toInteger(row.actorid), a.name = row.name, a.sex = sex . . 5- Load Writers: . :auto using periodic commit 500 . load csv with headers from “file:///writers.csv” as row fieldterminator “;” . create (w:Writer) . set w.writerId = toInteger(row.writerid), w.name = row.name . . 6- Create WRITTEN_BY: . :auto using periodic commit 500 . load csv with headers from “file:///moviestowriters.csv” as row fieldterminator “;” . match (m:Movie {movieId: toInteger(row.movieid)}) . match (w:Writer {writerId: toInteger(row.writerid)}) . create (m)-[:WRITTEN_BY]-&gt;(w) . . 7- Create DIRECTED_BY: . :auto using periodic commit 500 . load csv with headers from “file:///moviestodirectors.csv” as row fieldterminator “;” . match (d:Director {directorId : toInteger(row.directorid)}) . match (m:Movie {movieId : toInteger(row.movieid)}) . create (d)&lt;-[:DIRECTED_BY]-(m) . . 8- Create ACTED_IN: . :auto using periodic commit 500 . load csv with headers from “file:///moviestoactors.csv” as row fieldterminator “;” . match (m:Movie {movieId : toInteger(row.movieid)}) . match (a:Actor {actorId : toInteger(row.actorid)}) . create (a)-[:ACTED_IN]-&gt;(m) . . 9- Set genre property of Movies: . :auto using periodic commit . load csv with headers from “file:///moviestodirectors.csv” as row fieldterminator “;” . match (m:Movie {movieId : toInteger(row.movieid)}) . set m.genre = row.genre . . 10- Set rank &amp; votes properties of movies: . :auto using periodic commit . load csv with headers from “file:///ratings.csv” as row fieldterminator “;” . match (m:Movie {movieId : toInteger(row.movieid)}) . set m.rank = toFLoat(row.rank), m.votes= toInteger(row.votes) . . 11- Set time property of Movies as duration: . :auto using periodic commit . load csv with headers from “file:///runningtimes.csv” as row fieldterminator “;” . match (m:Movie {movieId : toInteger(row.movieid)}) . set m.time = duration({minutes: toInteger(row.time1)}) . Graph Structure . . . . . . . Data Excution Codes . 1- How many of movies have been directed by Ron Howard ? . match (m:Movie)-[:DIRECTED_BY]-&gt;(d:Director) . where d.name contains “Howard” and d.name contains “Ron” . return count(m.title) . . . . 2- Write a single query that shows both the number of female actors and the number of male actors in the dataset: . match (a:Actor) . where a.sex = “Female” . with count(a) as female . match (b:Actor) . where b.sex = “Male” . with count(b) as male, female . return male, female . . . 3- What is the year of the oldest movie listed in the database? . match (m:Movie) . return m.year order by m.year limit 1 . . . 4- List the movie titles and number of directors involved for movies with more than 5 directors: . match (m:Movie)-[:DIRECTED_BY]-&gt;(d:Director) . with m, count(d) as dircount . where dircount &gt; 5 . return m.title, dircount . . . 5- Number of movies with a running time of between 15m 00sec to 20mins 00 seconds (inclusive) Note: Count all versions of a movie, not just the originals; use the time1 column for movie lengths: . match (m:Movie) . where m.time.minutes &gt;= 15 AND m.time.minutes &lt;= 20 . return count(m) . . . 6- Give the movie titles which star both Bill Nighy and Ewan McGregor (i.e. both actors were in the same film) . match (b:Actor)-[:ACTED_IN]-&gt;(m:Movie)&lt;-[:ACTED_IN]-(a:Actor) . where a.name contains “Bill” and a.name contains “Nighy” and b.name contains “Ewan” and b.name contains “McGregor” . return m.title . . . 7- How many movies have fewer male actors than female actors? . match (a:Actor)-[:ACTED_IN]-&gt;(m:Movie) . where a.sex = “Female” . with count(a) as females, m . match (b:Actor)-[:ACTED_IN]-&gt;(m) . where b.sex = “Male” . with count(b) as males,females, m . where males &lt; females . return count(m) as Count_of_Movies . . . 8- List the actors (male/female) that have worked together on more than 10 films, include their names and number of films they’ve co-starred in: . match (a:Actor)-[:ACTED_IN]-&gt;(m:Movie)&lt;-[:ACTED_IN]-(b:Actor) . with a, b, count(m) as moviecount . where moviecount &gt; 10 and a.actorId &lt; b.actorId . return a.name, b.name, moviecount . . . 9- List the number of movies released per decade in this dataset, as listed below: (1970-79, 1980-89,1990-99,2000-2009, 2010-2019) : . match (m:Movie) . where 1970&lt;=m.year&lt;1980 . with count(m) as 1970-1979 . match (m:Movie) . where 1980&lt;=m.year&lt;1990 . with count(m) as 1980-1989, 1970-1979 . match (m:Movie) . where 1990&lt;=m.year&lt;2000 . with count(m) as 1990-1999, 1970-1979,1980-1989 . match (m:Movie) . where 2000&lt;=m.year&lt;2010 . with count(m) as 2000-2009, 1970-1979,1980-1989,1990-1999 . match (m:Movie) . where 2010&lt;=m.year&lt;2020 . with count(m) as 2010-2019, 1970-1979,1980-1989,1990-1999,2000-2009 . return 1970-1979, 1980-1989,1990-1999,2000-2009,2010-2019 . . . 10- How many movies did Tom Hanks act in in 1993? . match (a:Actor)-[:ACTED_IN]-&gt;(m:Movie) . where a.name contains “Tom” and a.name contains “Hanks” and m.year = 1993 . return count(m) . . . **11- Based on the average rank per movie genre, which are the top 4 highest scoring genres which have received 15,000 or more votes ** . match (m:Movie)-[r:DIRECTED_BY]-&gt;() . where m.votes &gt; 15000 . return r.genre as Genre, avg(m.rank) as Average_Rank order by Average_Rank desc limit 4 . . . 12- Show the shortest path between the actor ‘Leonard Nimoy’ and the 1985 ‘Back to the Future’ movie. Include nodes and edges – answer can be shown as an image or text description in form (a)-[ ]-&gt;(b)-[ ]-&gt; (c)… . match p=shortestPath((a:Actor)-[*]-(m:Movie)) . where a.name contains “Leonard” and a.name contains “Nimoy” and m.title = “Back to the Future” . return p . . . 13- List all actors (male/female) that have starred in 9 or more different film genres (show names, and number of genres): . match (a:Actor)-[:ACTED_IN]-&gt;(m:Movie) . with a, count(distinct m.genre) as genrecount . where genrecount&gt;=9 . return a.name, genrecount . . . 14- How many movies have a female actor that also directed and wrote the movie? . match (a:Actor)-[:ACTED_IN]-&gt;(m:Movie), (w:Writer)&lt;-[:WRITTEN_BY]-(m),(d:Director)&lt;-[:DIRECTED_BY]-(m) . where a.sex = “Female” and a.name = w.name = d.name . return count(distinct m) . . . 15- How many movies have been written and directed by an actor (male or female) that they didn’t star in? (i.e. the person who wrote and directed the movie is a film star but didn’t appear in the movie) : . match (a:Actor) . with a . match (w:Writer)&lt;-[:WRITTEN_BY]-(m:Movie)-[:DIRECTED_BY]-&gt;(d:Director) . where a.name = w.name = d.name and not (a)-[:ACTED_IN]-&gt;(m) . with distinct w.name as writers, d.name as directors, m.title as movies . return count(movies) .",
            "url": "https://moraouf.github.io/MoSpace/post/2021/04/01/Neo4j.html",
            "relUrl": "/post/2021/04/01/Neo4j.html",
            "date": " • Apr 1, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Creating a blog",
            "content": "A blog is born . These days, there are countless blogging alternatives covering an extremely wide range of needs, from writing everything in .docx format to customizing every minor nit with CSS and HTML. It is not difficult to get lost in this sea of blogging alternatives and end up being unable to choose one of the alternatives and set the blog up. I already experimented with a blog during my Google Summer of Code internship and I have now started this new blog. Thus, being this my second blog, I am far from an expert in blogging platforms. . I cannot write a complete guide for prospective bloggers, however, I still feel like writing about how I created and configured this blog can be useful to anyone who is considering creating a blog somewhat similar to this one. As you may have guessed already from other pages in the blog, I highly value understanding what I do and being able to modify (or at least see) its inner workings. . The first section is an overview of the technologies and libraries used in this blog. Afterwards there are 3 sections covering the base of blogging: writing the content, building the blog and hosting it. These cover the base elements needed to get the blog running. Eventually, there is one section on more advanced configuration and on the features I value the most. . Overview . The content for this blog is written using either jupyter notebooks, markdown or docx files. Posts are then converted thanks to fastpages to markdown and html files that Jekyll can use to build the static website. Every time a new commit is pushed to GitHub, GitHub Actions are used to automatically build the website and push it to the gh-pages branch. The website stored in the gh-pages branch is then hosted on GitHub pages. Here is the diagram of the workflow from the fastpages website: . . Writing content . Writing the content using Markdown is one of the most common alternatives for writing blog posts. Writing in Markdown has many advantages and produces files which are readable both rendered and without rendering. To write code tutorials however, they are not too convenient as we need to manually execute each cell and add the output of the code to the post. . If we use Jupyter notebooks instead, we can keep the code, its output and the explanations in the same executable file. This is really convenient as I can easily rerun the notebooks whenever there has been a significant change in the libraries used and keep the tutorials up to date. The main drawback of writing posts in Jupyter notebooks is having to convert the notebooks to markdown or html so the post can be added to the blog. I am taking advantage of fastpages to both take care of the conversion and to automate the process. . Eventually, I only have to write my posts in either markdown or Jupyter notebook files and push them to GitHub. This is crucial to me as it makes creating new posts be only about writing! I don’t have to take care about conversion. . Building the blog . This blog is built using Jekyll, an open source static website generator. Roughly speaking, it takes a configuration file and the website content as markdown or html files and generates a static website where the theme has been filled with the content, pages and blog posts. The configuration file, _config.yml contains information such as the theme or the markdown parser to be used. . fastpages uses the default Jekyll theme: minima. Even though minima has many features and everything works straight away with fastpages, I wanted to use a different theme, to give another look to the website and make it more attractive to readers. I therefore tried changing the theme specified in _config.yml for several new themes I found more visually attractive. I found basically basic (see screenshots below) and massively to work quite well right out of the box. Most of the themes did not work at all due to incompatibilities with fastpages. After comparing both themes for a while, I decided to go with massively. I therefore tweaked the theme to fix the minor incompatibilities it presented with fastpages. This is still a work in progress, not everything that works with fastpages+minima works with massively yet. . Some of the tweaks will be detailed in the last two sections of the post, but most of them will not be covered here. If you are interested in any of the two themes and their compatibility with fastpages please reach out in an issue, or what’s nearly the same (thanks utterances), comment below. . Hosting a static website . As explained in the overview, this website is hosted by GitHub pages. Even though this was initially the one that seemed more difficult to me, I actually found this step simpler than the other two. Basically GitHub Pages takes care of everything. . The main friction I encountered while using GitHub pages was the .nojekyll file. There are two main ways of interacting with GitHub pages. The first alternative is to push to gh-pages a directory containing a _config.yml file with the Jekyll configuration and the content in Markdown and HTML format. GitHub then builds the site for you using Jekyll. The second alternative is to build the site and push the result to gh-pages branch together with a .nojekyll file. The .nojekyll file tells GitHub to not build the site with Jekyll and host directly the contents of the branch instead. . In this particular case, we are using GitHub Actions from fastpages to convert the posts and build the site using Jekyll, so we are actually using the second alternative. Luckily, thanks to fastpages, these two alternatives do not affect the writing process at all. . The best of many worlds . So far we have described how to create and maintain a plain blog, the main difference with forking the minima theme and writing a blog in markdown is the ability to write posts in Jupyter notebooks. I would like this blog to be more than that. This blog combines features from fastpages, massively and basically basic. My aim was to hand pick the features of each source that were a better fit to my idea and needs for this website. . The blog is mobile friendly thanks to the massively theme plus some extra tweaks from Basically Basic theme. I also borrowed the text size scaling from Basically Basic, now whatever the screen size, the text should always be readable. . I customized the favicon too. As you can see, it is neither the one of fastpages nor of massively theme. It is a custom image of a MATLAB-style waterfall plot of a 2d group MOM prior. You can see the regular scale image used to generate the favicon below, more details on what is a group MOM prior will come in a future post. This was actually simpler than I expected, there are converters online to generate favicons from regular images, and then saving the favicon as images/favicon.ico is enough for everything to work. . . fastpages also has support for many other awesome features such as google analytics, comments or SEO tag management. For now, I decided to use google analytics and comments powered by utterances but remove the SEO related code. I may add it again at some point after I better understand how they work. . Notebook shields . Many of my posts will be tutorials written as Jupyter notebooks. Being able to include the content from ipynb files to the blog is awesome, but why stop here? fastpages allows to add 3 shields (shown below) to notebook posts so that the notebook can be opened in GitHub, Google Colab or Binder. Binder! :heart: . Binder generates a container where the notebook can be executed interactively without requiring any local installation. This allows readers to run the code included in the tutorial while they read it with little to no effort, just click on the binder shield. Binder cannot work straight out of the box however, to create the container it needs to know what should be installed. I have used an environment.yml to install the required Python libraries with Conda and a JuliaProject.toml for the Julia libraries. I may add also some R dependencies too. Guidance on specifying requirements for Binder can be found in its docs. . Tag Archive . The tag archive, similarly to the index of a reference book can be an invaluable help to readers in navigating the website and finding posts relevant to their interests. fastpages already includes a tag archive page unlike the massively theme, however, I think its single list formatting does not scale well with the number of posts and different tags. The table format from Basically Basic was much more attractive to me, so I combined the tag archive page from BB theme with the square layout of massively. I also removed the post image to get a more compact look. . Colour schemes and syntax highlighting . Choosing a colour scheme for our code editors can be a very personal choice influenced by many different reasons. When configuring our code editor, we can decide whatever we want and choose to completely ignore everyone else in the entire world. With websites and other public resources however this is not a choice. Websites should be careful with their colour themes to be accessible to people with colour vision deficiency. One clear example of a bad practice on this is GitHub symbols of open and closed pull requests. The image below uses the Mozilla add-on Let’s get colour blind. to simulate how someone with Deuteranomaly sees a list of GitHub pull requests. . I used this same add-on to make sure everything could be seen without too much effort and did a couple of changes to the fastpages-dracula pygments theme for syntax highlighting. I also tried high contrast colour schemes so feel free to contact me if you were to need help modifying the colour scheme of your website. The plots in my post use the arviz-darkgrid theme whose palette is colourblind friendly, so I have not modified them. In the future I’ll try to be more careful and try to not rely only on colour to distinguish lines in plots. . Social Media links and serch icon . Thanks to fontawesome I have been able to add links to GitHub and Twitter profiles and to the blog’s Atom feed: Subscribe in the navigation bar. Moreover, there is also a link to the search page. Search is powered by Lunr via fastpages. . All these social media links are also in the copyright notice found in the website footer, where thanks to academicons, the links to my ORCID and Google Scholar profiles are also available. Fontawsome icons worked out of the box with all the 3 themes I tinkered with, while academicons was not supported by any of them and had to be added manually following the instructions on their website. .",
            "url": "https://moraouf.github.io/MoSpace/post/jekyll/2020/07/10/blog.html",
            "relUrl": "/post/jekyll/2020/07/10/blog.html",
            "date": " • Jul 10, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "ArviZ in depth: plot_trace",
            "content": "Introduction . plot_trace is one of the most common plots to assess the convergence of MCMC runs, therefore, it is also one of the most used ArviZ functions. plot_trace has a lot of parameters that allow creating highly customizable plots, but they may not be straightforward to use. There are many reasons that can explain this convolutedness of the arguments and their format, there is no clear culprit: ArviZ has to integrate with several libraries such as xarray and matplotlib which provide amazing features and customization power, and we&#39;d like to allow ArviZ users to access all these features. However, we also aim to keep ArviZ usage simple and with sensible defaults; plot_xyz(idata) should generate acceptable results in most situations. . This post aims to be an extension to the API section on plot_trace, focusing mostly on arguments where examples may be lacking and arguments that appear often in questions posted to ArviZ issues. . Therefore, the most common arguments such as var_names will not be covered, and for arguments that I do not remeber appearing in issues or generating confusion only some examples will be shown without an in depth description. . import arviz as az import matplotlib.pyplot as plt import numpy as np import xarray as xr # html render is not correctly rendered in blog, # comment the line below if in jupyter xr.set_options(display_style=&quot;text&quot;) rng = np.random.default_rng() . az.style.use(&quot;arviz-darkgrid&quot;) . idata_centered = az.load_arviz_data(&quot;centered_eight&quot;) idata = az.load_arviz_data(&quot;rugby&quot;) . The kind argument . az.plot_trace generates two columns. The left one calls plot_dist to plot KDE/Histogram of the data, and the right column can contain either the trace itself (which gives the name to the plot) or a rank plot for which two visualizations are available. Rank plots are an alternative to trace plots, see https://arxiv.org/abs/1903.08008 for more details. . fig, axes = plt.subplots(3,2, figsize=(12,6)) for i, kind in enumerate((&quot;trace&quot;, &quot;rank_bars&quot;, &quot;rank_vlines&quot;)): az.plot_trace(idata, var_names=&quot;home&quot;, kind=kind, ax=axes[i,:]); fig.tight_layout() . /home/oriol/venvs/arviz-dev/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: This figure was using constrained_layout==True, but that is incompatible with subplots_adjust and or tight_layout: setting constrained_layout==False. after removing the cwd from sys.path. . The divergences argument . If present, divergences are indicated as a black rugplot in both columns of the trace plot. By default they are placed at the bottom of the plot, but they can be placed at the top or hidden. . az.plot_trace(idata_centered, var_names=&quot;tau&quot;); . az.plot_trace(idata_centered, var_names=&quot;tau&quot;, divergences=None); . The rug argument . rug adds a rug plot with the posterior samples at the bottom of the distribution plot, there are no changes in the trace plot column. . ax = az.plot_trace(idata, var_names=&quot;home&quot;, rug=True, rug_kwargs={&quot;alpha&quot;: .4}) . But what about having both rug and divergences at the same time? Fear not, ArviZ automatically modifies the default for divergences from bottom to top to prevent rug and divergences from overlapping: . az.plot_trace(idata_centered, var_names=&quot;mu&quot;, rug=True); . The lines argument . The description about lines in plot_trace&#39;s docstring is the following: . lines : list of tuple of (str, dict, array_like), optional . List of (var_name, {‘coord’: selection}, [line, positions]) to be overplotted as vertical lines on the density and horizontal lines on the trace. . It is possible that the first thought after reading this line is similar to &quot;What is with this weird format?&quot; Well, this format is actually the stardard way ArviZ uses to iterate over xarray.Dataset objects because it contains all the info about the variable and the selected coordinates as well as the values themselves. The main helper function that handles this is arviz.plots.plot_utils.xarray_var_iter. . This section will be a little different from the other ones, and will focus on boosting plot_trace capabilities with internal ArviZ functions. You may want to skip to the section altogether of go straigh to the end. . Let&#39;s see what xarray_var_iter does with a simple dataset. We will create a dataset with two variables: a will be a 2x3 matrix and b will be a scalar. In addition, the dimensions of a will be labeled. . ds = xr.Dataset({ &quot;a&quot;: ((&quot;pos&quot;, &quot;direction&quot;), rng.normal(size=(2,3))), &quot;b&quot;: 12, &quot;pos&quot;: [&quot;top&quot;, &quot;bottom&quot;], &quot;direction&quot;: [&quot;x&quot;, &quot;y&quot;, &quot;z&quot;] }) ds . &lt;xarray.Dataset&gt; Dimensions: (direction: 3, pos: 2) Coordinates: * pos (pos) &lt;U6 &amp;#x27;top&amp;#x27; &amp;#x27;bottom&amp;#x27; * direction (direction) &lt;U1 &amp;#x27;x&amp;#x27; &amp;#x27;y&amp;#x27; &amp;#x27;z&amp;#x27; Data variables: a (pos, direction) float64 -0.5306 0.8029 0.7965 ... 0.4623 -0.128 b int64 12 . from arviz.plots.plot_utils import xarray_var_iter for var_name, sel, values in xarray_var_iter(ds): print(var_name, sel, values) . a {&#39;pos&#39;: &#39;top&#39;, &#39;direction&#39;: &#39;x&#39;} -0.5306128314326483 a {&#39;pos&#39;: &#39;top&#39;, &#39;direction&#39;: &#39;y&#39;} 0.8029249611338745 a {&#39;pos&#39;: &#39;top&#39;, &#39;direction&#39;: &#39;z&#39;} 0.7965222104405889 a {&#39;pos&#39;: &#39;bottom&#39;, &#39;direction&#39;: &#39;x&#39;} -1.4255055469706215 a {&#39;pos&#39;: &#39;bottom&#39;, &#39;direction&#39;: &#39;y&#39;} 0.4622636712711883 a {&#39;pos&#39;: &#39;bottom&#39;, &#39;direction&#39;: &#39;z&#39;} -0.12804707435886095 b {} 12 . xarray_var_iter has iterated over every single scalar value without loosing track of where did every value come from. We can also modify the behaviour to skip some dimensions (i.e. in ArviZ we generally iterate over data dimensions and skip chain and draw dims). . for var_name, sel, values in xarray_var_iter(ds, skip_dims={&quot;direction&quot;}): print(var_name, sel, values) . a {&#39;pos&#39;: &#39;top&#39;} [-0.53061283 0.80292496 0.79652221] a {&#39;pos&#39;: &#39;bottom&#39;} [-1.42550555 0.46226367 -0.12804707] b {} 12 . Now that we know about xarray_var_iter and what it does, we can use it to generate a list in the required format directly from xarray objects. Let&#39;s say for example we were interested in plotting the mean as a line in the trace plot: . var_names = [&quot;home&quot;, &quot;atts&quot;] lines = list(xarray_var_iter(idata.posterior[var_names].mean(dim=(&quot;chain&quot;, &quot;draw&quot;)))) az.plot_trace(idata, var_names=var_names, lines=lines); . And what about quantile lines? Lets plot the 10% and 90% quantile lines but only for defs variable: . . Note: This same approach can also be used with az.hdi skipping hdi dimension . var_names = [&quot;home&quot;, &quot;defs&quot;] quantile_ds = idata.posterior[[&quot;defs&quot;]].quantile((.1, .9), dim=(&quot;chain&quot;, &quot;draw&quot;)) lines = list(xarray_var_iter(quantile_ds, skip_dims={&quot;quantile&quot;})) az.plot_trace(idata, var_names=var_names, lines=lines); . Aggregation kwargs . This section is dedicated to 5 different kwargs, closely related to each other: compact+compact_prop, combined+chain_prop and legend. If we focus on the distribution plots of the left column, we may want to aggregate data along 2 possible dimensions, chains or variable dimension(s) -- school dimension in centered_eight data, team dimension in rugby data... As aggragation or not along these 2 possible dimensions is independent, we end up with 4 possibilities. . In az.plot_trace, the argument combined governs the aggregation of all chains into a single plot (has no effect in trace, only in distributions), and compact governs the aggregation of the variable dimension(s). In order to be able to distinguish each single line after some aggregation has taken place, a legend argument is also available to show the legend with the data labels. chain_prop and compact_prop allow customization of the aesthetics mapping. . We&#39;ll now cover all 4 possibilities to showcase all supported cases and explore related customizations. . combined=False and compact=False . The default behaviour of plot_trace is to perform no aggregation at all. In this case therefore, all subplots will have exactly one line per chain in the posterior. In this chain only setting, the default mapping is to use color to distinguish chains: . az.plot_trace(idata, var_names=[&quot;home&quot;, &quot;defs&quot;], legend=True); . combined=True and compact=False . Chains are aggregated into a single quantity if possible. Therefore, distribution column will have one line per subplot due to the aggregation but the trace column will be the same as in the previous section. This is also a chain only setting, the default mapping is to use color to distinguish chains. However, we&#39;ll use this example to show usage of chain_prop to map the chain to the linewidth: . chain_prop = {&quot;linewidth&quot;: (.5, 1, 2, 3)} az.plot_trace( idata, var_names=[&quot;home&quot;, &quot;defs&quot;], combined=True, chain_prop=chain_prop, compact=False, legend=True ); . combined=False and compact=True . You are probably tired already from scrolling down and we have only 6 teams! Imagine having a variable with a dimension of length 100 or more :scream: . In these cases, it may be more convenient to analyze a compact version of the trace plot: . az.plot_trace(idata, var_names=[&quot;home&quot;, &quot;defs&quot;], combined=False, compact=True, legend=True); . The first two things that jump to the eye are that ArviZ has drastically modified the default aesthetic of the plot and that the plot fits now comfortable in a single screen, bye bye scrolling :wave: . We can also see that legend=True has included multiple legends to the figure. The chain legend is always included in the top right trace plot, and the plots in the distribution column contain a legend if necessary. . combined=True and compact=True . To reduce even more the clutter of lines in the trace plot, we can also combine chains. Moreover, the linestyle -&gt; chain mapping can be distracting, especially if we don&#39;t care too much about distinguishing the chains between them. Like we did before, we will use chain_prop to control this. . az.plot_trace(idata, var_names=[&quot;home&quot;, &quot;defs&quot;], combined=True, chain_prop={&quot;ls&quot;: &quot;-&quot;}, compact=True); . Finally, we will explore alternative usage options for chain_prop and compact_prop. In the two previous examples we have used a 2 element tuple where the second position of the tuple contained the properties to use. Another alternative is to pass a string present in plt.rcParams[&quot;axes.prop_cycle&quot;], which in our case is color only. . az.plot_trace( idata, var_names=[&quot;home&quot;, &quot;defs&quot;], combined=True, chain_prop=&quot;color&quot;, compact=True, compact_prop={&quot;lw&quot;: np.linspace(.5, 3, 6)} ); . Summing it all up . Now that we have covered most arguments, let&#39;s put everything to practice. Try to generate a trace plot following the instructions below: . Show variables home, defs and atts showing only Scotland, Ireland, Italy, Wales coordinates. | For defs variable, plot lines showing the 70% HDI. | Map chains to the following colors: &#39;C0&#39;, &#39;C1&#39;, &#39;C2&#39;, &#39;C3&#39;, &quot;xkcd:purple blue&quot; | Map team dimension to both linestyle (solid and dashed) and linewidth | . #collapse-hide coords = {&quot;team&quot;: [&quot;Scotland&quot;, &quot;Ireland&quot;, &quot;Italy&quot;, &quot;Wales&quot;]} quantile_ds = az.hdi(idata, var_names=&quot;defs&quot;, coords=coords, hdi_prob=.7) lines = list(xarray_var_iter(quantile_ds, skip_dims={&quot;hdi&quot;, &quot;team&quot;})) chain_prop = {&quot;color&quot;: [&#39;C0&#39;, &#39;C1&#39;, &#39;C2&#39;, &#39;C3&#39;, &quot;xkcd:purple blue&quot;]} az.plot_trace( idata, var_names=[&quot;home&quot;, &quot;defs&quot;, &quot;atts&quot;], combined=True, chain_prop=chain_prop, compact=True, compact_prop={&quot;lw&quot;: np.linspace(.5, 3, 6), &quot;ls&quot;: (&quot;-&quot;, &quot;--&quot;)}, lines=lines, coords=coords ); . . Package versions used to generate this post: . numpy 1.19.0 arviz 0.9.0 xarray 0.15.1 last updated: Mon Jun 29 2020 CPython 3.6.9 IPython 7.15.0 watermark 2.0.2 . . Comments are not enabled for the blog, to inquiry further about the contents of the post, ask on ArviZ Issues or PyMC Discourse .",
            "url": "https://moraouf.github.io/MoSpace/python/arviz/matplotlib/2020/06/20/plot-trace.html",
            "relUrl": "/python/arviz/matplotlib/2020/06/20/plot-trace.html",
            "date": " • Jun 20, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "ArviZ customization with rcParams",
            "content": "About . ArviZ not only builds on top of matplotlib&#39;s rcParams but also adds its own rcParams instance to handle specific settings. This post will only graze matplotlib&#39;s rcParams, which are already detailed in matplotlib&#39;s docs; it will dive into specific ArviZ rcParams. . Introduction . Paraphrasing the description on rcParams in the documentation of matplotlib: . ArviZ uses arvizrc configuration files to customize all kinds of properties, which we call rcParams. You can control the defaults of many properties in ArviZ:data loading mode (lazy or eager), automatically showing generated plots, the default information criteria and so on. There are several ways of modifying arviz.rcParams instance, each of them targeted to specific needs. . import arviz as az import matplotlib.pyplot as plt idata = az.load_arviz_data(&quot;centered_eight&quot;) . Customizing ArviZ . arvizrc file . To define default values on a per user or per project basis, arvizrc file should be used. When imported, ArviZ search for an arvizrc file in several locations sorted below by priority: . $PWD/arvizrc | $ARVIZ_DATA/arvizrc | On Linux, $XDG_CONFIG_HOME/arviz/arvizrc (if $XDG_CONFIG_HOME is defined) | or $HOME/.config/arviz/arvizrc (if $XDG_CONFIG_HOME is not defined) | . | On other platforms, $HOME/.arviz/arvizrc if $HOME is defined | . | . Once one of these files is found, ArviZ stops looking and loads its configuration. If none of them are present, the values hardcoded in ArviZ codebase are used. The file used to set the default values in ArviZ can be obtained with the following command: . import arviz as az print(az.rcparams.get_arviz_rcfile()) . None . ArviZ has loaded a file used to set defaults on a per user basis. Unless I use a different rc file in the current directory or modify rcParams as explained above, this configuration will be automatically used every time ArviZ is imported. . This can be really useful to define the favourite backend or information criterion, written once in the rc file and ArviZ automatically uses the desired values. . . Important: You should not rely on ArviZ defaults being always the same. . ArviZ strives to encourage best practices and therefore will change the default values whenever a new algorithm is developed to achieve this goal. If you rely on a specific value, you should either use an arvizrc template or set the defaults at the beggining of every script/notebook. . Dynamic rc settings . To set default values on a per file or per project basis, rcParams can also be modified dynamically, either overwritting a specific key: . az.rcParams[&quot;data.load&quot;] = &quot;eager&quot; . Note that rcParams is the instance to be modified, exactly like in matplotlib. Careful with capitalization! . Another option is to define a dictionary with several new defaults and update rcParams all at once. . rc = { &quot;data.load&quot;: &quot;lazy&quot;, &quot;plot.max_subplots&quot;: 30, &quot;stats.ic_scale&quot;: &quot;negative_log&quot;, &quot;plot.matplotlib.constrained_layout&quot;: False } az.rcParams.update(rc) . rc_context . And last but not least, to temporarily use a different set of defaults, ArviZ also has a rc_context function. Its main difference and advantage is that it is a context manager, therefore, all code executed inside the context will use the defaults defined by rc_context but once we exit the context, everything goes back to normal. Let&#39;s generate 3 posterior plots with the same command to show this: . _, axes = plt.subplots(1,3, figsize=(15,4)) az.plot_posterior(idata, var_names=&quot;mu&quot;, ax=axes[0]) with az.rc_context({&quot;plot.point_estimate&quot;: &quot;mode&quot;, &quot;stats.hdi_prob&quot;: 0.7}): az.plot_posterior(idata, var_names=&quot;mu&quot;, ax=axes[1]) az.plot_posterior(idata, var_names=&quot;mu&quot;, ax=axes[2]); . ArviZ default settings . This section will describe ArviZ rcParams as version 0.8.3 (see GitHub for an up to date version). . Data . The rcParams in this section are related to the data module in ArviZ, that is, they are either related to from_xyz converter functions or to InferenceData class. . data.http_protocol : {https, http} . Only the first two example datasets centered_eight and non_centered_eight come as part of ArviZ. All the others are downloaded from figshare the first time and stored locally to help reloading them the next time. We can get the names of the data available by not passing any argument to az.load_arviz_data (you can also get the description of each of them with az.list_datasets): . az.load_arviz_data().keys() . dict_keys([&#39;centered_eight&#39;, &#39;non_centered_eight&#39;, &#39;radon&#39;, &#39;rugby&#39;, &#39;regression1d&#39;, &#39;regression10d&#39;, &#39;classification1d&#39;, &#39;classification10d&#39;]) . Thus, the first time you call az.load_arviz_data(&quot;radon&quot;), ArviZ downloads the dataset using data.http_protocol. The default is set to https but if needed, it can be modified to http. Notice how there is no fallback, if downloading with https fails, there is no second try with http, an error is risen. To use http you have to set the rcParam explicitly. . data.index_origin : {0, 1} . ArviZ integration with Stan and Julia who use 1 based indexing motivate this rcParam. This rcParam is still at an early stage and its implementation is bound to vary, therefore it has no detailed description. . data.load : {lazy, eager} . Even when not using Dask, xarray&#39;s default is to load data lazily into memory when reading from disk. ArviZ&#39;s from_netcdf also uses the same default. That is, ArviZ functions that read data from disk from_netcdf and load_arviz_data do not load the data into memory unless data.load rcParam is set to eager. . Most use cases not only do not require loading data into memory but will also benefit from lazy loading. However, there is one clear exception: when too many files are lazily opened at the same time, xarray ends up crashing with extremely cryptic error messages, these cases require setting data loading to eager mode. One example of such situation is generating ArviZ documentation, we therefore set data.load to eager in sphinx configuration file. . data.metagroups : mapping of {str : list of str} . . Warning: Do not overwrite data.metagroups as things may break, to add custom metagroups add new keys to the dictionary as shown below . One of the current projects in ArviZ is to extend the capabilities of InferenceData. One of the limitations was not allowing its functions and methods to be applied to several groups at the same time. Starting with ArviZ 0.8.0, InferenceData methods take arguments groups and filter_groups to overcome this limitation. These two combined arguments have the same capabilities as var_names+filter_vars in plotting functions: exact matching, like and regex matching like pandas and support for ArviZ ~ negation prefix and one extra feature: metagroups. So what are metagroups? Let&#39;s see . #collapse-hide for metagroup, groups in az.rcParams[&quot;data.metagroups&quot;].items(): print(f&quot;{metagroup}: n {groups} n&quot;) . . posterior_groups: (&#39;posterior&#39;, &#39;posterior_predictive&#39;, &#39;sample_stats&#39;, &#39;log_likelihood&#39;) prior_groups: (&#39;prior&#39;, &#39;prior_predictive&#39;, &#39;sample_stats_prior&#39;) posterior_groups_warmup: (&#39;_warmup_posterior&#39;, &#39;_warmup_posterior_predictive&#39;, &#39;_warmup_sample_stats&#39;) latent_vars: (&#39;posterior&#39;, &#39;prior&#39;) observed_vars: (&#39;posterior_predictive&#39;, &#39;observed_data&#39;, &#39;prior_predictive&#39;) . Imagine the data you passed to the model was rescaled, after converting to InferenceData you have to rescale the data again to its original values, but not only the observations, posterior and prior predictive values too! . Having to apply the rescaling manually to each of the three groups is tedious at best, and creating a variable called observed_vars storing a list with these 3 groups is problematic -- when doing prior checks there is no posterior_predictive group, it&#39;s a highway towards errors at every turn. Metagroups are similar to the variable approach but it&#39;s already there and it applies the function only to present groups. Let&#39;s add a new metagroup and use it to shift our data: . az.rcParams[&quot;data.metagroups&quot;][&quot;sampled&quot;] = ( &#39;posterior&#39;, &#39;posterior_predictive&#39;, &#39;sample_stats&#39;, &#39;log_likelihood&#39;, &#39;prior&#39;, &#39;prior_predictive&#39; ) shifted_idata = idata.map(lambda x: x-7, groups=&quot;sampled&quot;) . data.save_warmup : bool . If True, converter functions will store warmup iterations in the corresponding groups by default. . Note: data.save_warmup does not affect from_netcdf, all groups are always loaded from file . . Plot . General . plot.backend : {matplotlib, bokeh} . Default plotting backend. . plot.max_subplots : int . Maximum number of subplots in a single figure. Adding too many subplots into a figure can be really slow, to the point that it looks like everthing has crashed without any error message. When there are more variables to plot than max_subplots allowed, ArviZ sends a warning and plots at most max_suplots. See for yourselves: . with az.rc_context({&quot;plot.max_subplots&quot;: 3}): az.plot_posterior(idata); . /home/oriol/venvs/arviz-dev/lib/python3.6/site-packages/arviz/plots/plot_utils.py:563: UserWarning: rcParams[&#39;plot.max_subplots&#39;] (3) is smaller than the number of variables to plot (10) in plot_posterior, generating only 3 plots UserWarning, . plot.point_estimate : {mean, median, model, None} . Default point estimate to include in plots like plot_posterior or plot_density. . Bokeh . plot.bokeh.bounds_x_range, plot.bokeh.bounds_y_range : auto, None or tuple of (float, float), default auto . plot.bokeh.figure.dpi : int, default 60 . plot.bokeh.figure.height, plot.bokeh.figure.width : int, default 500 . plot.bokeh.layout.order : str, default default . Select subplot structure for bokeh. One of default, column, row, square, square_trimmed or Ncolumn (Nrow) where N is an integer number of columns (rows), here is one example to generate a subplot grid with 2 columns and the necessary rows to fit all variables. . with az.rc_context({&quot;plot.bokeh.layout.order&quot;: &quot;2column&quot;}): az.plot_ess(idata, backend=&quot;bokeh&quot;) . plot.bokeh.layout.sizing_mode : {fixed, stretch_width, stretch_height, stretch_both, scale_width, scale_height, scale_both} . plot.bokeh.layout.toolbar_location : {above, below, left, right, None} . Location for toolbar on bokeh layouts. None will hide the toolbar. . plot.bokeh.marker : str, default Cross . Default marker for bokeh plots. See bokeh reference on markers for more details. . plot.bokeh.output_backend : {webgl, canvas, svg} . plot.bokeh.show : bool, default True . Show bokeh plot before returning in ArviZ function. . plot.bokeh.tools : str, default reset,pan,box_zoom,wheel_zoom,lasso_select,undo,save,hove . Default tools in bokeh plots. More details on Configuring Plot Tools docs . Matplotlib . Matplotlib already has its own rcParams, which are actually the inspiration for ArviZ rcParams. Therefore, this section is minimalistic. . plot.matplotlib.show : bool, default False . Call plt.show from within ArviZ plotting functions. This generally makes no difference in jupyter like environments, but it can be useful for instance in the IPython terminal when we don&#39;t want to customize the plots genereted by ArviZ by changing titles or labels. . . Stats . stats.hdi_prob : float . Default probability of the calculated HDI intervals. . Important: This probability is completely arbitrary. ArviZ using 0.94 instead of the more common 0.95 aims to emphasize this arbitrary choice. . stats.information_criterion : {loo, waic} . Default information criterion used by compare and plot_elpd . stats.ic_pointwise : bool, default False . Return pointwise values when calling loo or waic. Pointwise values are an intermediate result and therefore setting ic_pointwise to true does not require extra computation. . stats.ic_scale : {log, deviance, negative_log} . Default information criterion scale. See docs on loo or waic for more detail. . . . Tip: Is there any extra rcParam you&#8217;d like to see in ArviZ? Check out arviz-devs/arviz#792, it&#8217;s more than possible you&#8217;ll be able to add it yourself! . Package versions used to generate this post: . arviz 0.9.0 last updated: Mon Jun 29 2020 CPython 3.6.9 IPython 7.15.0 watermark 2.0.2 . . Comments are not enabled for the blog, to inquiry further about the contents of the post, ask on ArviZ Issues. .",
            "url": "https://moraouf.github.io/MoSpace/project/python/arviz/2020/06/19/rcParams.html",
            "relUrl": "/project/python/arviz/2020/06/19/rcParams.html",
            "date": " • Jun 19, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I’m a rebel physicist/engineer who loves statistical data analysis. By day, I am currently a Research Assistant on Bayesian Model Selection with David Rossell at UPF, Barcelona. By night I am a core developer of ArviZ a Python package for exploratory analysis of Bayesian models. In addition to data analysis probabilistic modeling, I also love programming and teaching. . I think that the culture in scientific research needs deep changes towards a more collaborative, open and diverse model. I am interested in open science, reproducible research and science communication. I want to pursue a career in probabilistic modeling and statistical research with special emphasis on openness and reproducibility. . In my spare time, I like playing board games and going to the beach to do water activities. I have been sailing and snorkeling regularly since I was little and more recently I added kayaking to the mix too! I generally spend the summer at the Costa Brava. Here I leave you a sneak peak of the views when nobody is around . . Projects . PyMCon 2020: PyMCon 2020 is an asynchronous-first virtual conference for the Bayesian community | . Open source work . Here are highlighted some open source projects I contribute to, check out my GitHub profile for a complete list of the projects I contribute to. . ArviZ: Exploratory analysis of Bayesian models in Python or Julia | mombf: Bayesian model selection and averaging for regression and mixtures for non-local and local priors. | exosherlock: Smooth your interactions with the NASA Exoplanet Archive using Python and pandas. | PyMC3/4: Friendly probabilistic programming in Python. | . Talks and conferences . PROBPROG 2020: Coming on autumn 2020 | StanCon 2020: ArviZ, InferenceData, and NetCDF: A unified file format for Bayesians. Slides and video presentation are available at GitHub, the slides are executable thanks to Binder! | . Publications . M. Badenas-Agusti, M. N. Günther, T. Daylan, et al., 2020, HD 191939: Three Sub-Neptunes Transiting a Sun-like Star Only 54 pc Away | D. Foreman-Mackey, W. Farr, M. Sinha, A. Archibald, et al., 2019, emcee v3: A Python ensemble sampling toolkit for affine-invariant MCMC. Get the emcee package code! | . | .",
          "url": "https://moraouf.github.io/MoSpace/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  
      ,"page2": {
          "title": "Blog",
          "content": "",
          "url": "https://moraouf.github.io/MoSpace/blog/",
          "relUrl": "/blog/",
          "date": ""
      }
      
  

  

  

  
  

  
      ,"page6": {
          "title": "Projects",
          "content": "",
          "url": "https://moraouf.github.io/MoSpace/projects/",
          "relUrl": "/projects/",
          "date": ""
      }
      
  

  
  

  

  
  

  

  
  

  
  

  
  

  
  

  
      ,"page15": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://moraouf.github.io/MoSpace/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

  
  

}