{
  
    
        "post0": {
            "title": "Holiday Package Recommendations using different ML algorithms",
            "content": "The dataset can be found here: https://www.kaggle.com/susant4learning/holiday-package-purchase-prediction | . The dataset has the follwing features: . CusotmerID: Unique Cusotmer ID | ProdTaken: Whether a customer ourchased the product or not | Age: Customer age | TypeofContact: Mean of communication with the customer | CityTier: City tier depends on the development of a city, population, facilities, and living standards. The categories are ordered i.e. Tier 1 &gt; Tier 2 &gt; Tier 3 | DurationofPitch: Duration of the pitch by a salesperson to the customer | Occupation: Occupation of customer | NumberOfPersonVisiting: Total number of persons planning to take the trip with the customer | NumberOfFollowups: Total number of follow-ups has been done by the salesperson after the sales pitch | ProductPitched: Package proposed to the customer | PreferredPropertyStar: PPreferred hotel property rating by customer | MaritalStatus: Marital status of customer | NumberOfTrips: Average number of trips in a year by customer | Passport: The customer has a passport or not (0: No, 1: Yes) | PitchSatisfactionScore: CSales pitch satisfaction score | OwnCar: Whether the customers own a car or not (0: No, 1: Yes) | NumberOfChildrenVisiting: Total number of children with age less than 5 planning to take the trip with the customer | Designation: Designation of the customer in the current organization | MonthlyIncome: Gross monthly income of the customer | . # import libraries import numpy as np import pandas as pd import matplotlib.pyplot as plt %matplotlib inline import seaborn as sns . #read in the dataset data = pd.read_csv(&quot;Holiday Package Prediction.csv&quot;) . data.head() . CustomerID ProdTaken Age TypeofContact CityTier DurationOfPitch Occupation Gender NumberOfPersonVisiting NumberOfFollowups ProductPitched PreferredPropertyStar MaritalStatus NumberOfTrips Passport PitchSatisfactionScore OwnCar NumberOfChildrenVisiting Designation MonthlyIncome . 0 200000 | 1 | 41.0 | Self Enquiry | 3 | 6.0 | Salaried | Female | 3 | 3.0 | Deluxe | 3.0 | Single | 1.0 | 1 | 2 | 1 | 0.0 | Manager | 20993.0 | . 1 200001 | 0 | 49.0 | Company Invited | 1 | 14.0 | Salaried | Male | 3 | 4.0 | Deluxe | 4.0 | Divorced | 2.0 | 0 | 3 | 1 | 2.0 | Manager | 20130.0 | . 2 200002 | 1 | 37.0 | Self Enquiry | 1 | 8.0 | Free Lancer | Male | 3 | 4.0 | Basic | 3.0 | Single | 7.0 | 1 | 3 | 0 | 0.0 | Executive | 17090.0 | . 3 200003 | 0 | 33.0 | Company Invited | 1 | 9.0 | Salaried | Female | 2 | 3.0 | Basic | 3.0 | Divorced | 2.0 | 1 | 5 | 1 | 1.0 | Executive | 17909.0 | . 4 200004 | 0 | NaN | Self Enquiry | 1 | 8.0 | Small Business | Male | 2 | 3.0 | Basic | 4.0 | Divorced | 1.0 | 0 | 5 | 1 | 0.0 | Executive | 18468.0 | . data.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 4888 entries, 0 to 4887 Data columns (total 20 columns): # Column Non-Null Count Dtype -- -- 0 CustomerID 4888 non-null int64 1 ProdTaken 4888 non-null int64 2 Age 4662 non-null float64 3 TypeofContact 4863 non-null object 4 CityTier 4888 non-null int64 5 DurationOfPitch 4637 non-null float64 6 Occupation 4888 non-null object 7 Gender 4888 non-null object 8 NumberOfPersonVisiting 4888 non-null int64 9 NumberOfFollowups 4843 non-null float64 10 ProductPitched 4888 non-null object 11 PreferredPropertyStar 4862 non-null float64 12 MaritalStatus 4888 non-null object 13 NumberOfTrips 4748 non-null float64 14 Passport 4888 non-null int64 15 PitchSatisfactionScore 4888 non-null int64 16 OwnCar 4888 non-null int64 17 NumberOfChildrenVisiting 4822 non-null float64 18 Designation 4888 non-null object 19 MonthlyIncome 4655 non-null float64 dtypes: float64(7), int64(7), object(6) memory usage: 763.9+ KB . As we can see, we have couple of columns which have missing values &amp; need to be processed . print(&quot;Unique Values of columns: n&quot;) for col in data.columns: print(&quot;{}: {}&quot;.format(col, data[col].unique())) . Unique Values of columns: CustomerID: [200000 200001 200002 ... 204885 204886 204887] ProdTaken: [1 0] Age: [41. 49. 37. 33. nan 32. 59. 30. 38. 36. 35. 31. 34. 28. 29. 22. 53. 21. 42. 44. 46. 39. 24. 43. 50. 27. 26. 48. 55. 45. 56. 23. 51. 40. 54. 58. 20. 25. 19. 57. 52. 47. 18. 60. 61.] TypeofContact: [&#39;Self Enquiry&#39; &#39;Company Invited&#39; nan] CityTier: [3 1 2] DurationOfPitch: [ 6. 14. 8. 9. 30. 29. 33. 22. 21. 32. 25. 27. 11. 17. 15. 13. 12. 16. 10. 31. 18. nan 24. 35. 28. 20. 26. 34. 23. 5. 19. 126. 7. 36. 127.] Occupation: [&#39;Salaried&#39; &#39;Free Lancer&#39; &#39;Small Business&#39; &#39;Large Business&#39;] Gender: [&#39;Female&#39; &#39;Male&#39; &#39;Fe Male&#39;] NumberOfPersonVisiting: [3 2 1 4 5] NumberOfFollowups: [ 3. 4. 2. 5. nan 1. 6.] ProductPitched: [&#39;Deluxe&#39; &#39;Basic&#39; &#39;Standard&#39; &#39;Super Deluxe&#39; &#39;King&#39;] PreferredPropertyStar: [ 3. 4. 5. nan] MaritalStatus: [&#39;Single&#39; &#39;Divorced&#39; &#39;Married&#39; &#39;Unmarried&#39;] NumberOfTrips: [ 1. 2. 7. 5. 6. 3. 4. 19. 21. 8. nan 20. 22.] Passport: [1 0] PitchSatisfactionScore: [2 3 5 4 1] OwnCar: [1 0] NumberOfChildrenVisiting: [ 0. 2. 1. nan 3.] Designation: [&#39;Manager&#39; &#39;Executive&#39; &#39;Senior Manager&#39; &#39;AVP&#39; &#39;VP&#39;] MonthlyIncome: [20993. 20130. 17090. ... 22097. 22995. 21471.] . data.describe() . CustomerID ProdTaken Age CityTier DurationOfPitch NumberOfPersonVisiting NumberOfFollowups PreferredPropertyStar NumberOfTrips Passport PitchSatisfactionScore OwnCar NumberOfChildrenVisiting MonthlyIncome . count 4888.000000 | 4888.000000 | 4662.000000 | 4888.000000 | 4637.000000 | 4888.000000 | 4843.000000 | 4862.000000 | 4748.000000 | 4888.000000 | 4888.000000 | 4888.000000 | 4822.000000 | 4655.000000 | . mean 202443.500000 | 0.188216 | 37.622265 | 1.654255 | 15.490835 | 2.905074 | 3.708445 | 3.581037 | 3.236521 | 0.290917 | 3.078151 | 0.620295 | 1.187267 | 23619.853491 | . std 1411.188388 | 0.390925 | 9.316387 | 0.916583 | 8.519643 | 0.724891 | 1.002509 | 0.798009 | 1.849019 | 0.454232 | 1.365792 | 0.485363 | 0.857861 | 5380.698361 | . min 200000.000000 | 0.000000 | 18.000000 | 1.000000 | 5.000000 | 1.000000 | 1.000000 | 3.000000 | 1.000000 | 0.000000 | 1.000000 | 0.000000 | 0.000000 | 1000.000000 | . 25% 201221.750000 | 0.000000 | 31.000000 | 1.000000 | 9.000000 | 2.000000 | 3.000000 | 3.000000 | 2.000000 | 0.000000 | 2.000000 | 0.000000 | 1.000000 | 20346.000000 | . 50% 202443.500000 | 0.000000 | 36.000000 | 1.000000 | 13.000000 | 3.000000 | 4.000000 | 3.000000 | 3.000000 | 0.000000 | 3.000000 | 1.000000 | 1.000000 | 22347.000000 | . 75% 203665.250000 | 0.000000 | 44.000000 | 3.000000 | 20.000000 | 3.000000 | 4.000000 | 4.000000 | 4.000000 | 1.000000 | 4.000000 | 1.000000 | 2.000000 | 25571.000000 | . max 204887.000000 | 1.000000 | 61.000000 | 3.000000 | 127.000000 | 5.000000 | 6.000000 | 5.000000 | 22.000000 | 1.000000 | 5.000000 | 1.000000 | 3.000000 | 98678.000000 | . From the chart above, we can clearly see that the numerical features have different ranges, hence scaling should be considered. . data.describe(include = &quot;object&quot;) . TypeofContact Occupation Gender ProductPitched MaritalStatus Designation . count 4863 | 4888 | 4888 | 4888 | 4888 | 4888 | . unique 2 | 4 | 3 | 5 | 4 | 5 | . top Self Enquiry | Salaried | Male | Basic | Married | Executive | . freq 3444 | 2368 | 2916 | 1842 | 2340 | 1842 | . travel = data.copy() . numeric_cols = [col for col in travel.columns if data[col].dtype in [np.int64, np.float64]] print(numeric_cols) . [&#39;CustomerID&#39;, &#39;ProdTaken&#39;, &#39;Age&#39;, &#39;CityTier&#39;, &#39;DurationOfPitch&#39;, &#39;NumberOfPersonVisiting&#39;, &#39;NumberOfFollowups&#39;, &#39;PreferredPropertyStar&#39;, &#39;NumberOfTrips&#39;, &#39;Passport&#39;, &#39;PitchSatisfactionScore&#39;, &#39;OwnCar&#39;, &#39;NumberOfChildrenVisiting&#39;, &#39;MonthlyIncome&#39;] . object_cols = [col for col in travel.columns if data[col].dtype == object] print(object_cols) . [&#39;TypeofContact&#39;, &#39;Occupation&#39;, &#39;Gender&#39;, &#39;ProductPitched&#39;, &#39;MaritalStatus&#39;, &#39;Designation&#39;] . Data Preprocessing &amp; Visualization . travel[numeric_cols].hist(figsize=(20,15)) plt.show() . plt.figure(figsize=(16,10)) for i, col in zip(range(1, 7), object_cols): plt.subplot(2, 3, i) travel[col].value_counts().plot(kind = &quot;bar&quot;) plt.title(str(col)) plt.xticks(rotation = 15) plt.show() . travel.loc[data[&quot;Occupation&quot;] == &quot;Free Lancer&quot;, [&quot;Occupation&quot;]] = &quot;Salaried&quot; . travel[&quot;Gender&quot;].value_counts() . Male 2916 Female 1817 Fe Male 155 Name: Gender, dtype: int64 . travel.loc[data[&quot;Gender&quot;] == &quot;Fe Male&quot;, [&quot;Gender&quot;]] = &quot;Female&quot; . travel[&quot;Gender&quot;].value_counts() . Male 2916 Female 1972 Name: Gender, dtype: int64 . travel[&quot;MaritalStatus&quot;].value_counts() . Married 2340 Divorced 950 Single 916 Unmarried 682 Name: MaritalStatus, dtype: int64 . travel.loc[data[&quot;MaritalStatus&quot;] == &quot;Unmarried&quot;, [&quot;MaritalStatus&quot;]] = &quot;Single&quot; . travel[&quot;MaritalStatus&quot;].value_counts() . Married 2340 Single 1598 Divorced 950 Name: MaritalStatus, dtype: int64 . Dealing with Outliers . plt.figure(figsize = (15,5)) for i, col in zip(range(1,4),[&quot;DurationOfPitch&quot;, &quot;NumberOfTrips&quot;, &quot;MonthlyIncome&quot;]): plt.subplot(1,3, i) travel.boxplot(column = col) plt.show() . travel.loc[travel[&quot;MonthlyIncome&quot;] &lt; 5000, &quot;MonthlyIncome&quot;] . 142 1000.0 2586 4678.0 Name: MonthlyIncome, dtype: float64 . # After finding outliers indices, we drop them travel = travel.drop(travel.index[[38, 2482, 385, 816, 2829, 3260, 142, 2586, 1434,3878]]) . Features Correlation . travel[numeric_cols].corr()[&quot;ProdTaken&quot;] . CustomerID 0.056792 ProdTaken 1.000000 Age -0.146591 CityTier 0.087495 DurationOfPitch 0.084515 NumberOfPersonVisiting 0.009953 NumberOfFollowups 0.112551 PreferredPropertyStar 0.100368 NumberOfTrips 0.013816 Passport 0.260593 PitchSatisfactionScore 0.050603 OwnCar -0.011985 NumberOfChildrenVisiting 0.006712 MonthlyIncome -0.134025 Name: ProdTaken, dtype: float64 . plt.figure(figsize=(10,7)) corr = travel.corr() # corr = corr.iloc[1:, :-1] mask = np.triu(np.ones_like(corr), k=1) sns.heatmap(corr, xticklabels=corr.columns.values, yticklabels=corr.columns.values, mask = mask, annot = True) . &lt;AxesSubplot:&gt; . Dealing with Missing Data . travel.isnull().sum() . CustomerID 0 ProdTaken 0 Age 225 TypeofContact 25 CityTier 0 DurationOfPitch 251 Occupation 0 Gender 0 NumberOfPersonVisiting 0 NumberOfFollowups 45 ProductPitched 0 PreferredPropertyStar 25 MaritalStatus 0 NumberOfTrips 140 Passport 0 PitchSatisfactionScore 0 OwnCar 0 NumberOfChildrenVisiting 66 Designation 0 MonthlyIncome 233 dtype: int64 . cols_with_missing = [&quot;Age&quot;, &quot;TypeofContact&quot;, &quot;DurationOfPitch&quot;, &quot;NumberOfFollowups&quot;, &quot;PreferredPropertyStar&quot;, &quot;NumberOfTrips&quot;, &quot;NumberOfChildrenVisiting&quot;, &quot;MonthlyIncome&quot;] travel[cols_with_missing].describe(include = &quot;all&quot;) . Age TypeofContact DurationOfPitch NumberOfFollowups PreferredPropertyStar NumberOfTrips NumberOfChildrenVisiting MonthlyIncome . count 4653.000000 | 4853 | 4627.000000 | 4833.000000 | 4853.000000 | 4738.000000 | 4812.000000 | 4645.000000 | . unique NaN | 2 | NaN | NaN | NaN | NaN | NaN | NaN | . top NaN | Self Enquiry | NaN | NaN | NaN | NaN | NaN | NaN | . freq NaN | 3440 | NaN | NaN | NaN | NaN | NaN | NaN | . mean 37.621320 | NaN | 15.449319 | 3.708463 | 3.581290 | 3.222035 | 1.187864 | 23600.630786 | . std 9.321348 | NaN | 8.208413 | 1.003123 | 0.798182 | 1.780694 | 0.858059 | 5147.662625 | . min 18.000000 | NaN | 5.000000 | 1.000000 | 3.000000 | 1.000000 | 0.000000 | 16009.000000 | . 25% 31.000000 | NaN | 9.000000 | 3.000000 | 3.000000 | 2.000000 | 1.000000 | 20346.000000 | . 50% 36.000000 | NaN | 13.000000 | 4.000000 | 3.000000 | 3.000000 | 1.000000 | 22351.000000 | . 75% 44.000000 | NaN | 20.000000 | 4.000000 | 4.000000 | 4.000000 | 2.000000 | 25571.000000 | . max 61.000000 | NaN | 36.000000 | 6.000000 | 5.000000 | 8.000000 | 3.000000 | 38677.000000 | . travel.loc[travel[&quot;TypeofContact&quot;].isnull(), [&quot;TypeofContact&quot;]] = &quot;Self Enquiry&quot; . travel[&quot;TypeofContact&quot;].value_counts(dropna = False) . Self Enquiry 3465 Company Invited 1413 Name: TypeofContact, dtype: int64 . Imputing missing values of numerical features will be done after train-test-split of the dataset to avoid any leakage from the test data into the train data | . . Classification . . Model Building . y = travel[&quot;ProdTaken&quot;] X = travel.drop([&quot;CustomerID&quot;, &quot;ProdTaken&quot;], axis =1) print(&quot;The target class is `{}`: n{}&quot;.format(&quot;ProdTaken&quot;, y.unique())) print(&quot; nThe features are : n n{}&quot;.format(X.columns)) . The target class is `ProdTaken`: [1 0] The features are : Index([&#39;Age&#39;, &#39;TypeofContact&#39;, &#39;CityTier&#39;, &#39;DurationOfPitch&#39;, &#39;Occupation&#39;, &#39;Gender&#39;, &#39;NumberOfPersonVisiting&#39;, &#39;NumberOfFollowups&#39;, &#39;ProductPitched&#39;, &#39;PreferredPropertyStar&#39;, &#39;MaritalStatus&#39;, &#39;NumberOfTrips&#39;, &#39;Passport&#39;, &#39;PitchSatisfactionScore&#39;, &#39;OwnCar&#39;, &#39;NumberOfChildrenVisiting&#39;, &#39;Designation&#39;, &#39;MonthlyIncome&#39;], dtype=&#39;object&#39;) . from sklearn.model_selection import train_test_split from sklearn.impute import SimpleImputer from sklearn.preprocessing import OrdinalEncoder from sklearn.preprocessing import OneHotEncoder . Split the data &amp; consider stratification . X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1,stratify = y) print(&quot;Check proper stratification applied: n n&quot;) print(&quot;Proportion of customers who took the product: &quot;,len(y[y == 1]) / len(y)) print(&quot;Proportion of train set where customers took the product: &quot;,len(X_train[y == 1]) / len(X_train)) print(&quot;Proportion of test set where customers took the product: &quot;,len(X_test[y == 1]) / len(X_test)) . Check proper stratification applied: Proportion of customers who took the product: 0.1881918819188192 Proportion of train set where customers took the product: 0.18810866222450026 Proportion of test set where customers took the product: 0.1885245901639344 . &lt;ipython-input-31-5eeae4a3ab14&gt;:4: UserWarning: Boolean Series key will be reindexed to match DataFrame index. print(&#34;Proportion of train set where customers took the product: &#34;,len(X_train[y == 1]) / len(X_train)) &lt;ipython-input-31-5eeae4a3ab14&gt;:5: UserWarning: Boolean Series key will be reindexed to match DataFrame index. print(&#34;Proportion of test set where customers took the product: &#34;,len(X_test[y == 1]) / len(X_test)) . numeric_cols_with_missing = cols_with_missing.copy() numeric_cols_with_missing.remove(&quot;TypeofContact&quot;) . numeric_cols_with_missing . [&#39;Age&#39;, &#39;DurationOfPitch&#39;, &#39;NumberOfFollowups&#39;, &#39;PreferredPropertyStar&#39;, &#39;NumberOfTrips&#39;, &#39;NumberOfChildrenVisiting&#39;, &#39;MonthlyIncome&#39;] . Numeric Imputation &amp; filling missing data . # impute X_train imputer = SimpleImputer(strategy = &quot;median&quot;) imputed_X_train = pd.DataFrame(imputer.fit_transform(X_train[numeric_cols_with_missing])) imputed_X_train.columns = numeric_cols_with_missing imputed_X_train.index = X_train.index imputed_X_train_cols = imputed_X_train.columns print(imputed_X_train.isnull().any()) print(&quot; n&quot;,imputed_X_train_cols) . Age False DurationOfPitch False NumberOfFollowups False PreferredPropertyStar False NumberOfTrips False NumberOfChildrenVisiting False MonthlyIncome False dtype: bool Index([&#39;Age&#39;, &#39;DurationOfPitch&#39;, &#39;NumberOfFollowups&#39;, &#39;PreferredPropertyStar&#39;, &#39;NumberOfTrips&#39;, &#39;NumberOfChildrenVisiting&#39;, &#39;MonthlyIncome&#39;], dtype=&#39;object&#39;) . # impute X_test imputed_X_test = pd.DataFrame(imputer.transform(X_test[numeric_cols_with_missing])) imputed_X_test.columns = numeric_cols_with_missing imputed_X_test.index = X_test.index . imputed_X_test.isnull().any() . Age False DurationOfPitch False NumberOfFollowups False PreferredPropertyStar False NumberOfTrips False NumberOfChildrenVisiting False MonthlyIncome False dtype: bool . unimputed_X_train_cols = [col for col in numeric_cols if col not in imputed_X_train_cols] unimputed_X_train_cols.remove(&quot;CustomerID&quot;) unimputed_X_train_cols.remove(&quot;ProdTaken&quot;) unimputed_X_train = X_train[unimputed_X_train_cols] unimputed_X_test = X_test[unimputed_X_train_cols] . unimputed_X_train.isnull().any() . CityTier False NumberOfPersonVisiting False Passport False PitchSatisfactionScore False OwnCar False dtype: bool . unimputed_X_test.isnull().any() . CityTier False NumberOfPersonVisiting False Passport False PitchSatisfactionScore False OwnCar False dtype: bool . middle_X_train = pd.concat([imputed_X_train, unimputed_X_train], axis = 1) middle_X_test = pd.concat([imputed_X_test, unimputed_X_test], axis = 1) . middle_X_train.isnull().any() . Age False DurationOfPitch False NumberOfFollowups False PreferredPropertyStar False NumberOfTrips False NumberOfChildrenVisiting False MonthlyIncome False CityTier False NumberOfPersonVisiting False Passport False PitchSatisfactionScore False OwnCar False dtype: bool . Ordinal Encoder &amp; One-Hot Encoder . ord_cat = [&quot;Occupation&quot;, &quot;ProductPitched&quot;, &quot;Designation&quot;] oh_cat = [&quot;TypeofContact&quot;, &quot;Gender&quot;, &quot;MaritalStatus&quot;] labelled_X_train = X_train.copy() labelled_X_test = X_test.copy() . ord_encoder = OrdinalEncoder() labelled_X_train[ord_cat] = ord_encoder.fit_transform(X_train[ord_cat]) labelled_X_test[ord_cat] = ord_encoder.transform(X_test[ord_cat]) . oh_encoder = OneHotEncoder(handle_unknown=&#39;ignore&#39;, sparse=False) . oh_X_train = pd.DataFrame(oh_encoder.fit_transform(X_train[oh_cat])) oh_X_test = pd.DataFrame(oh_encoder.transform(X_test[oh_cat])) oh_X_train.index = X_train.index oh_X_test.index = X_test.index . final_X_train = pd.concat([middle_X_train, labelled_X_train[ord_cat], oh_X_train], axis = 1) final_X_test = pd.concat([middle_X_test, labelled_X_test[ord_cat], oh_X_test], axis = 1) . final_X_train . Age DurationOfPitch NumberOfFollowups PreferredPropertyStar NumberOfTrips NumberOfChildrenVisiting MonthlyIncome CityTier NumberOfPersonVisiting Passport ... Occupation ProductPitched Designation 0 1 2 3 4 5 6 . 781 36.0 | 6.0 | 4.0 | 5.0 | 2.0 | 1.0 | 22338.0 | 1 | 2 | 0 | ... | 2.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 0.0 | 0.0 | . 2039 36.0 | 14.0 | 4.0 | 5.0 | 2.0 | 0.0 | 22587.0 | 3 | 3 | 0 | ... | 1.0 | 3.0 | 3.0 | 1.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 1.0 | . 1944 24.0 | 29.0 | 1.0 | 3.0 | 2.0 | 0.0 | 17725.0 | 1 | 2 | 1 | ... | 2.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | . 3099 34.0 | 10.0 | 4.0 | 4.0 | 5.0 | 2.0 | 20955.0 | 2 | 3 | 1 | ... | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | . 346 41.0 | 12.0 | 4.0 | 4.0 | 7.0 | 0.0 | 21032.0 | 1 | 3 | 1 | ... | 2.0 | 1.0 | 2.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 0.0 | 1.0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 2522 38.0 | 14.0 | 4.0 | 3.0 | 6.0 | 1.0 | 32342.0 | 1 | 2 | 0 | ... | 2.0 | 3.0 | 3.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 0.0 | 1.0 | . 931 30.0 | 13.0 | 3.0 | 3.0 | 1.0 | 2.0 | 19695.0 | 1 | 3 | 1 | ... | 2.0 | 1.0 | 2.0 | 1.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 1.0 | . 820 35.0 | 17.0 | 4.0 | 3.0 | 2.0 | 0.0 | 19968.0 | 3 | 2 | 0 | ... | 2.0 | 1.0 | 2.0 | 1.0 | 0.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | . 50 48.0 | 6.0 | 4.0 | 3.0 | 1.0 | 2.0 | 20381.0 | 1 | 3 | 1 | ... | 1.0 | 3.0 | 3.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 0.0 | 1.0 | . 1537 45.0 | 13.0 | 3.0 | 5.0 | 3.0 | 0.0 | 24724.0 | 1 | 2 | 0 | ... | 1.0 | 3.0 | 3.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | . 3902 rows × 22 columns . final_X_train.isnull().any() . Age False DurationOfPitch False NumberOfFollowups False PreferredPropertyStar False NumberOfTrips False NumberOfChildrenVisiting False MonthlyIncome False CityTier False NumberOfPersonVisiting False Passport False PitchSatisfactionScore False OwnCar False Occupation False ProductPitched False Designation False 0 False 1 False 2 False 3 False 4 False 5 False 6 False dtype: bool . Naive Bayes . from sklearn.naive_bayes import GaussianNB from sklearn.naive_bayes import CategoricalNB . gaussian_clf = GaussianNB() gaussian_clf.fit(final_X_train, y_train) pred_labels = gaussian_clf.predict(final_X_test) score = gaussian_clf.score(final_X_test, y_test) print(&#39;Accuracy Score: &#39;, score) . Accuracy Score: 0.8329918032786885 . from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, roc_curve, roc_auc_score print(&quot;Confusion Matrix: n&quot;, confusion_matrix(y_test, pred_labels)) print(&quot; nPrecision Score: &quot;, precision_score(y_test, pred_labels)) print(&quot;Recall Score: &quot;, recall_score(y_test, pred_labels)) print(&quot;F1 Score: &quot;, f1_score(y_test, pred_labels)) . Confusion Matrix: [[728 64] [ 99 85]] Precision Score: 0.5704697986577181 Recall Score: 0.46195652173913043 F1 Score: 0.5105105105105106 . fpr, tpr, thresholds = roc_curve(y_test, pred_labels) . plt.figure(figsize=(8,6)) plt.plot(fpr, tpr, linewidth=2, label=&quot;ROC Curve&quot;) plt.plot([0, 1], [0, 1], &#39;k--&#39;) # dashed diagonal plt.xlim(0,1) plt.ylim(0,1) plt.show() . roc_auc_score(y_test, pred_labels) . 0.6905742204655249 . pred_probas = gaussian_clf.predict_proba(final_X_test) fpr, tpr, thresholds = roc_curve(y_test, pred_probas[:, 1]) . plt.figure(figsize=(8,6)) plt.plot(fpr, tpr, linewidth=2, label=&quot;ROC Curve&quot;) plt.plot([0, 1], [0, 1], &#39;k--&#39;) # dashed diagonal plt.xlim(0,1) plt.ylim(0,1) plt.show() . Cross-Validation . from sklearn.model_selection import cross_val_predict, cross_val_score cv_gaussian_clf = GaussianNB() y_train_scores = cross_val_score(cv_gaussian_clf, final_X_train, y_train, cv=5, scoring = &quot;accuracy&quot;) . print(y_train_scores) print(y_train_scores.mean()) . [0.82586428 0.80921895 0.80769231 0.83974359 0.82179487] 0.8208627991726584 . cat_X_train = pd.concat([labelled_X_train[ord_cat], oh_X_train], axis = 1) cat_X_test = pd.concat([labelled_X_test[ord_cat], oh_X_test], axis = 1) . Mixed Naive Bayes with CategoricalNB &amp; GaussianNB . second_gaussian_clf = GaussianNB() G_clf = second_gaussian_clf.fit(middle_X_train, y_train) # Categorical model for discrete independent variable categorical_clf = CategoricalNB() C_clf = categorical_clf.fit(cat_X_train, y_train) . # Get probability predictions from each model # On training data G_train_probas = second_gaussian_clf.predict_proba(middle_X_train) C_train_probas = categorical_clf.predict_proba(cat_X_train) # On testing data G_test_probas = second_gaussian_clf.predict_proba(middle_X_test) C_test_probas = categorical_clf.predict_proba(cat_X_test) . X_new_train = np.c_[(G_train_probas[:,1], C_train_probas[:,1])] # Train X_new_test = np.c_[(G_test_probas[:,1], C_test_probas[:,1])] # Test . last_gaussian_clf = GaussianNB() last_gaussian_clf.fit(X_new_train, y_train) last_pred_labels = last_gaussian_clf.predict(X_new_test) . new_score = last_gaussian_clf.score(X_new_test, y_test) new_score . 0.8391393442622951 . print(&quot;Confusion Matrix: n&quot;, confusion_matrix(y_test, last_pred_labels)) print(&quot; nPrecision Score: &quot;, precision_score(y_test, last_pred_labels)) print(&quot;Recall Score: &quot;, recall_score(y_test, last_pred_labels)) print(&quot;F1 Score: &quot;, f1_score(y_test, last_pred_labels)) . Confusion Matrix: [[729 63] [ 94 90]] Precision Score: 0.5882352941176471 Recall Score: 0.4891304347826087 F1 Score: 0.5341246290801187 . . Clustering . . X_train_clustering = middle_X_train[[&quot;Age&quot;, &quot;MonthlyIncome&quot;]] X_test_clustering = middle_X_test[[&quot;Age&quot;, &quot;MonthlyIncome&quot;]] . X_train_clustering . Age MonthlyIncome . 781 36.0 | 22338.0 | . 2039 36.0 | 22587.0 | . 1944 24.0 | 17725.0 | . 3099 34.0 | 20955.0 | . 346 41.0 | 21032.0 | . ... ... | ... | . 2522 38.0 | 32342.0 | . 931 30.0 | 19695.0 | . 820 35.0 | 19968.0 | . 50 48.0 | 20381.0 | . 1537 45.0 | 24724.0 | . 3902 rows × 2 columns . X_train_clustering.describe() . Age MonthlyIncome . count 3902.000000 | 3902.000000 | . mean 37.574833 | 23583.390056 | . std 9.158515 | 5106.695893 | . min 18.000000 | 16009.000000 | . 25% 31.000000 | 20514.500000 | . 50% 36.000000 | 22338.000000 | . 75% 43.000000 | 25402.250000 | . max 61.000000 | 38677.000000 | . plt.figure(figsize = (10,8)) plt.scatter(X_train_clustering.iloc[:, 0], X_train_clustering.iloc[:, 1]) plt.xlabel(&quot;Age&quot;) plt.ylabel(&quot;Salary&quot;) plt.show() . We can see the range of the values for each feature is different &amp; this will adversly affect the performance of the KMeans algorithm, so scaling them is necessary. . Features Scaling . from sklearn.preprocessing import MinMaxScaler . scaler = MinMaxScaler() scaled_X_train_clustering = scaler.fit_transform(X_train_clustering) . Model Building . from sklearn.cluster import KMeans from sklearn.metrics import silhouette_samples, silhouette_score . kmeans = KMeans(n_clusters = 3, random_state = 1) kmeans.fit(scaled_X_train_clustering) kmeans.score(scaled_X_train_clustering) . -124.8779525561468 . n_clusters_range = np.arange(5, 15) inertias = [] for n_clusters in n_clusters_range: kmeans = KMeans(n_clusters = n_clusters, random_state = 1) kmeans.fit(scaled_X_train_clustering) inertias.append(kmeans.inertia_) . Elbow Method . plt.figure(figsize = (10,8)) plt.plot(n_clusters_range, inertias) plt.xlabel(&quot;K&quot;, fontsize = 12) plt.ylabel(&quot;Inertia&quot;, fontsize = 12) plt.show() . Silhouettte Score Method . import matplotlib.cm as cm range_n_clusters = [6, 7, 8, 9, 10] for n_clusters in range_n_clusters: fig, (ax1, ax2) = plt.subplots(1, 2) fig.set_size_inches(18, 7) ax1.set_xlim([-1, 1]) ax1.set_ylim([0, len(scaled_X_train_clustering) + (n_clusters + 1) * 10]) clusterer = KMeans(n_clusters=n_clusters, random_state=10) cluster_labels = clusterer.fit_predict(scaled_X_train_clustering) silhouette_avg = silhouette_score(scaled_X_train_clustering, cluster_labels) print(&quot;For n_clusters =&quot;, n_clusters, &quot;The average silhouette_score is :&quot;, silhouette_avg) sample_silhouette_values = silhouette_samples(scaled_X_train_clustering, cluster_labels) y_lower = 10 for i in range(n_clusters): ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i] ith_cluster_silhouette_values.sort() size_cluster_i = ith_cluster_silhouette_values.shape[0] y_upper = y_lower + size_cluster_i color = cm.nipy_spectral(float(i) / n_clusters) ax1.fill_betweenx(np.arange(y_lower, y_upper), 0, ith_cluster_silhouette_values, facecolor=color, edgecolor=color, alpha=0.7) ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i)) y_lower = y_upper + 10 # 10 for the 0 samples ax1.set_title(&quot;The silhouette plot for the various clusters.&quot;) ax1.set_xlabel(&quot;The silhouette coefficient values&quot;) ax1.set_ylabel(&quot;Cluster label&quot;) # The vertical line for average silhouette score of all the values ax1.axvline(x=silhouette_avg, color=&quot;red&quot;, linestyle=&quot;--&quot;) ax1.set_yticks([]) ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1]) # 2nd Plot showing the actual clusters formed colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters) ax2.scatter(scaled_X_train_clustering[:, 0], scaled_X_train_clustering[:, 1], marker=&#39;.&#39;, s=30, lw=0, alpha=0.7, c=colors, edgecolor=&#39;k&#39;) # Labeling the clusters centers = clusterer.cluster_centers_ # Draw white circles at cluster centers ax2.scatter(centers[:, 0], centers[:, 1], marker=&#39;o&#39;, c=&quot;white&quot;, alpha=1, s=200, edgecolor=&#39;k&#39;) for i, c in enumerate(centers): ax2.scatter(c[0], c[1], marker=&#39;$%d$&#39; % i, alpha=1, s=50, edgecolor=&#39;k&#39;) ax2.set_title(&quot;The visualization of the clustered data.&quot;) ax2.set_xlabel(&quot;Feature space for the 1st feature&quot;) ax2.set_ylabel(&quot;Feature space for the 2nd feature&quot;) plt.suptitle((&quot;Silhouette analysis for KMeans clustering on sample data &quot; &quot;with n_clusters = %d&quot; % n_clusters), fontsize=14, fontweight=&#39;bold&#39;) plt.show() . For n_clusters = 6 The average silhouette_score is : 0.3448214634401572 For n_clusters = 7 The average silhouette_score is : 0.3520383009504232 For n_clusters = 8 The average silhouette_score is : 0.35351743531025764 For n_clusters = 9 The average silhouette_score is : 0.3576312842903845 For n_clusters = 10 The average silhouette_score is : 0.3647261439213562 . **The dataset is not suitable for clustring, but we can see that the algorithm can come up with different clusters for the selected features. As there are no clear patterns of clusters, increasing the number of clusters doesnt draw any valuable information.** . **If we compare clustering with classfication on this dataset, we can successfully accomplish a decent result of classifying the customers according to whether they will purchase the product or no, but to find any implicit patterns among customers collected data, the KMeans algorithm doesn perform so well** . scaled_X_test_clustering = scaler.transform(X_test_clustering) . kmeans = KMeans(n_clusters = 6, random_state = 1) kmeans.fit(scaled_X_train_clustering) kmeans.predict(scaled_X_test_clustering)[:10] . array([2, 4, 2, 4, 5, 2, 4, 5, 5, 5]) . Kmodes Clustering for categorical Variables . from kmodes.kmodes import KModes . costs = [] travel_kmodes_clustering = travel.copy() k = range(4,9) for num_clusters in list(k): kmode = KModes(n_clusters=num_clusters, init = &quot;Cao&quot;, n_init = 10, verbose=1) kmode.fit_predict(travel_kmodes_clustering[object_cols]) costs.append(kmode.cost_) plt.plot(k, costs, &#39;bx-&#39;) plt.xlabel(&#39;K&#39;) plt.ylabel(&#39;Cost&#39;) plt.title(&#39;Elbow Method For Optimal k&#39;) plt.show() . Initialization method and algorithm are deterministic. Setting n_init to 1. Init: initializing centroids Init: initializing clusters Starting iterations... Run 1, iteration: 1/100, moves: 812, cost: 8634.0 Initialization method and algorithm are deterministic. Setting n_init to 1. Init: initializing centroids Init: initializing clusters Starting iterations... Run 1, iteration: 1/100, moves: 724, cost: 8196.0 Initialization method and algorithm are deterministic. Setting n_init to 1. Init: initializing centroids Init: initializing clusters Starting iterations... Run 1, iteration: 1/100, moves: 572, cost: 7658.0 Initialization method and algorithm are deterministic. Setting n_init to 1. Init: initializing centroids Init: initializing clusters Starting iterations... Run 1, iteration: 1/100, moves: 572, cost: 6846.0 Initialization method and algorithm are deterministic. Setting n_init to 1. Init: initializing centroids Init: initializing clusters Starting iterations... Run 1, iteration: 1/100, moves: 538, cost: 6720.0 . From the figure above, we can see an elbow at k=7, so we will apply kmodes using this number of clusters . kmode = KModes(n_clusters=7, init = &quot;random&quot;, n_init = 10, verbose=1) clusters = kmode.fit_predict(travel_kmodes_clustering[object_cols]) travel_kmodes_clustering[&quot;clusters&quot;] = clusters clusters . Init: initializing centroids Init: initializing clusters Starting iterations... Run 1, iteration: 1/100, moves: 926, cost: 7031.0 Run 1, iteration: 2/100, moves: 599, cost: 7031.0 Init: initializing centroids Init: initializing clusters Starting iterations... Run 2, iteration: 1/100, moves: 0, cost: 7663.0 Init: initializing centroids Init: initializing clusters Starting iterations... Run 3, iteration: 1/100, moves: 1274, cost: 7253.0 Init: initializing centroids Init: initializing clusters Starting iterations... Run 4, iteration: 1/100, moves: 978, cost: 7527.0 Init: initializing centroids Init: initializing clusters Starting iterations... Run 5, iteration: 1/100, moves: 314, cost: 7817.0 Init: initializing centroids Init: initializing clusters Starting iterations... Run 6, iteration: 1/100, moves: 130, cost: 7711.0 Init: initializing centroids Init: initializing clusters Starting iterations... Run 7, iteration: 1/100, moves: 1194, cost: 8319.0 Run 7, iteration: 2/100, moves: 139, cost: 8319.0 Init: initializing centroids Init: initializing clusters Starting iterations... Run 8, iteration: 1/100, moves: 675, cost: 7645.0 Init: initializing centroids Init: initializing clusters Starting iterations... Run 9, iteration: 1/100, moves: 0, cost: 7661.0 Init: initializing centroids Init: initializing clusters Starting iterations... Run 10, iteration: 1/100, moves: 366, cost: 7059.0 Best run was number 1 . array([3, 1, 2, ..., 0, 4, 2], dtype=uint16) . Visualizing different features counts of k=7 . for col in object_cols: plt.subplots(figsize = (10,5)) sns.countplot(x=&#39;clusters&#39; ,hue=col, data = travel_kmodes_clustering) plt.show() . . Decision Trees . from sklearn.tree import DecisionTreeClassifier,plot_tree . Apply Decision Trees to the X_train dataset without cross-validation . final_X_train.columns . Index([ &#39;Age&#39;, &#39;DurationOfPitch&#39;, &#39;NumberOfFollowups&#39;, &#39;PreferredPropertyStar&#39;, &#39;NumberOfTrips&#39;, &#39;NumberOfChildrenVisiting&#39;, &#39;MonthlyIncome&#39;, &#39;CityTier&#39;, &#39;NumberOfPersonVisiting&#39;, &#39;Passport&#39;, &#39;PitchSatisfactionScore&#39;, &#39;OwnCar&#39;, &#39;Occupation&#39;, &#39;ProductPitched&#39;, &#39;Designation&#39;, 0, 1, 2, 3, 4, 5, 6], dtype=&#39;object&#39;) . final_X_train_dt = final_X_train[[ &#39;Age&#39;, &#39;DurationOfPitch&#39;, &#39;NumberOfFollowups&#39;, &#39;PreferredPropertyStar&#39;, &#39;NumberOfTrips&#39;, &#39;NumberOfChildrenVisiting&#39;, &#39;MonthlyIncome&#39;, &#39;CityTier&#39;, &#39;NumberOfPersonVisiting&#39;, &#39;Passport&#39;, &#39;PitchSatisfactionScore&#39;, &#39;OwnCar&#39;, &#39;Occupation&#39;, &#39;ProductPitched&#39;, &#39;Designation&#39;]] final_X_test_dt = final_X_test[[ &#39;Age&#39;, &#39;DurationOfPitch&#39;, &#39;NumberOfFollowups&#39;, &#39;PreferredPropertyStar&#39;, &#39;NumberOfTrips&#39;, &#39;NumberOfChildrenVisiting&#39;, &#39;MonthlyIncome&#39;, &#39;CityTier&#39;, &#39;NumberOfPersonVisiting&#39;, &#39;Passport&#39;, &#39;PitchSatisfactionScore&#39;, &#39;OwnCar&#39;, &#39;Occupation&#39;, &#39;ProductPitched&#39;, &#39;Designation&#39;]] . Tree of depth = 1 . dt_clf = DecisionTreeClassifier(max_depth=1, random_state=1) dt_clf.fit(final_X_train_dt, y_train) . DecisionTreeClassifier(max_depth=1, random_state=1) . fig = plt.figure(figsize=(8,4)) plot_tree(dt_clf, fontsize = 15,feature_names = final_X_train_dt.columns, class_names = [&quot;Not Taken&quot;, &quot;Taken&quot;], label = &quot;all&quot;, rounded = True ,filled = True) . [Text(223.2, 163.07999999999998, &#39;Passport &lt;= 0.5 ngini = 0.305 nsamples = 3902 nvalue = [3168, 734] nclass = Not Taken&#39;), Text(111.6, 54.360000000000014, &#39;gini = 0.22 nsamples = 2800 nvalue = [2447, 353] nclass = Not Taken&#39;), Text(334.79999999999995, 54.360000000000014, &#39;gini = 0.452 nsamples = 1102 nvalue = [721, 381] nclass = Not Taken&#39;)] . dt_clf.predict_proba(final_X_test_dt) . array([[0.65426497, 0.34573503], [0.87392857, 0.12607143], [0.87392857, 0.12607143], ..., [0.87392857, 0.12607143], [0.87392857, 0.12607143], [0.87392857, 0.12607143]]) . dt_clf.predict(final_X_test_dt) . array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int64) . score = dt_clf.score(final_X_test_dt, y_test) score . 0.8114754098360656 . Tree of depth = 2 . dt_clf_2 = DecisionTreeClassifier(max_depth=2, random_state=1) dt_clf_2.fit(final_X_train_dt, y_train) . DecisionTreeClassifier(max_depth=2, random_state=1) . fig = plt.figure(figsize=(15,6)) plot_tree(dt_clf_2, fontsize = 15,feature_names = final_X_train_dt.columns, class_names = [&quot;Not Taken&quot;, &quot;Taken&quot;], label = &quot;all&quot;, rounded = True ,filled = True) . [Text(418.5, 271.8, &#39;Passport &lt;= 0.5 ngini = 0.305 nsamples = 3902 nvalue = [3168, 734] nclass = Not Taken&#39;), Text(209.25, 163.08, &#39;Age &lt;= 21.5 ngini = 0.22 nsamples = 2800 nvalue = [2447, 353] nclass = Not Taken&#39;), Text(104.625, 54.360000000000014, &#39;gini = 0.499 nsamples = 71 nvalue = [37, 34] nclass = Not Taken&#39;), Text(313.875, 54.360000000000014, &#39;gini = 0.206 nsamples = 2729 nvalue = [2410, 319] nclass = Not Taken&#39;), Text(627.75, 163.08, &#39;ProductPitched &lt;= 0.5 ngini = 0.452 nsamples = 1102 nvalue = [721, 381] nclass = Not Taken&#39;), Text(523.125, 54.360000000000014, &#39;gini = 0.494 nsamples = 440 nvalue = [195, 245] nclass = Taken&#39;), Text(732.375, 54.360000000000014, &#39;gini = 0.326 nsamples = 662 nvalue = [526, 136] nclass = Not Taken&#39;)] . score = dt_clf_2.score(final_X_test_dt, y_test) score . 0.8381147540983607 . Tree of depth = 3 . dt_clf_3 = DecisionTreeClassifier(max_depth=3, random_state=1) dt_clf_3.fit(final_X_train_dt, y_train) . DecisionTreeClassifier(max_depth=3, random_state=1) . fig = plt.figure(figsize=(25,10)) plot_tree(dt_clf_3, fontsize = 15,feature_names = final_X_train_dt.columns, class_names = [&quot;Not Taken&quot;, &quot;Taken&quot;], label = &quot;all&quot;, rounded = True ,filled = True) . [Text(697.5, 475.65000000000003, &#39;Passport &lt;= 0.5 ngini = 0.305 nsamples = 3902 nvalue = [3168, 734] nclass = Not Taken&#39;), Text(348.75, 339.75, &#39;Age &lt;= 21.5 ngini = 0.22 nsamples = 2800 nvalue = [2447, 353] nclass = Not Taken&#39;), Text(174.375, 203.85000000000002, &#39;Occupation &lt;= 0.5 ngini = 0.499 nsamples = 71 nvalue = [37, 34] nclass = Not Taken&#39;), Text(87.1875, 67.94999999999999, &#39;gini = 0.0 nsamples = 11 nvalue = [0, 11] nclass = Taken&#39;), Text(261.5625, 67.94999999999999, &#39;gini = 0.473 nsamples = 60 nvalue = [37, 23] nclass = Not Taken&#39;), Text(523.125, 203.85000000000002, &#39;PreferredPropertyStar &lt;= 4.5 ngini = 0.206 nsamples = 2729 nvalue = [2410, 319] nclass = Not Taken&#39;), Text(435.9375, 67.94999999999999, &#39;gini = 0.176 nsamples = 2215 nvalue = [1999, 216] nclass = Not Taken&#39;), Text(610.3125, 67.94999999999999, &#39;gini = 0.32 nsamples = 514 nvalue = [411, 103] nclass = Not Taken&#39;), Text(1046.25, 339.75, &#39;ProductPitched &lt;= 0.5 ngini = 0.452 nsamples = 1102 nvalue = [721, 381] nclass = Not Taken&#39;), Text(871.875, 203.85000000000002, &#39;Age &lt;= 30.5 ngini = 0.494 nsamples = 440 nvalue = [195, 245] nclass = Taken&#39;), Text(784.6875, 67.94999999999999, &#39;gini = 0.412 nsamples = 172 nvalue = [50, 122] nclass = Taken&#39;), Text(959.0625, 67.94999999999999, &#39;gini = 0.497 nsamples = 268 nvalue = [145, 123] nclass = Not Taken&#39;), Text(1220.625, 203.85000000000002, &#39;CityTier &lt;= 2.5 ngini = 0.326 nsamples = 662 nvalue = [526, 136] nclass = Not Taken&#39;), Text(1133.4375, 67.94999999999999, &#39;gini = 0.179 nsamples = 382 nvalue = [344, 38] nclass = Not Taken&#39;), Text(1307.8125, 67.94999999999999, &#39;gini = 0.455 nsamples = 280 nvalue = [182, 98] nclass = Not Taken&#39;)] . score = dt_clf_3.score(final_X_test_dt, y_test) score . 0.8391393442622951 . Classifying with lower number of features . dt_cols = [&#39;Age&#39;,&#39;PreferredPropertyStar&#39;,&#39;MonthlyIncome&#39;,&#39;CityTier&#39;,&#39;OwnCar&#39;,&#39;Occupation&#39;, &#39;Designation&#39;] X_train_dt = final_X_train[dt_cols] y_train_dt = final_X_train[&#39;ProductPitched&#39;] X_test_dt = final_X_test[dt_cols] y_test_dt = final_X_test[&#39;ProductPitched&#39;] . dt_clf_4 = DecisionTreeClassifier(max_depth=3, random_state=1) dt_clf_4.fit(X_train_dt, y_train_dt) . DecisionTreeClassifier(max_depth=3, random_state=1) . fig = plt.figure(figsize=(25,10)) plot_tree(dt_clf_4, fontsize = 15,feature_names = X_train_dt.columns, class_names = [&quot;Basic&quot;, &quot;Standard&quot;, &quot;Deluxe&quot;, &quot;Super Deluxe&quot;, &quot;King&quot;], label = &quot;all&quot;, rounded = True ,filled = True) . [Text(620.0, 475.65000000000003, &#39;Designation &lt;= 1.5 ngini = 0.703 nsamples = 3902 nvalue = [1476, 1376, 195, 576, 279] nclass = Basic&#39;), Text(310.0, 339.75, &#39;Designation &lt;= 0.5 ngini = 0.267 nsamples = 1755 nvalue = [1476, 0, 0, 0, 279] nclass = Basic&#39;), Text(155.0, 203.85000000000002, &#39;gini = 0.0 nsamples = 279 nvalue = [0, 0, 0, 0, 279] nclass = King&#39;), Text(465.0, 203.85000000000002, &#39;gini = 0.0 nsamples = 1476 nvalue = [1476, 0, 0, 0, 0] nclass = Basic&#39;), Text(930.0, 339.75, &#39;Designation &lt;= 2.5 ngini = 0.509 nsamples = 2147 nvalue = [0, 1376, 195, 576, 0] nclass = Standard&#39;), Text(775.0, 203.85000000000002, &#39;gini = 0.0 nsamples = 1376 nvalue = [0, 1376, 0, 0, 0] nclass = Standard&#39;), Text(1085.0, 203.85000000000002, &#39;Designation &lt;= 3.5 ngini = 0.378 nsamples = 771 nvalue = [0, 0, 195, 576, 0] nclass = Super Deluxe&#39;), Text(930.0, 67.94999999999999, &#39;gini = 0.0 nsamples = 576 nvalue = [0, 0, 0, 576, 0] nclass = Super Deluxe&#39;), Text(1240.0, 67.94999999999999, &#39;gini = 0.0 nsamples = 195 nvalue = [0, 0, 195, 0, 0] nclass = Deluxe&#39;)] . Here the decision tree cant be branched any further as its leaf nodes are totally pure &amp; it seems it overfitted the training data . ord_encoder.categories_ . [array([&#39;Large Business&#39;, &#39;Salaried&#39;, &#39;Small Business&#39;], dtype=object), array([&#39;Basic&#39;, &#39;Deluxe&#39;, &#39;King&#39;, &#39;Standard&#39;, &#39;Super Deluxe&#39;], dtype=object), array([&#39;AVP&#39;, &#39;Executive&#39;, &#39;Manager&#39;, &#39;Senior Manager&#39;, &#39;VP&#39;], dtype=object)] . score = dt_clf_4.score(X_test_dt, y_test) score . 0.29405737704918034 . Limiting max # of features . dt_clf_5 = DecisionTreeClassifier(max_depth=3, max_features = 2, random_state=1) dt_clf_5.fit(X_train_dt, y_train_dt) fig = plt.figure(figsize=(30,10)) plot_tree(dt_clf_5, fontsize = 15,feature_names = X_train_dt.columns, class_names = [&quot;Basic&quot;, &quot;Standard&quot;, &quot;Deluxe&quot;, &quot;Super Deluxe&quot;, &quot;King&quot;], label = &quot;all&quot;, rounded = True ,filled = True) . [Text(837.0, 475.65000000000003, &#39;MonthlyIncome &lt;= 22623.5 ngini = 0.703 nsamples = 3902 nvalue = [1476, 1376, 195, 576, 279] nclass = Basic&#39;), Text(418.5, 339.75, &#39;MonthlyIncome &lt;= 18694.5 ngini = 0.468 nsamples = 2120 nvalue = [1387, 683, 2, 45, 3] nclass = Basic&#39;), Text(209.25, 203.85000000000002, &#39;Age &lt;= 35.5 ngini = 0.127 nsamples = 666 nvalue = [621, 39, 1, 4, 1] nclass = Basic&#39;), Text(104.625, 67.94999999999999, &#39;gini = 0.042 nsamples = 419 nvalue = [410, 7, 1, 1, 0] nclass = Basic&#39;), Text(313.875, 67.94999999999999, &#39;gini = 0.253 nsamples = 247 nvalue = [211, 32, 0, 3, 1] nclass = Basic&#39;), Text(627.75, 203.85000000000002, &#39;CityTier &lt;= 2.5 ngini = 0.525 nsamples = 1454 nvalue = [766, 644, 1, 41, 2] nclass = Basic&#39;), Text(523.125, 67.94999999999999, &#39;gini = 0.479 nsamples = 1035 nvalue = [656, 357, 0, 22, 0] nclass = Basic&#39;), Text(732.375, 67.94999999999999, &#39;gini = 0.46 nsamples = 419 nvalue = [110, 287, 1, 19, 2] nclass = Standard&#39;), Text(1255.5, 339.75, &#39;MonthlyIncome &lt;= 25644.0 ngini = 0.722 nsamples = 1782 nvalue = [89, 693, 193, 531, 276] nclass = Standard&#39;), Text(1046.25, 203.85000000000002, &#39;Designation &lt;= 2.5 ngini = 0.427 nsamples = 870 nvalue = [71, 634, 0, 164, 1] nclass = Standard&#39;), Text(941.625, 67.94999999999999, &#39;gini = 0.183 nsamples = 706 nvalue = [71, 634, 0, 0, 1] nclass = Standard&#39;), Text(1150.875, 67.94999999999999, &#39;gini = 0.0 nsamples = 164 nvalue = [0, 0, 0, 164, 0] nclass = Super Deluxe&#39;), Text(1464.75, 203.85000000000002, &#39;Age &lt;= 39.5 ngini = 0.698 nsamples = 912 nvalue = [18, 59, 193, 367, 275] nclass = Super Deluxe&#39;), Text(1360.125, 67.94999999999999, &#39;gini = 0.414 nsamples = 270 nvalue = [5, 39, 5, 202, 19] nclass = Super Deluxe&#39;), Text(1569.375, 67.94999999999999, &#39;gini = 0.688 nsamples = 642 nvalue = [13, 20, 188, 165, 256] nclass = King&#39;)] . score = dt_clf_5.score(X_test_dt, y_test) score . 0.39549180327868855 . Limiting # of samples in a leaf node . dt_clf_6 = DecisionTreeClassifier(max_depth=3, max_features = 2, min_samples_leaf = 400, random_state=1) dt_clf_6.fit(X_train_dt, y_train_dt) fig = plt.figure(figsize=(30,10)) plot_tree(dt_clf_6, fontsize = 15,feature_names = X_train_dt.columns, class_names = [&quot;Basic&quot;, &quot;Standard&quot;, &quot;Deluxe&quot;, &quot;Super Deluxe&quot;, &quot;King&quot;], label = &quot;all&quot;, rounded = True ,filled = True) . [Text(837.0, 475.65000000000003, &#39;MonthlyIncome &lt;= 22623.5 ngini = 0.703 nsamples = 3902 nvalue = [1476, 1376, 195, 576, 279] nclass = Basic&#39;), Text(334.8, 339.75, &#39;MonthlyIncome &lt;= 18694.5 ngini = 0.468 nsamples = 2120 nvalue = [1387, 683, 2, 45, 3] nclass = Basic&#39;), Text(167.4, 203.85000000000002, &#39;gini = 0.127 nsamples = 666 nvalue = [621, 39, 1, 4, 1] nclass = Basic&#39;), Text(502.20000000000005, 203.85000000000002, &#39;MonthlyIncome &lt;= 20765.5 ngini = 0.525 nsamples = 1454 nvalue = [766, 644, 1, 41, 2] nclass = Basic&#39;), Text(334.8, 67.94999999999999, &#39;gini = 0.424 nsamples = 403 nvalue = [116, 283, 0, 4, 0] nclass = Standard&#39;), Text(669.6, 67.94999999999999, &#39;gini = 0.498 nsamples = 1051 nvalue = [650, 361, 1, 37, 2] nclass = Basic&#39;), Text(1339.2, 339.75, &#39;CityTier &lt;= 2.5 ngini = 0.722 nsamples = 1782 nvalue = [89, 693, 193, 531, 276] nclass = Standard&#39;), Text(1171.8, 203.85000000000002, &#39;MonthlyIncome &lt;= 25644.0 ngini = 0.754 nsamples = 1098 nvalue = [77, 350, 158, 331, 182] nclass = Standard&#39;), Text(1004.4000000000001, 67.94999999999999, &#39;gini = 0.485 nsamples = 488 nvalue = [66, 332, 0, 90, 0] nclass = Standard&#39;), Text(1339.2, 67.94999999999999, &#39;gini = 0.687 nsamples = 610 nvalue = [11, 18, 158, 241, 182] nclass = Super Deluxe&#39;), Text(1506.6000000000001, 203.85000000000002, &#39;gini = 0.641 nsamples = 684 nvalue = [12, 343, 35, 200, 94] nclass = Standard&#39;)] . score = dt_clf_6.score(X_test_dt, y_test) score . 0.41290983606557374 . Applying cross-validation . y_train_scores = cross_val_score(dt_clf_6, X_train_dt, y_train_dt, cv=5, scoring = &quot;accuracy&quot;) print(y_train_scores) print(y_train_scores.mean()) . [0.61331626 0.60819462 0.62564103 0.61153846 0.60769231] 0.6132765356709019 . y_train_scores = cross_val_score(dt_clf_5, X_train_dt, y_train_dt, cv=5, scoring = &quot;accuracy&quot;) print(y_train_scores) print(y_train_scores.mean()) . [0.72599232 0.72215109 0.73205128 0.71025641 0.71538462] 0.7211671427164386 . Its obvious that corss-validation applied on dt_clf_5 &amp; dt_clf_6 increased the training accuracy . y_train_scores = cross_val_score(dt_clf_3, final_X_test_dt, y_test, cv=5, scoring = &quot;accuracy&quot;) print(y_train_scores) print(y_train_scores.mean()) . [0.85714286 0.79487179 0.83589744 0.84102564 0.86666667] 0.8391208791208792 . dt_clf_3_1 = DecisionTreeClassifier(max_depth=3, max_features = 2, random_state=1) dt_clf_3_1.fit(final_X_train_dt, y_train) fig = plt.figure(figsize=(25,10)) plot_tree(dt_clf_3_1, fontsize = 15,feature_names = final_X_train_dt.columns, class_names = [&quot;Not Taken&quot;, &quot;Taken&quot;], label = &quot;all&quot;, rounded = True ,filled = True) . [Text(697.5, 475.65000000000003, &#39;MonthlyIncome &lt;= 21304.5 ngini = 0.305 nsamples = 3902 nvalue = [3168, 734] nclass = Not Taken&#39;), Text(348.75, 339.75, &#39;MonthlyIncome &lt;= 16927.5 ngini = 0.39 nsamples = 1441 nvalue = [1058, 383] nclass = Not Taken&#39;), Text(174.375, 203.85000000000002, &#39;MonthlyIncome &lt;= 16748.0 ngini = 0.497 nsamples = 41 nvalue = [19, 22] nclass = Taken&#39;), Text(87.1875, 67.94999999999999, &#39;gini = 0.496 nsamples = 35 nvalue = [19, 16] nclass = Not Taken&#39;), Text(261.5625, 67.94999999999999, &#39;gini = 0.0 nsamples = 6 nvalue = [0, 6] nclass = Taken&#39;), Text(523.125, 203.85000000000002, &#39;Occupation &lt;= 0.5 ngini = 0.383 nsamples = 1400 nvalue = [1039, 361] nclass = Not Taken&#39;), Text(435.9375, 67.94999999999999, &#39;gini = 0.476 nsamples = 136 nvalue = [83, 53] nclass = Not Taken&#39;), Text(610.3125, 67.94999999999999, &#39;gini = 0.369 nsamples = 1264 nvalue = [956, 308] nclass = Not Taken&#39;), Text(1046.25, 339.75, &#39;NumberOfFollowups &lt;= 5.5 ngini = 0.245 nsamples = 2461 nvalue = [2110, 351] nclass = Not Taken&#39;), Text(871.875, 203.85000000000002, &#39;ProductPitched &lt;= 0.5 ngini = 0.234 nsamples = 2367 nvalue = [2047, 320] nclass = Not Taken&#39;), Text(784.6875, 67.94999999999999, &#39;gini = 0.387 nsamples = 443 nvalue = [327, 116] nclass = Not Taken&#39;), Text(959.0625, 67.94999999999999, &#39;gini = 0.19 nsamples = 1924 nvalue = [1720, 204] nclass = Not Taken&#39;), Text(1220.625, 203.85000000000002, &#39;Age &lt;= 35.5 ngini = 0.442 nsamples = 94 nvalue = [63, 31] nclass = Not Taken&#39;), Text(1133.4375, 67.94999999999999, &#39;gini = 0.494 nsamples = 36 nvalue = [20, 16] nclass = Not Taken&#39;), Text(1307.8125, 67.94999999999999, &#39;gini = 0.383 nsamples = 58 nvalue = [43, 15] nclass = Not Taken&#39;)] . y_train_scores = cross_val_score(dt_clf_3_1, final_X_test_dt, y_test, cv=5, scoring = &quot;accuracy&quot;) print(y_train_scores) print(y_train_scores.mean()) . [0.82142857 0.81538462 0.78974359 0.80512821 0.8 ] 0.8063369963369963 . Cross-validation applied on decision tree of dt_clf_3_1 reduced the accuracy by limiting the number of features in each split, which indicates the model can be generalized to external datasets . Linear Regression &amp; Neural Networks . Applying Logistic Regression without cross-validation . from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score . log_reg = LogisticRegression(solver=&quot;lbfgs&quot;, max_iter=1000, random_state=1) log_reg.fit(middle_X_train, y_train) . LogisticRegression(max_iter=1000, random_state=1) . y_pred = log_reg.predict(middle_X_test) print(&quot;Accuracy:&quot;, accuracy_score(y_test, y_pred)) print(&quot;Precision:&quot;, precision_score(y_test, y_pred)) print(&quot;Recall:&quot;, recall_score(y_test, y_pred)) . Accuracy: 0.8391393442622951 Precision: 0.7454545454545455 Recall: 0.22282608695652173 . The Logistic Classifier can be generalized to new data as it has accuracy of 84% . Apply Softmax Classifier on Multiclass Target . softmax_reg = LogisticRegression(multi_class=&quot;multinomial&quot;, solver=&quot;lbfgs&quot;, random_state=1, max_iter=3000) softmax_reg.fit(middle_X_train, y_train_dt) #using previously choosen decision tree target &quot;y_train_dt&quot; of multiclasses . LogisticRegression(max_iter=3000, multi_class=&#39;multinomial&#39;, random_state=1) . pred_softmax = softmax_reg.predict(middle_X_test) print(&quot;Accuracy:&quot;, accuracy_score(y_test_dt, pred_softmax)) . Accuracy: 0.5522540983606558 . Softmax Classifier seems to be less accurate compared to Binary Logistic Regression Classifier . Applying Logistic Regression with cross-validation . logreg_scores = cross_val_score(log_reg, middle_X_train, y_train, cv=10, scoring = &quot;accuracy&quot;) print(logreg_scores) print(logreg_scores.mean()) . [0.81585678 0.8286445 0.83846154 0.81282051 0.81538462 0.84615385 0.83846154 0.83076923 0.83589744 0.81794872] 0.8280398714669813 . logreg_softmax_scores = cross_val_score(softmax_reg, middle_X_train, y_train_dt, cv=10, scoring = &quot;accuracy&quot;) print(logreg_softmax_scores) print(logreg_softmax_scores.mean()) . C: Users MR anaconda3 lib site-packages sklearn linear_model _logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression n_iter_i = _check_optimize_result( . [0.59079284 0.60102302 0.59487179 0.57179487 0.56923077 0.55897436 0.59487179 0.60512821 0.54358974 0.53846154] 0.576873893370057 . From the experimentation, cross validation didnt improve the Binary Classifier accuracy, but it did improve the Multiclass Classifier accuracy with a small margin . Perceptrons . from sklearn.neural_network import MLPClassifier . Training simple NN with default parameters . mlp = MLPClassifier(max_iter=3000, random_state=1) mlp.fit(middle_X_train, y_train) mlp.score(middle_X_test, y_test) . 0.8114754098360656 . Varying the MLP parameters to check its performance . mlp_2 = MLPClassifier(hidden_layer_sizes = (5,4,3), activation = &quot;logistic&quot;, solver = &quot;sgd&quot;, max_iter=5000,learning_rate_init = 0.01, random_state=1) mlp_2.fit(middle_X_train, y_train) mlp_2.score(middle_X_test, y_test) . 0.8114754098360656 . mlp_3 = MLPClassifier(hidden_layer_sizes = (2,3,2), activation = &quot;tanh&quot;, solver = &quot;sgd&quot;, max_iter=4000,learning_rate_init = 0.01, random_state=1) mlp_3.fit(middle_X_train, y_train) mlp_3.score(middle_X_test, y_test) . 0.8114754098360656 . mlp_4 = MLPClassifier(hidden_layer_sizes = (4,6), activation = &quot;relu&quot;, solver = &quot;adam&quot;, max_iter=6000,learning_rate_init = 0.003, random_state=1) mlp_4.fit(middle_X_train, y_train) mlp_4.score(middle_X_test, y_test) . 0.8114754098360656 . Apply MLP on multiclass target . mlp_5 = MLPClassifier(hidden_layer_sizes = (5,4,3), activation = &quot;logistic&quot;, solver = &quot;sgd&quot;, max_iter=5000,learning_rate_init = 0.01, random_state=1) mlp_5.fit(middle_X_train, y_train_dt) mlp_5.score(middle_X_test, y_test_dt) . 0.36885245901639346 . mlp_6 = MLPClassifier(hidden_layer_sizes = (2,3,5), activation = &quot;tanh&quot;, solver = &quot;sgd&quot;, max_iter=10000,learning_rate_init = 0.001, random_state=1) mlp_6.fit(middle_X_train, y_train_dt) mlp_6.score(middle_X_test, y_test_dt) . 0.36885245901639346 . mlp_7 = MLPClassifier(hidden_layer_sizes = (4,6), activation = &quot;relu&quot;, solver = &quot;adam&quot;, max_iter=6000,learning_rate_init = 0.003, random_state=1) mlp_7.fit(middle_X_train, y_train_dt) mlp_7.score(middle_X_test, y_test_dt) . 0.36885245901639346 . From expermintation, the MLP performance is very close to Binary Logistic Regression Classifier, while its performance is very poor compared to the Multiclass Logistic Regression Classifier. . Also, changing the parameters of the Neural Network didnt change the resulting accuracy, which might be because of the simplicity &amp; the small size of the dataset when its fed to the MLP . Research Question . The Question: . During my experimentation, I encountered a difficulty to apply Clustering on mixed variables of numerical &amp; categorical types. By doing some research, I found a library called Kmodes that applies clustering on categorical variables only, meanwhile sklearn clustering requires numerical input to be able to group similar instances. So, as I have a dataset of mixed variables, I wanted to apply clustering on the full dataset all at once &amp; be able to cluster them combinedly. . **My question is, how to efficiently cluster mixed dataset with categorical &amp; numerical variables ? &amp; how to visualize the resulting clusters in one graph?** . The Answer: . For the first part of the question, the initial idea I thought of is to preprocess one type of the variables &amp; transform it into the other type &amp; combine the result in one-typed dataset. For categorical variables, experimentation can be done to quantify categories in either ordered or unordered manner &amp; find a way to deal with their discrete numerical values to fit in the numerical clusterer, then combine them to the numerical variables &amp; use sklearn to perform clustering. For, numerical variables, we can experiment with different methods of discretization to convert them into discrete values &amp; apply kmodes or a similar library on the final categorical dataset. . For the second part, one way of the representation is to visualize the clusters with counts of each variables in each cluster - wheteher its numerical or categorical - in one graph. Other visualization libraries can be consulted to find a more suitable &amp; effective way of visualizeing the result clusters. . References: . [1] A. Chaturvedi, P.E.Green, and J.D. Carroll (2001). K-modes Clustering. Researchgate [Viewed 2 November 2021]. Available form: https://www.researchgate.net/publication/226946703_K-modes_Clustering . [2] K. Bindra, A. Mishra (2017). &quot;A Detailed Study of Clustering Algorithms&quot;. IEEEXplore [Viewed 4 November 2021]. Available form: https://ieeexplore.ieee.org/document/8342454 . [3] K. Wilamowska, M. Manic (2001). &quot;Unsupervised pattern clustering for data mining&quot;. IEEEXplore [Viewed 4 November 2021]. Available form: https://ieeexplore.ieee.org/document/975574 . [4] Mayra Z. Rodriguez, Cesar H. Comin ,Dalcimar Casanova, Odemir M. Bruno, Diego R. Amancio, Luciano da F. Costa, Francisco A. Rodrigues (2019). &quot;Clustering algorithms: A comparative approach&quot;. Plos One [Viewed 7 November 2021]. Available form: https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0210236 .",
            "url": "https://moraouf.github.io/MoSpace/project/2021/11/28/holiday-package-recommendations-using-different-ml-algorithms.html",
            "relUrl": "/project/2021/11/28/holiday-package-recommendations-using-different-ml-algorithms.html",
            "date": " • Nov 28, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Visualizing Earnings Based On College Majors",
            "content": "import pandas as pd import numpy as np import matplotlib.pyplot as plt %matplotlib inline . recent_grads = pd.read_csv(&#39;recent-grads.csv&#39;) recent_grads.iloc[0] . Rank 1 Major_code 2419 Major PETROLEUM ENGINEERING Total 2339 Men 2057 Women 282 Major_category Engineering ShareWomen 0.120564 Sample_size 36 Employed 1976 Full_time 1849 Part_time 270 Full_time_year_round 1207 Unemployed 37 Unemployment_rate 0.0183805 Median 110000 P25th 95000 P75th 125000 College_jobs 1534 Non_college_jobs 364 Low_wage_jobs 193 Name: 0, dtype: object . recent_grads.head() . Rank Major_code Major Total Men Women Major_category ShareWomen Sample_size Employed ... Part_time Full_time_year_round Unemployed Unemployment_rate Median P25th P75th College_jobs Non_college_jobs Low_wage_jobs . 0 1 | 2419 | PETROLEUM ENGINEERING | 2339.0 | 2057.0 | 282.0 | Engineering | 0.120564 | 36 | 1976 | ... | 270 | 1207 | 37 | 0.018381 | 110000 | 95000 | 125000 | 1534 | 364 | 193 | . 1 2 | 2416 | MINING AND MINERAL ENGINEERING | 756.0 | 679.0 | 77.0 | Engineering | 0.101852 | 7 | 640 | ... | 170 | 388 | 85 | 0.117241 | 75000 | 55000 | 90000 | 350 | 257 | 50 | . 2 3 | 2415 | METALLURGICAL ENGINEERING | 856.0 | 725.0 | 131.0 | Engineering | 0.153037 | 3 | 648 | ... | 133 | 340 | 16 | 0.024096 | 73000 | 50000 | 105000 | 456 | 176 | 0 | . 3 4 | 2417 | NAVAL ARCHITECTURE AND MARINE ENGINEERING | 1258.0 | 1123.0 | 135.0 | Engineering | 0.107313 | 16 | 758 | ... | 150 | 692 | 40 | 0.050125 | 70000 | 43000 | 80000 | 529 | 102 | 0 | . 4 5 | 2405 | CHEMICAL ENGINEERING | 32260.0 | 21239.0 | 11021.0 | Engineering | 0.341631 | 289 | 25694 | ... | 5180 | 16697 | 1672 | 0.061098 | 65000 | 50000 | 75000 | 18314 | 4440 | 972 | . 5 rows × 21 columns . recent_grads.tail() . Rank Major_code Major Total Men Women Major_category ShareWomen Sample_size Employed ... Part_time Full_time_year_round Unemployed Unemployment_rate Median P25th P75th College_jobs Non_college_jobs Low_wage_jobs . 168 169 | 3609 | ZOOLOGY | 8409.0 | 3050.0 | 5359.0 | Biology &amp; Life Science | 0.637293 | 47 | 6259 | ... | 2190 | 3602 | 304 | 0.046320 | 26000 | 20000 | 39000 | 2771 | 2947 | 743 | . 169 170 | 5201 | EDUCATIONAL PSYCHOLOGY | 2854.0 | 522.0 | 2332.0 | Psychology &amp; Social Work | 0.817099 | 7 | 2125 | ... | 572 | 1211 | 148 | 0.065112 | 25000 | 24000 | 34000 | 1488 | 615 | 82 | . 170 171 | 5202 | CLINICAL PSYCHOLOGY | 2838.0 | 568.0 | 2270.0 | Psychology &amp; Social Work | 0.799859 | 13 | 2101 | ... | 648 | 1293 | 368 | 0.149048 | 25000 | 25000 | 40000 | 986 | 870 | 622 | . 171 172 | 5203 | COUNSELING PSYCHOLOGY | 4626.0 | 931.0 | 3695.0 | Psychology &amp; Social Work | 0.798746 | 21 | 3777 | ... | 965 | 2738 | 214 | 0.053621 | 23400 | 19200 | 26000 | 2403 | 1245 | 308 | . 172 173 | 3501 | LIBRARY SCIENCE | 1098.0 | 134.0 | 964.0 | Education | 0.877960 | 2 | 742 | ... | 237 | 410 | 87 | 0.104946 | 22000 | 20000 | 22000 | 288 | 338 | 192 | . 5 rows × 21 columns . recent_grads.describe() . Rank Major_code Total Men Women ShareWomen Sample_size Employed Full_time Part_time Full_time_year_round Unemployed Unemployment_rate Median P25th P75th College_jobs Non_college_jobs Low_wage_jobs . count 173.000000 | 173.000000 | 172.000000 | 172.000000 | 172.000000 | 172.000000 | 173.000000 | 173.000000 | 173.000000 | 173.000000 | 173.000000 | 173.000000 | 173.000000 | 173.000000 | 173.000000 | 173.000000 | 173.000000 | 173.000000 | 173.000000 | . mean 87.000000 | 3879.815029 | 39370.081395 | 16723.406977 | 22646.674419 | 0.522223 | 356.080925 | 31192.763006 | 26029.306358 | 8832.398844 | 19694.427746 | 2416.329480 | 0.068191 | 40151.445087 | 29501.445087 | 51494.219653 | 12322.635838 | 13284.497110 | 3859.017341 | . std 50.084928 | 1687.753140 | 63483.491009 | 28122.433474 | 41057.330740 | 0.231205 | 618.361022 | 50675.002241 | 42869.655092 | 14648.179473 | 33160.941514 | 4112.803148 | 0.030331 | 11470.181802 | 9166.005235 | 14906.279740 | 21299.868863 | 23789.655363 | 6944.998579 | . min 1.000000 | 1100.000000 | 124.000000 | 119.000000 | 0.000000 | 0.000000 | 2.000000 | 0.000000 | 111.000000 | 0.000000 | 111.000000 | 0.000000 | 0.000000 | 22000.000000 | 18500.000000 | 22000.000000 | 0.000000 | 0.000000 | 0.000000 | . 25% 44.000000 | 2403.000000 | 4549.750000 | 2177.500000 | 1778.250000 | 0.336026 | 39.000000 | 3608.000000 | 3154.000000 | 1030.000000 | 2453.000000 | 304.000000 | 0.050306 | 33000.000000 | 24000.000000 | 42000.000000 | 1675.000000 | 1591.000000 | 340.000000 | . 50% 87.000000 | 3608.000000 | 15104.000000 | 5434.000000 | 8386.500000 | 0.534024 | 130.000000 | 11797.000000 | 10048.000000 | 3299.000000 | 7413.000000 | 893.000000 | 0.067961 | 36000.000000 | 27000.000000 | 47000.000000 | 4390.000000 | 4595.000000 | 1231.000000 | . 75% 130.000000 | 5503.000000 | 38909.750000 | 14631.000000 | 22553.750000 | 0.703299 | 338.000000 | 31433.000000 | 25147.000000 | 9948.000000 | 16891.000000 | 2393.000000 | 0.087557 | 45000.000000 | 33000.000000 | 60000.000000 | 14444.000000 | 11783.000000 | 3466.000000 | . max 173.000000 | 6403.000000 | 393735.000000 | 173809.000000 | 307087.000000 | 0.968954 | 4212.000000 | 307933.000000 | 251540.000000 | 115172.000000 | 199897.000000 | 28169.000000 | 0.177226 | 110000.000000 | 95000.000000 | 125000.000000 | 151643.000000 | 148395.000000 | 48207.000000 | . recent_grads.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 173 entries, 0 to 172 Data columns (total 21 columns): # Column Non-Null Count Dtype -- -- 0 Rank 173 non-null int64 1 Major_code 173 non-null int64 2 Major 173 non-null object 3 Total 172 non-null float64 4 Men 172 non-null float64 5 Women 172 non-null float64 6 Major_category 173 non-null object 7 ShareWomen 172 non-null float64 8 Sample_size 173 non-null int64 9 Employed 173 non-null int64 10 Full_time 173 non-null int64 11 Part_time 173 non-null int64 12 Full_time_year_round 173 non-null int64 13 Unemployed 173 non-null int64 14 Unemployment_rate 173 non-null float64 15 Median 173 non-null int64 16 P25th 173 non-null int64 17 P75th 173 non-null int64 18 College_jobs 173 non-null int64 19 Non_college_jobs 173 non-null int64 20 Low_wage_jobs 173 non-null int64 dtypes: float64(5), int64(14), object(2) memory usage: 28.5+ KB . print(&#39;Number of Rows Before :&#39;, len(recent_grads)) recent_grads = recent_grads.dropna() print(&#39;Number of Rows After :&#39;, len(recent_grads)) . Number of Rows Before : 173 Number of Rows After : 172 . p1 = recent_grads.plot(x = &#39;Sample_size&#39;, y = &#39;Median&#39;, kind = &#39;scatter&#39;) p2 = recent_grads.plot(x = &#39;Sample_size&#39;, y = &#39;Unemployment_rate&#39;, kind = &#39;scatter&#39;) p3 = recent_grads.plot(x = &#39;Full_time&#39;, y = &#39;Median&#39;, kind = &#39;scatter&#39;) p4 = recent_grads.plot(x = &#39;ShareWomen&#39;, y = &#39;Unemployment_rate&#39;, kind = &#39;scatter&#39;) p5 = recent_grads.plot(x = &#39;Men&#39;, y = &#39;Median&#39;, kind = &#39;scatter&#39;) p6 = recent_grads.plot(x = &#39;Women&#39;, y = &#39;Median&#39;, kind = &#39;scatter&#39;) . h1 = recent_grads[&#39;Sample_size&#39;].hist(bins = 10, range = (0,4500)) h1.set_title(&#39;Sample_size&#39;) . Text(0.5, 1.0, &#39;Sample_size&#39;) . h2 = recent_grads[&#39;Median&#39;].hist(bins = 20, range = (22000,110000)) h2.set_title(&#39;Median&#39;) . Text(0.5, 1.0, &#39;Median&#39;) . h3 = recent_grads[&#39;Employed&#39;].hist(bins = 10, range = (0,300000)) h3.set_title(&#39;Employed&#39;) . Text(0.5, 1.0, &#39;Employed&#39;) . h4 = recent_grads[&#39;Full_time&#39;].hist(bins = 10, range = (0,250000)) h4.set_title(&#39;Full_time&#39;) . Text(0.5, 1.0, &#39;Full_time&#39;) . h5 = recent_grads[&#39;ShareWomen&#39;].hist(bins = 20, range = (0,1)) h5.set_title(&#39;Share Women&#39;) . Text(0.5, 1.0, &#39;Share Women&#39;) . h6 = recent_grads[&#39;Men&#39;].hist(bins = 10, range = (110,175000)) h6.set_title(&#39;Men&#39;) . Text(0.5, 1.0, &#39;Men&#39;) . h7 = recent_grads[&#39;Women&#39;].hist(bins = 10, range = (0,300000)) h7.set_title(&#39;Women&#39;) . Text(0.5, 1.0, &#39;Women&#39;) . from pandas.plotting import scatter_matrix matrix1 = scatter_matrix(recent_grads[[&#39;Sample_size&#39;, &#39;Median&#39;]]) matrix2 = scatter_matrix(recent_grads[[&#39;Sample_size&#39;, &#39;Median&#39;, &#39;Unemployment_rate&#39;]]) . recent_grads[&#39;ShareWomen&#39;][:10].plot(kind = &#39;bar&#39;) . &lt;AxesSubplot:&gt; . recent_grads[&#39;ShareWomen&#39;][-10:-1].plot(kind = &#39;bar&#39;) . &lt;AxesSubplot:&gt; . recent_grads[:10].plot.bar(x=&#39;Major_category&#39;, y=&#39;Unemployment_rate&#39;) # OR # recent_grads[&#39;Unemployment_rate&#39;][:10].plot(kind = &#39;bar&#39;) . &lt;AxesSubplot:xlabel=&#39;Major_category&#39;&gt; . recent_grads[-10:-1].plot.bar(x=&#39;Major_category&#39;, y=&#39;Unemployment_rate&#39;) . &lt;AxesSubplot:xlabel=&#39;Major_category&#39;&gt; .",
            "url": "https://moraouf.github.io/MoSpace/project/2021/05/12/visualizing-earnings.html",
            "relUrl": "/project/2021/05/12/visualizing-earnings.html",
            "date": " • May 12, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "PyMC3 with labeled coords and dims",
            "content": "import arviz as az import matplotlib.pyplot as plt import numpy as np import pandas as pd import pymc3 as pm import theano.tensor as tt rng = np.random.default_rng() . az.style.use(&quot;arviz-darkgrid&quot;) . For the :heart: of labeled arrays . For all of us who love labeled arrays, PyMC 3.9.0 came with some amazing news: support for using coordinate and dimension names to specify the shapes of variables in a pm.Model. While this is good news by its own merit, its seamless integration with ArviZ even more impactful and relevant. . This post will focus on using PyMC3 coords and dims and the conversion of traces and models to InferenceData using arviz.from_pymc3. To see InferenceData in action, refer to this example in PyMC docs. . We will use an example based approach and use models from the example gallery to illustrate how to use coords and dims within PyMC3 models. . 1st example: rugby analytics . We will use an alternative parametrization of the same model used in the rugby analytics example taking advantage of dims and coords. Here, we will use as observations a 2d matrix, whose rows are the matches and whose columns are the field: home and away. . The first step after preprocessing is to define the dimensions used by the model and their coordinates. In our case, we have 3 dimensions: . team: each team will have its own offensive and defensive power | match: an integer identifying the match. There are 6 teams who play twice against each other so we have 6*5*2=60 matches | field: either home or away. | . These coordinates are passed to pm.Model as a dict whose keys are dimension names and whose values are coordinate values. The dimensions can then be used when defining PyMC3 variables to indicate their shape. . df_rugby = pd.read_csv(pm.get_data(&#39;rugby.csv&#39;), index_col=0) home_idx, teams = pd.factorize(df_rugby[&quot;home_team&quot;], sort=True) away_idx, _ = pd.factorize(df_rugby[&quot;away_team&quot;], sort=True) . coords = { &quot;team&quot;: teams, &quot;match&quot;: np.arange(60), &quot;field&quot;: [&quot;home&quot;, &quot;away&quot;], } with pm.Model(coords=coords) as rugby_model: # global model parameters home = pm.Flat(&#39;home&#39;) sd_att = pm.HalfStudentT(&#39;sd_att&#39;, nu=3, sigma=2.5) sd_def = pm.HalfStudentT(&#39;sd_def&#39;, nu=3, sigma=2.5) intercept = pm.Flat(&#39;intercept&#39;) # team-specific model parameters atts_star = pm.Normal(&quot;atts_star&quot;, mu=0, sigma=sd_att, dims=&quot;team&quot;) defs_star = pm.Normal(&quot;defs_star&quot;, mu=0, sigma=sd_def, dims=&quot;team&quot;) atts = pm.Deterministic(&#39;atts&#39;, atts_star - tt.mean(atts_star), dims=&quot;team&quot;) defs = pm.Deterministic(&#39;defs&#39;, defs_star - tt.mean(defs_star), dims=&quot;team&quot;) home_theta = tt.exp(intercept + home + atts[home_idx] + defs[away_idx]) away_theta = tt.exp(intercept + atts[away_idx] + defs[home_idx]) # likelihood of observed data points = pm.Poisson( &#39;home_points&#39;, mu=tt.stack((home_theta, away_theta)).T, observed=df_rugby[[&quot;home_score&quot;, &quot;away_score&quot;]], dims=(&quot;match&quot;, &quot;field&quot;) ) rugby_trace = pm.sample(1000, tune=1000, cores=4) . Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (4 chains in 4 jobs) NUTS: [defs_star, atts_star, intercept, sd_def, sd_att, home] . . 100.00% [8000/8000 00:12&lt;00:00 Sampling 4 chains, 0 divergences] Sampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 13 seconds. The number of effective samples is smaller than 25% for some parameters. . We have now defined the shapes or our variables, which is convenient and helps understanding the code, but the dimensions and coordinates are lost during sampling. pm.MultiTrace objects do not store the labeled coordinates of their variables. . print(rugby_trace) print(rugby_trace[&quot;atts&quot;]) . &lt;MultiTrace: 4 chains, 1000 iterations, 10 variables&gt; [[ 0.17387955 -0.02422528 0.13753953 -0.36843038 -0.04525461 0.1264912 ] [ 0.25936021 -0.10837995 0.14656402 -0.2846651 -0.16805903 0.15517984] [ 0.24825271 -0.10056153 0.05230409 -0.23062107 -0.10570005 0.13632584] ... [ 0.24992917 -0.0886659 0.08541612 -0.32679123 -0.04984795 0.1299598 ] [ 0.25626853 -0.07558793 0.14828311 -0.36233563 -0.19319771 0.22656963] [ 0.25679719 -0.12705182 0.12258069 -0.29169309 -0.11814407 0.1575111 ]] . To also take advantage of the labeled coords and dims for exploratory analysis of the results, we have to convert the results to az.InferenceData. This can be done with az.from_pymc3 or using the return_inferencedata=True argument in pm.sample. To avoid having to resample again, we will use the former and use the latter in the second example. . ArviZ is aware of the model context, and will use it to get the coords and dims automatically. If necessary however, we may also modify or add dimensions or coordinates using the dims/coords arguments of from_pymc3. We&#39;ll also see an example of this afterwards. . rugby_idata = az.from_pymc3(rugby_trace, model=rugby_model) rugby_idata . Inference data with groups: &gt; posterior &gt; log_likelihood &gt; sample_stats &gt; observed_data . 2nd example: radon multilevel model . We will now use one of the many models in the A Primer on Bayesian Methods for Multilevel Modeling notebook to dive deeper into coords and dims functionality. We won&#39;t cover the model itself, it&#39;s already explained in the example notebook, we will explain in detail how are labeled coords and dims being used. . The code used to load and clean the data is hidden, click the button below to see it. . #collapse-hide srrs2 = pd.read_csv(pm.get_data(&quot;srrs2.dat&quot;)) srrs2.columns = srrs2.columns.map(str.strip) srrs_mn = srrs2[srrs2.state == &quot;MN&quot;].copy() srrs_mn[&quot;fips&quot;] = srrs_mn.stfips * 1000 + srrs_mn.cntyfips cty = pd.read_csv(pm.get_data(&quot;cty.dat&quot;)) cty_mn = cty[cty.st == &quot;MN&quot;].copy() cty_mn[&quot;fips&quot;] = 1000 * cty_mn.stfips + cty_mn.ctfips srrs_mn = srrs_mn.merge(cty_mn[[&quot;fips&quot;, &quot;Uppm&quot;]], on=&quot;fips&quot;) srrs_mn = srrs_mn.drop_duplicates(subset=&quot;idnum&quot;) u = np.log(srrs_mn.Uppm).unique() n = len(srrs_mn) srrs_mn.county = srrs_mn.county.map(str.strip) srrs_mn[&quot;county_code&quot;], mn_counties = pd.factorize(srrs_mn.county) srrs_mn[&quot;log_radon&quot;] = np.log(srrs_mn.activity + 0.1) . . The first step is again defining the dimensions and their coordinate values: . Level: observations can correspond to the basement or the first floor | obs_id: unique integer identifying each observation | County: each county has its own basement, intercept: a, and first floor, slope b, effects. Details are in the example notebook | param: one of a, b | param_bis: same as param, used for the covariance matrix because a variable can&#39;t have repeated dimensions | . coords = { &quot;Level&quot;: [&quot;Basement&quot;, &quot;Floor&quot;], &quot;obs_id&quot;: np.arange(n), &quot;County&quot;: mn_counties, &quot;param&quot;: [&quot;a&quot;, &quot;b&quot;], &quot;param_bis&quot;: [&quot;a&quot;, &quot;b&quot;], } . We&#39;ll begin to define the model creating the indexing arrays that will implement the hierarchical model. We are using the pm.Data container to tell ArviZ to store the variables in the constant_data group. Moreover, pm.Data defines a theano shared variable, so its values can be modified in order to call pm.sample_posterior_predictive using different data. This is particularly interesting for regressions for example in order to generate predictions for the model. . with pm.Model(coords=coords) as radon_model: floor_idx = pm.Data(&quot;floor_idx&quot;, srrs_mn.floor, dims=&quot;obs_id&quot;) county_idx = pm.Data(&quot;county_idx&quot;, srrs_mn.county_code, dims=&quot;obs_id&quot;) . We&#39;ll also use a LKJCholeskyCovas prior for the covariance matrix. As you can see, it has no dims argument. Given that we are going to use return_inferencedata=True here in order to get an InferenceData directly as a result of pm.sample, we will have to indicate the dims that correspond to these variables as idata_kwargs. idata_kwargs is used to indicate pm.sample what arguments to pass to az.from_pymc3, which is called internally to convert the trace to InferenceData. . with radon_model: sd_dist = pm.Exponential.dist(0.5) a = pm.Normal(&quot;a&quot;, mu=0.0, sigma=5.0) b = pm.Normal(&quot;b&quot;, mu=0.0, sigma=1.0) z = pm.Normal(&quot;z&quot;, 0.0, 1.0, dims=(&quot;param&quot;, &quot;County&quot;)) chol, corr, stds = pm.LKJCholeskyCov( &quot;chol&quot;, n=2, eta=2.0, sd_dist=sd_dist, compute_corr=True ) . We now will store two intermediate results as variables. However, one is wrapped inside a pm.Deterministic whereas the other is not. Both are equally valid. pm.Deterministic tells PyMC3 to store that variable in the trace. Thus pm.Deterministic should only be used when we actively want to store the intermediate result. In our case, we want to store ab_county but not theta. . with radon_model: ab_county = pm.Deterministic(&quot;ab_county&quot;, tt.dot(chol, z).T, dims=(&quot;County&quot;, &quot;param&quot;)) theta = a + ab_county[county_idx, 0] + (b + ab_county[county_idx, 1]) * floor_idx sigma = pm.Exponential(&quot;sigma&quot;, 1.0) y = pm.Normal(&quot;y&quot;, theta, sigma=sigma, observed=srrs_mn.log_radon, dims=&quot;obs_id&quot;) . Finally we will call pm.sample with return_inferencedata=True and defining the dimensions of the covariance matrix as idata_kwargs. . with radon_model: radon_idata = pm.sample( 2000, tune=2000, target_accept=0.99, random_seed=75625, return_inferencedata=True, idata_kwargs={&quot;dims&quot;: {&quot;chol_stds&quot;: [&quot;param&quot;], &quot;chol_corr&quot;: [&quot;param&quot;, &quot;param_bis&quot;]}} ) . Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (4 chains in 4 jobs) NUTS: [sigma, chol, z, b, a] . . 100.00% [16000/16000 03:17&lt;00:00 Sampling 4 chains, 0 divergences] Sampling 4 chains for 2_000 tune and 2_000 draw iterations (8_000 + 8_000 draws total) took 197 seconds. The number of effective samples is smaller than 25% for some parameters. . There is life outside the posterior . The posterior is the center of Bayesian analysis but other quantities such as the prior or the posterior predictive are also crucial to an analysis workflow. We&#39;ll use a linear regression to quickly showcase some of the key steps in a Bayesian workflow: prior predictive checks, posterior sampling, posterior predictive checks (using LOO-PIT) and out of sample predictions. . We will start generating some simulated data (code hidden, click to expand) and defining the model. As it&#39;s a simple linear regression we&#39;ll only have scalar parameters, a, b and sigma. . #collapse-hide a_ = 2 b_ = -0.4 x_ = np.linspace(0, 10, 31) year_ = np.arange(2021-len(x_), 2021) y_ = a_ + b_ * x_ + rng.normal(size=len(x_)) fig, ax = plt.subplots() ax.plot(x_, y_, &quot;o-&quot;) ax.text( 0.93, 0.9, r&quot;$y_i = a + bx_i + mathcal{N}(0,1)$&quot;, ha=&#39;right&#39;, va=&#39;top&#39;, transform=ax.transAxes, fontsize=18 ) ax.set_xticks(x_[::3]) ax.set_xticklabels(year_[::3]) ax.set_yticks([]) ax.set_xlabel(&quot;Year&quot;) ax.set_ylabel(&quot;Quantity of interest&quot;); . . coords = {&quot;year&quot;: year_} with pm.Model(coords=coords) as linreg_model: x = pm.Data(&quot;x&quot;, x_, dims=&quot;year&quot;) a = pm.Normal(&quot;a&quot;, 0, 3) b = pm.Normal(&quot;b&quot;, 0, 2) sigma = pm.HalfNormal(&quot;sigma&quot;, 2) y = pm.Normal(&quot;y&quot;, a + b * x, sigma, observed=y_, dims=&quot;year&quot;) . We have now written a model in order to study our super interesting quantity y. We have used everything we have seen so far, the pm.Data container and the labeled dims and coords. We will now simulate a workflow starting from prior predictive checks and finishing with predicting the values of our quantity of interest in 2021 and 2022. . Priors . We start by sampling both prior and prior predictive with pm.sample_prior_predictive. This will generate a dictionary whose keys are variable names and whose values are numpy arrays. We can then pass this dictionary to az.from_pymc3 as the prior argument. ArviZ will then use the information in the pm.Model instance to 1) split the variables between prior and prior_predictive groups, 2) fill the observed_data and constant_data groups and 3) get the dims and coords if present. . with linreg_model: prior = pm.sample_prior_predictive(700) linreg_idata = az.from_pymc3(prior=prior) linreg_idata . Inference data with groups: &gt; prior &gt; prior_predictive &gt; observed_data . We can now use plot_ppc to perform prior predictive checks for our model. . az.plot_ppc(linreg_idata, group=&quot;prior&quot;); . Posterior . The next step will be computing the posterior. As we have seen, we can use return_inferencedata to get an InferenceData as a result of pm.sample. In this case however, we will store it as an auxiliary variable to then use InferenceData.extend and add the new groups to the linreg_idata. . with linreg_model: idata_aux = pm.sample(return_inferencedata=True) linreg_idata.extend(idata_aux) . Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (4 chains in 4 jobs) NUTS: [sigma, b, a] . . 100.00% [8000/8000 00:03&lt;00:00 Sampling 4 chains, 0 divergences] Sampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 4 seconds. The acceptance probability does not match the target. It is 0.879454342399517, but should be close to 0.8. Try to increase the number of tuning steps. . linreg_idata . Inference data with groups: &gt; prior &gt; prior_predictive &gt; observed_data &gt; posterior &gt; log_likelihood &gt; sample_stats &gt; constant_data . az.plot_pair(linreg_idata); . Posterior predictive . Our third step will be to evaluate the posterior predictive at the observations so we can perform model checking with functions such as plot_ppc or plot_loo_pit. Here again we are using the extend trick to keep all our data as part of the same InferenceData. This has two main advantages. plot_loo_pit requires both the posterior_predictive group, generated here and the log_likelihood group which was created in pm.sample. In addition, keeping all our data in a single InferenceData means we can store it as a netCDF and share a single file to allow reproducing the whole exploratory analysis of our model. . with linreg_model: post_pred = pm.sample_posterior_predictive(linreg_idata) idata_aux = az.from_pymc3(posterior_predictive=post_pred) linreg_idata.extend(idata_aux) . . 100.00% [4000/4000 00:36&lt;00:00] linreg_idata . Inference data with groups: &gt; prior &gt; prior_predictive &gt; observed_data &gt; posterior &gt; log_likelihood &gt; sample_stats &gt; constant_data &gt; posterior_predictive . We will now get to use plot_loo_pit, which as expected does not show any issues. To learn how to interpret those plots, you can read the LOO-PIT tutorial. . az.plot_loo_pit(linreg_idata, y=&quot;y&quot;); . Predictions . Finally, our last step will be to get some predictions, which in this case is evaluating the posterior predictive at positions different than the observations. In the example below, we are evaluating our predictions at 2021 and 2020. To do so, we are using pm.set_data to modify the values of x to the ones that correspond to these two future years. . Here we will use from_pymc3_predictions instead of from_pymc3+extend. from_pymc3_predictions combines functionality from both of these functions and let&#39;s the user choose how to handle predictions depending on the goal at hand: if idata_orig is not present, the returned object will be an InferenceData containing only the predictions groups; if idata_orig is present and inplace=False the returned InferenceData will be a copy of idata_orig with the predictions groups added, and with inplace=True there is no returned object, the preditcions groups are added to idata_orig which is not returned. . with linreg_model: pm.set_data({&quot;x&quot;: x_[-1] + x_[1:3]}) predictions = pm.sample_posterior_predictive(linreg_idata) az.from_pymc3_predictions( predictions, coords={&quot;year&quot;: [2021, 2022]}, idata_orig=linreg_idata, inplace=True ) . . 100.00% [4000/4000 01:13&lt;00:00] linreg_idata . Inference data with groups: &gt; posterior &gt; posterior_predictive &gt; predictions &gt; log_likelihood &gt; sample_stats &gt; prior &gt; prior_predictive &gt; observed_data &gt; constant_data &gt; predictions_constant_data . az.plot_posterior(linreg_idata, group=&quot;predictions&quot;); . Extra: generating the post image . Here is the code used to generate the summary image for this post. Take a look if you want to use matplotlib to create array diagrams! . #collapse-hide from mpl_toolkits.mplot3d import Axes3D points = np.array([ [-1, -1, -1], [1, -1, -1 ], [1, 1, -1], [-1, 1, -1], [-1, -1, 1], [1, -1, 1 ], [1, 1, 1], [-1, 1, 1] ]) fig = plt.figure(dpi=300) ax = fig.add_axes([0, .05, 1, .8], projection=&#39;3d&#39;) side = 3 r = side * np.array([-1,1]) one_2d = side * np.ones((1,1)) one_1d = side * np.ones(1) X, Y = np.meshgrid(r, r) ################ ### theta ### ################ ax.plot_surface(X,Y,one_2d, color=&quot;C0&quot;, zorder=1) ax.plot_surface(X,-one_2d,Y, color=&quot;C0&quot;, zorder=1) ax.plot_surface(one_2d,X,Y, color=&quot;C0&quot;, zorder=1) ax.text2D(0.3, 0.14, r&quot;$ theta$&quot;, transform=ax.transAxes) # school dim school_grid = np.linspace(-1, 1, 8) * side school_one = np.ones_like(school_grid) * side * 1.1 schools = np.array([&quot;Choate&quot;, &quot;Deerfield&quot;, &quot;Phillips Andover&quot;, &quot;Phillips Exeter&quot;, &quot;Hotchkiss&quot;, &quot;Lawrenceville&quot;, &quot;St. Paul&#39;s&quot;, &quot;Mt. Hermon&quot;]) ax.plot(school_grid, school_one, school_one, marker=&quot;|&quot;, color=&quot;k&quot;) for school, pos in zip(schools, school_grid): ax.text(pos, side*1.1, side*1.15, f&quot;{school}&quot;, zdir=&quot;y&quot;, clip_on=False) # chain dim chain_grid = np.linspace(-1, 1, 6) * side chain_one = np.ones_like(chain_grid) * side * 1.1 ax.plot(-chain_one, chain_grid, chain_one, marker=&quot;|&quot;, color=&quot;k&quot;) for chain, pos in enumerate(chain_grid): ax.text(-side*1.1, pos, side*1.15, f&quot;{chain}&quot;, zdir=&quot;y&quot;, va=&quot;bottom&quot;, ha=&quot;center&quot;) ax.text(-side*1.3, 0, side*1.3, &quot;Chain&quot;, zdir=(1, 1, .2), va=&quot;bottom&quot;, ha=&quot;center&quot;) # draw dim draw_grid = np.linspace(-1, 1, 50) * side draw_one = np.ones_like(draw_grid) * side * 1.1 draws = np.arange(0, 50, 10) ax.plot(-draw_one, -draw_one, draw_grid, marker=&quot;_&quot;, color=&quot;k&quot;) for draw, pos in zip(draws, draw_grid[draws]): ax.text(-side*1.1, -side*1.15, pos, f&quot;{draw}&quot;, zdir=None, ha=&quot;right&quot;) ax.text2D(0.12, 0.4, &quot;Draw&quot;, transform=ax.transAxes, rotation=&quot;vertical&quot;) ################ ### tau ### ################ ax.plot_surface(4+one_2d,X,Y, color=&quot;C1&quot;, zorder=1) ax.text2D(0.525, 0.1, r&quot;$ tau$&quot;, transform=ax.transAxes) ################ ### mu ### ################ ax.plot_surface(8+one_2d,X,Y, color=&quot;C2&quot;, zorder=1) ax.text2D(0.66, 0.08, r&quot;$ mu$&quot;, transform=ax.transAxes) ################ ### grids ### ################ grid = np.linspace(-1, 1, 10) * side grid_one = np.ones(10) lw = .3; alpha = .7 for chain_pos in chain_grid: ax.plot(grid, grid_one * chain_pos, grid_one * side, color=&quot;k&quot;, alpha=alpha, zorder=3, lw=lw) ax.plot(grid_one * side, grid_one * chain_pos, grid, color=&quot;k&quot;, alpha=alpha, zorder=5, lw=lw) ax.plot(grid_one * side + 4, grid_one * chain_pos, grid, color=&quot;k&quot;, alpha=alpha, zorder=6, lw=lw) ax.plot(grid_one * side + 8, grid_one * chain_pos, grid, color=&quot;k&quot;, alpha=alpha, zorder=7, lw=lw) for draw_pos in draw_grid: ax.plot(grid, -grid_one * side, grid_one * draw_pos, color=&quot;k&quot;, alpha=alpha, zorder=4, lw=lw) ax.plot(grid_one * side, grid, grid_one * draw_pos, color=&quot;k&quot;, alpha=alpha, zorder=5, lw=lw) ax.plot(grid_one * side + 4, grid, grid_one * draw_pos, color=&quot;k&quot;, alpha=alpha, zorder=6, lw=lw) ax.plot(grid_one * side + 8, grid, grid_one * draw_pos, color=&quot;k&quot;, alpha=alpha, zorder=7, lw=lw) for school_pos in school_grid: ax.plot(grid_one * school_pos, grid, grid_one * side, color=&quot;k&quot;, alpha=alpha, zorder=4, lw=lw) ax.plot(grid_one * school_pos, -grid_one * side, grid, color=&quot;k&quot;, alpha=alpha, zorder=4, lw=lw) ax.axis(&quot;off&quot;); ax.view_init(azim=-69) #fig.savefig(&quot;labeled_arys.png&quot;, dpi=300) . . Package versions used to generate this post: . pymc3 3.9.3 matplotlib 3.3.1 pandas 1.1.1 numpy 1.19.1 arviz 0.10.0 xarray 0.16.1 last updated: Thu Sep 24 2020 CPython 3.6.9 IPython 7.16.1 watermark 2.0.2 . . Comments are not enabled for this post, to inquiry further about the contents of the post, ask on PyMC Discourse. Feel free to tag me at @OriolAbril .",
            "url": "https://moraouf.github.io/MoSpace/python/arviz/pymc3/xarray/2020/09/22/pymc3-arviz.html",
            "relUrl": "/python/arviz/pymc3/xarray/2020/09/22/pymc3-arviz.html",
            "date": " • Sep 22, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Creating a blog",
            "content": "A blog is born . These days, there are countless blogging alternatives covering an extremely wide range of needs, from writing everything in .docx format to customizing every minor nit with CSS and HTML. It is not difficult to get lost in this sea of blogging alternatives and end up being unable to choose one of the alternatives and set the blog up. I already experimented with a blog during my Google Summer of Code internship and I have now started this new blog. Thus, being this my second blog, I am far from an expert in blogging platforms. . I cannot write a complete guide for prospective bloggers, however, I still feel like writing about how I created and configured this blog can be useful to anyone who is considering creating a blog somewhat similar to this one. As you may have guessed already from other pages in the blog, I highly value understanding what I do and being able to modify (or at least see) its inner workings. . The first section is an overview of the technologies and libraries used in this blog. Afterwards there are 3 sections covering the base of blogging: writing the content, building the blog and hosting it. These cover the base elements needed to get the blog running. Eventually, there is one section on more advanced configuration and on the features I value the most. . Overview . The content for this blog is written using either jupyter notebooks, markdown or docx files. Posts are then converted thanks to fastpages to markdown and html files that Jekyll can use to build the static website. Every time a new commit is pushed to GitHub, GitHub Actions are used to automatically build the website and push it to the gh-pages branch. The website stored in the gh-pages branch is then hosted on GitHub pages. Here is the diagram of the workflow from the fastpages website: . . Writing content . Writing the content using Markdown is one of the most common alternatives for writing blog posts. Writing in Markdown has many advantages and produces files which are readable both rendered and without rendering. To write code tutorials however, they are not too convenient as we need to manually execute each cell and add the output of the code to the post. . If we use Jupyter notebooks instead, we can keep the code, its output and the explanations in the same executable file. This is really convenient as I can easily rerun the notebooks whenever there has been a significant change in the libraries used and keep the tutorials up to date. The main drawback of writing posts in Jupyter notebooks is having to convert the notebooks to markdown or html so the post can be added to the blog. I am taking advantage of fastpages to both take care of the conversion and to automate the process. . Eventually, I only have to write my posts in either markdown or Jupyter notebook files and push them to GitHub. This is crucial to me as it makes creating new posts be only about writing! I don’t have to take care about conversion. . Building the blog . This blog is built using Jekyll, an open source static website generator. Roughly speaking, it takes a configuration file and the website content as markdown or html files and generates a static website where the theme has been filled with the content, pages and blog posts. The configuration file, _config.yml contains information such as the theme or the markdown parser to be used. . fastpages uses the default Jekyll theme: minima. Even though minima has many features and everything works straight away with fastpages, I wanted to use a different theme, to give another look to the website and make it more attractive to readers. I therefore tried changing the theme specified in _config.yml for several new themes I found more visually attractive. I found basically basic (see screenshots below) and massively to work quite well right out of the box. Most of the themes did not work at all due to incompatibilities with fastpages. After comparing both themes for a while, I decided to go with massively. I therefore tweaked the theme to fix the minor incompatibilities it presented with fastpages. This is still a work in progress, not everything that works with fastpages+minima works with massively yet. . Some of the tweaks will be detailed in the last two sections of the post, but most of them will not be covered here. If you are interested in any of the two themes and their compatibility with fastpages please reach out in an issue, or what’s nearly the same (thanks utterances), comment below. . Hosting a static website . As explained in the overview, this website is hosted by GitHub pages. Even though this was initially the one that seemed more difficult to me, I actually found this step simpler than the other two. Basically GitHub Pages takes care of everything. . The main friction I encountered while using GitHub pages was the .nojekyll file. There are two main ways of interacting with GitHub pages. The first alternative is to push to gh-pages a directory containing a _config.yml file with the Jekyll configuration and the content in Markdown and HTML format. GitHub then builds the site for you using Jekyll. The second alternative is to build the site and push the result to gh-pages branch together with a .nojekyll file. The .nojekyll file tells GitHub to not build the site with Jekyll and host directly the contents of the branch instead. . In this particular case, we are using GitHub Actions from fastpages to convert the posts and build the site using Jekyll, so we are actually using the second alternative. Luckily, thanks to fastpages, these two alternatives do not affect the writing process at all. . The best of many worlds . So far we have described how to create and maintain a plain blog, the main difference with forking the minima theme and writing a blog in markdown is the ability to write posts in Jupyter notebooks. I would like this blog to be more than that. This blog combines features from fastpages, massively and basically basic. My aim was to hand pick the features of each source that were a better fit to my idea and needs for this website. . The blog is mobile friendly thanks to the massively theme plus some extra tweaks from Basically Basic theme. I also borrowed the text size scaling from Basically Basic, now whatever the screen size, the text should always be readable. . I customized the favicon too. As you can see, it is neither the one of fastpages nor of massively theme. It is a custom image of a MATLAB-style waterfall plot of a 2d group MOM prior. You can see the regular scale image used to generate the favicon below, more details on what is a group MOM prior will come in a future post. This was actually simpler than I expected, there are converters online to generate favicons from regular images, and then saving the favicon as images/favicon.ico is enough for everything to work. . . fastpages also has support for many other awesome features such as google analytics, comments or SEO tag management. For now, I decided to use google analytics and comments powered by utterances but remove the SEO related code. I may add it again at some point after I better understand how they work. . Notebook shields . Many of my posts will be tutorials written as Jupyter notebooks. Being able to include the content from ipynb files to the blog is awesome, but why stop here? fastpages allows to add 3 shields (shown below) to notebook posts so that the notebook can be opened in GitHub, Google Colab or Binder. Binder! :heart: . Binder generates a container where the notebook can be executed interactively without requiring any local installation. This allows readers to run the code included in the tutorial while they read it with little to no effort, just click on the binder shield. Binder cannot work straight out of the box however, to create the container it needs to know what should be installed. I have used an environment.yml to install the required Python libraries with Conda and a JuliaProject.toml for the Julia libraries. I may add also some R dependencies too. Guidance on specifying requirements for Binder can be found in its docs. . Tag Archive . The tag archive, similarly to the index of a reference book can be an invaluable help to readers in navigating the website and finding posts relevant to their interests. fastpages already includes a tag archive page unlike the massively theme, however, I think its single list formatting does not scale well with the number of posts and different tags. The table format from Basically Basic was much more attractive to me, so I combined the tag archive page from BB theme with the square layout of massively. I also removed the post image to get a more compact look. . Colour schemes and syntax highlighting . Choosing a colour scheme for our code editors can be a very personal choice influenced by many different reasons. When configuring our code editor, we can decide whatever we want and choose to completely ignore everyone else in the entire world. With websites and other public resources however this is not a choice. Websites should be careful with their colour themes to be accessible to people with colour vision deficiency. One clear example of a bad practice on this is GitHub symbols of open and closed pull requests. The image below uses the Mozilla add-on Let’s get colour blind. to simulate how someone with Deuteranomaly sees a list of GitHub pull requests. . I used this same add-on to make sure everything could be seen without too much effort and did a couple of changes to the fastpages-dracula pygments theme for syntax highlighting. I also tried high contrast colour schemes so feel free to contact me if you were to need help modifying the colour scheme of your website. The plots in my post use the arviz-darkgrid theme whose palette is colourblind friendly, so I have not modified them. In the future I’ll try to be more careful and try to not rely only on colour to distinguish lines in plots. . Social Media links and serch icon . Thanks to fontawesome I have been able to add links to GitHub and Twitter profiles and to the blog’s Atom feed: Subscribe in the navigation bar. Moreover, there is also a link to the search page. Search is powered by Lunr via fastpages. . All these social media links are also in the copyright notice found in the website footer, where thanks to academicons, the links to my ORCID and Google Scholar profiles are also available. Fontawsome icons worked out of the box with all the 3 themes I tinkered with, while academicons was not supported by any of them and had to be added manually following the instructions on their website. .",
            "url": "https://moraouf.github.io/MoSpace/post/jekyll/2020/07/10/blog.html",
            "relUrl": "/post/jekyll/2020/07/10/blog.html",
            "date": " • Jul 10, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "ArviZ in depth: plot_trace",
            "content": "Introduction . plot_trace is one of the most common plots to assess the convergence of MCMC runs, therefore, it is also one of the most used ArviZ functions. plot_trace has a lot of parameters that allow creating highly customizable plots, but they may not be straightforward to use. There are many reasons that can explain this convolutedness of the arguments and their format, there is no clear culprit: ArviZ has to integrate with several libraries such as xarray and matplotlib which provide amazing features and customization power, and we&#39;d like to allow ArviZ users to access all these features. However, we also aim to keep ArviZ usage simple and with sensible defaults; plot_xyz(idata) should generate acceptable results in most situations. . This post aims to be an extension to the API section on plot_trace, focusing mostly on arguments where examples may be lacking and arguments that appear often in questions posted to ArviZ issues. . Therefore, the most common arguments such as var_names will not be covered, and for arguments that I do not remeber appearing in issues or generating confusion only some examples will be shown without an in depth description. . import arviz as az import matplotlib.pyplot as plt import numpy as np import xarray as xr # html render is not correctly rendered in blog, # comment the line below if in jupyter xr.set_options(display_style=&quot;text&quot;) rng = np.random.default_rng() . az.style.use(&quot;arviz-darkgrid&quot;) . idata_centered = az.load_arviz_data(&quot;centered_eight&quot;) idata = az.load_arviz_data(&quot;rugby&quot;) . The kind argument . az.plot_trace generates two columns. The left one calls plot_dist to plot KDE/Histogram of the data, and the right column can contain either the trace itself (which gives the name to the plot) or a rank plot for which two visualizations are available. Rank plots are an alternative to trace plots, see https://arxiv.org/abs/1903.08008 for more details. . fig, axes = plt.subplots(3,2, figsize=(12,6)) for i, kind in enumerate((&quot;trace&quot;, &quot;rank_bars&quot;, &quot;rank_vlines&quot;)): az.plot_trace(idata, var_names=&quot;home&quot;, kind=kind, ax=axes[i,:]); fig.tight_layout() . /home/oriol/venvs/arviz-dev/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: This figure was using constrained_layout==True, but that is incompatible with subplots_adjust and or tight_layout: setting constrained_layout==False. after removing the cwd from sys.path. . The divergences argument . If present, divergences are indicated as a black rugplot in both columns of the trace plot. By default they are placed at the bottom of the plot, but they can be placed at the top or hidden. . az.plot_trace(idata_centered, var_names=&quot;tau&quot;); . az.plot_trace(idata_centered, var_names=&quot;tau&quot;, divergences=None); . The rug argument . rug adds a rug plot with the posterior samples at the bottom of the distribution plot, there are no changes in the trace plot column. . ax = az.plot_trace(idata, var_names=&quot;home&quot;, rug=True, rug_kwargs={&quot;alpha&quot;: .4}) . But what about having both rug and divergences at the same time? Fear not, ArviZ automatically modifies the default for divergences from bottom to top to prevent rug and divergences from overlapping: . az.plot_trace(idata_centered, var_names=&quot;mu&quot;, rug=True); . The lines argument . The description about lines in plot_trace&#39;s docstring is the following: . lines : list of tuple of (str, dict, array_like), optional . List of (var_name, {‘coord’: selection}, [line, positions]) to be overplotted as vertical lines on the density and horizontal lines on the trace. . It is possible that the first thought after reading this line is similar to &quot;What is with this weird format?&quot; Well, this format is actually the stardard way ArviZ uses to iterate over xarray.Dataset objects because it contains all the info about the variable and the selected coordinates as well as the values themselves. The main helper function that handles this is arviz.plots.plot_utils.xarray_var_iter. . This section will be a little different from the other ones, and will focus on boosting plot_trace capabilities with internal ArviZ functions. You may want to skip to the section altogether of go straigh to the end. . Let&#39;s see what xarray_var_iter does with a simple dataset. We will create a dataset with two variables: a will be a 2x3 matrix and b will be a scalar. In addition, the dimensions of a will be labeled. . ds = xr.Dataset({ &quot;a&quot;: ((&quot;pos&quot;, &quot;direction&quot;), rng.normal(size=(2,3))), &quot;b&quot;: 12, &quot;pos&quot;: [&quot;top&quot;, &quot;bottom&quot;], &quot;direction&quot;: [&quot;x&quot;, &quot;y&quot;, &quot;z&quot;] }) ds . &lt;xarray.Dataset&gt; Dimensions: (direction: 3, pos: 2) Coordinates: * pos (pos) &lt;U6 &amp;#x27;top&amp;#x27; &amp;#x27;bottom&amp;#x27; * direction (direction) &lt;U1 &amp;#x27;x&amp;#x27; &amp;#x27;y&amp;#x27; &amp;#x27;z&amp;#x27; Data variables: a (pos, direction) float64 -0.5306 0.8029 0.7965 ... 0.4623 -0.128 b int64 12 . from arviz.plots.plot_utils import xarray_var_iter for var_name, sel, values in xarray_var_iter(ds): print(var_name, sel, values) . a {&#39;pos&#39;: &#39;top&#39;, &#39;direction&#39;: &#39;x&#39;} -0.5306128314326483 a {&#39;pos&#39;: &#39;top&#39;, &#39;direction&#39;: &#39;y&#39;} 0.8029249611338745 a {&#39;pos&#39;: &#39;top&#39;, &#39;direction&#39;: &#39;z&#39;} 0.7965222104405889 a {&#39;pos&#39;: &#39;bottom&#39;, &#39;direction&#39;: &#39;x&#39;} -1.4255055469706215 a {&#39;pos&#39;: &#39;bottom&#39;, &#39;direction&#39;: &#39;y&#39;} 0.4622636712711883 a {&#39;pos&#39;: &#39;bottom&#39;, &#39;direction&#39;: &#39;z&#39;} -0.12804707435886095 b {} 12 . xarray_var_iter has iterated over every single scalar value without loosing track of where did every value come from. We can also modify the behaviour to skip some dimensions (i.e. in ArviZ we generally iterate over data dimensions and skip chain and draw dims). . for var_name, sel, values in xarray_var_iter(ds, skip_dims={&quot;direction&quot;}): print(var_name, sel, values) . a {&#39;pos&#39;: &#39;top&#39;} [-0.53061283 0.80292496 0.79652221] a {&#39;pos&#39;: &#39;bottom&#39;} [-1.42550555 0.46226367 -0.12804707] b {} 12 . Now that we know about xarray_var_iter and what it does, we can use it to generate a list in the required format directly from xarray objects. Let&#39;s say for example we were interested in plotting the mean as a line in the trace plot: . var_names = [&quot;home&quot;, &quot;atts&quot;] lines = list(xarray_var_iter(idata.posterior[var_names].mean(dim=(&quot;chain&quot;, &quot;draw&quot;)))) az.plot_trace(idata, var_names=var_names, lines=lines); . And what about quantile lines? Lets plot the 10% and 90% quantile lines but only for defs variable: . . Note: This same approach can also be used with az.hdi skipping hdi dimension . var_names = [&quot;home&quot;, &quot;defs&quot;] quantile_ds = idata.posterior[[&quot;defs&quot;]].quantile((.1, .9), dim=(&quot;chain&quot;, &quot;draw&quot;)) lines = list(xarray_var_iter(quantile_ds, skip_dims={&quot;quantile&quot;})) az.plot_trace(idata, var_names=var_names, lines=lines); . Aggregation kwargs . This section is dedicated to 5 different kwargs, closely related to each other: compact+compact_prop, combined+chain_prop and legend. If we focus on the distribution plots of the left column, we may want to aggregate data along 2 possible dimensions, chains or variable dimension(s) -- school dimension in centered_eight data, team dimension in rugby data... As aggragation or not along these 2 possible dimensions is independent, we end up with 4 possibilities. . In az.plot_trace, the argument combined governs the aggregation of all chains into a single plot (has no effect in trace, only in distributions), and compact governs the aggregation of the variable dimension(s). In order to be able to distinguish each single line after some aggregation has taken place, a legend argument is also available to show the legend with the data labels. chain_prop and compact_prop allow customization of the aesthetics mapping. . We&#39;ll now cover all 4 possibilities to showcase all supported cases and explore related customizations. . combined=False and compact=False . The default behaviour of plot_trace is to perform no aggregation at all. In this case therefore, all subplots will have exactly one line per chain in the posterior. In this chain only setting, the default mapping is to use color to distinguish chains: . az.plot_trace(idata, var_names=[&quot;home&quot;, &quot;defs&quot;], legend=True); . combined=True and compact=False . Chains are aggregated into a single quantity if possible. Therefore, distribution column will have one line per subplot due to the aggregation but the trace column will be the same as in the previous section. This is also a chain only setting, the default mapping is to use color to distinguish chains. However, we&#39;ll use this example to show usage of chain_prop to map the chain to the linewidth: . chain_prop = {&quot;linewidth&quot;: (.5, 1, 2, 3)} az.plot_trace( idata, var_names=[&quot;home&quot;, &quot;defs&quot;], combined=True, chain_prop=chain_prop, compact=False, legend=True ); . combined=False and compact=True . You are probably tired already from scrolling down and we have only 6 teams! Imagine having a variable with a dimension of length 100 or more :scream: . In these cases, it may be more convenient to analyze a compact version of the trace plot: . az.plot_trace(idata, var_names=[&quot;home&quot;, &quot;defs&quot;], combined=False, compact=True, legend=True); . The first two things that jump to the eye are that ArviZ has drastically modified the default aesthetic of the plot and that the plot fits now comfortable in a single screen, bye bye scrolling :wave: . We can also see that legend=True has included multiple legends to the figure. The chain legend is always included in the top right trace plot, and the plots in the distribution column contain a legend if necessary. . combined=True and compact=True . To reduce even more the clutter of lines in the trace plot, we can also combine chains. Moreover, the linestyle -&gt; chain mapping can be distracting, especially if we don&#39;t care too much about distinguishing the chains between them. Like we did before, we will use chain_prop to control this. . az.plot_trace(idata, var_names=[&quot;home&quot;, &quot;defs&quot;], combined=True, chain_prop={&quot;ls&quot;: &quot;-&quot;}, compact=True); . Finally, we will explore alternative usage options for chain_prop and compact_prop. In the two previous examples we have used a 2 element tuple where the second position of the tuple contained the properties to use. Another alternative is to pass a string present in plt.rcParams[&quot;axes.prop_cycle&quot;], which in our case is color only. . az.plot_trace( idata, var_names=[&quot;home&quot;, &quot;defs&quot;], combined=True, chain_prop=&quot;color&quot;, compact=True, compact_prop={&quot;lw&quot;: np.linspace(.5, 3, 6)} ); . Summing it all up . Now that we have covered most arguments, let&#39;s put everything to practice. Try to generate a trace plot following the instructions below: . Show variables home, defs and atts showing only Scotland, Ireland, Italy, Wales coordinates. | For defs variable, plot lines showing the 70% HDI. | Map chains to the following colors: &#39;C0&#39;, &#39;C1&#39;, &#39;C2&#39;, &#39;C3&#39;, &quot;xkcd:purple blue&quot; | Map team dimension to both linestyle (solid and dashed) and linewidth | . #collapse-hide coords = {&quot;team&quot;: [&quot;Scotland&quot;, &quot;Ireland&quot;, &quot;Italy&quot;, &quot;Wales&quot;]} quantile_ds = az.hdi(idata, var_names=&quot;defs&quot;, coords=coords, hdi_prob=.7) lines = list(xarray_var_iter(quantile_ds, skip_dims={&quot;hdi&quot;, &quot;team&quot;})) chain_prop = {&quot;color&quot;: [&#39;C0&#39;, &#39;C1&#39;, &#39;C2&#39;, &#39;C3&#39;, &quot;xkcd:purple blue&quot;]} az.plot_trace( idata, var_names=[&quot;home&quot;, &quot;defs&quot;, &quot;atts&quot;], combined=True, chain_prop=chain_prop, compact=True, compact_prop={&quot;lw&quot;: np.linspace(.5, 3, 6), &quot;ls&quot;: (&quot;-&quot;, &quot;--&quot;)}, lines=lines, coords=coords ); . . Package versions used to generate this post: . numpy 1.19.0 arviz 0.9.0 xarray 0.15.1 last updated: Mon Jun 29 2020 CPython 3.6.9 IPython 7.15.0 watermark 2.0.2 . . Comments are not enabled for the blog, to inquiry further about the contents of the post, ask on ArviZ Issues or PyMC Discourse .",
            "url": "https://moraouf.github.io/MoSpace/python/arviz/matplotlib/2020/06/20/plot-trace.html",
            "relUrl": "/python/arviz/matplotlib/2020/06/20/plot-trace.html",
            "date": " • Jun 20, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "ArviZ customization with rcParams",
            "content": "About . ArviZ not only builds on top of matplotlib&#39;s rcParams but also adds its own rcParams instance to handle specific settings. This post will only graze matplotlib&#39;s rcParams, which are already detailed in matplotlib&#39;s docs; it will dive into specific ArviZ rcParams. . Introduction . Paraphrasing the description on rcParams in the documentation of matplotlib: . ArviZ uses arvizrc configuration files to customize all kinds of properties, which we call rcParams. You can control the defaults of many properties in ArviZ:data loading mode (lazy or eager), automatically showing generated plots, the default information criteria and so on. There are several ways of modifying arviz.rcParams instance, each of them targeted to specific needs. . import arviz as az import matplotlib.pyplot as plt idata = az.load_arviz_data(&quot;centered_eight&quot;) . Customizing ArviZ . arvizrc file . To define default values on a per user or per project basis, arvizrc file should be used. When imported, ArviZ search for an arvizrc file in several locations sorted below by priority: . $PWD/arvizrc | $ARVIZ_DATA/arvizrc | On Linux, $XDG_CONFIG_HOME/arviz/arvizrc (if $XDG_CONFIG_HOME is defined) | or $HOME/.config/arviz/arvizrc (if $XDG_CONFIG_HOME is not defined) | . | On other platforms, $HOME/.arviz/arvizrc if $HOME is defined | . | . Once one of these files is found, ArviZ stops looking and loads its configuration. If none of them are present, the values hardcoded in ArviZ codebase are used. The file used to set the default values in ArviZ can be obtained with the following command: . import arviz as az print(az.rcparams.get_arviz_rcfile()) . None . ArviZ has loaded a file used to set defaults on a per user basis. Unless I use a different rc file in the current directory or modify rcParams as explained above, this configuration will be automatically used every time ArviZ is imported. . This can be really useful to define the favourite backend or information criterion, written once in the rc file and ArviZ automatically uses the desired values. . . Important: You should not rely on ArviZ defaults being always the same. . ArviZ strives to encourage best practices and therefore will change the default values whenever a new algorithm is developed to achieve this goal. If you rely on a specific value, you should either use an arvizrc template or set the defaults at the beggining of every script/notebook. . Dynamic rc settings . To set default values on a per file or per project basis, rcParams can also be modified dynamically, either overwritting a specific key: . az.rcParams[&quot;data.load&quot;] = &quot;eager&quot; . Note that rcParams is the instance to be modified, exactly like in matplotlib. Careful with capitalization! . Another option is to define a dictionary with several new defaults and update rcParams all at once. . rc = { &quot;data.load&quot;: &quot;lazy&quot;, &quot;plot.max_subplots&quot;: 30, &quot;stats.ic_scale&quot;: &quot;negative_log&quot;, &quot;plot.matplotlib.constrained_layout&quot;: False } az.rcParams.update(rc) . rc_context . And last but not least, to temporarily use a different set of defaults, ArviZ also has a rc_context function. Its main difference and advantage is that it is a context manager, therefore, all code executed inside the context will use the defaults defined by rc_context but once we exit the context, everything goes back to normal. Let&#39;s generate 3 posterior plots with the same command to show this: . _, axes = plt.subplots(1,3, figsize=(15,4)) az.plot_posterior(idata, var_names=&quot;mu&quot;, ax=axes[0]) with az.rc_context({&quot;plot.point_estimate&quot;: &quot;mode&quot;, &quot;stats.hdi_prob&quot;: 0.7}): az.plot_posterior(idata, var_names=&quot;mu&quot;, ax=axes[1]) az.plot_posterior(idata, var_names=&quot;mu&quot;, ax=axes[2]); . ArviZ default settings . This section will describe ArviZ rcParams as version 0.8.3 (see GitHub for an up to date version). . Data . The rcParams in this section are related to the data module in ArviZ, that is, they are either related to from_xyz converter functions or to InferenceData class. . data.http_protocol : {https, http} . Only the first two example datasets centered_eight and non_centered_eight come as part of ArviZ. All the others are downloaded from figshare the first time and stored locally to help reloading them the next time. We can get the names of the data available by not passing any argument to az.load_arviz_data (you can also get the description of each of them with az.list_datasets): . az.load_arviz_data().keys() . dict_keys([&#39;centered_eight&#39;, &#39;non_centered_eight&#39;, &#39;radon&#39;, &#39;rugby&#39;, &#39;regression1d&#39;, &#39;regression10d&#39;, &#39;classification1d&#39;, &#39;classification10d&#39;]) . Thus, the first time you call az.load_arviz_data(&quot;radon&quot;), ArviZ downloads the dataset using data.http_protocol. The default is set to https but if needed, it can be modified to http. Notice how there is no fallback, if downloading with https fails, there is no second try with http, an error is risen. To use http you have to set the rcParam explicitly. . data.index_origin : {0, 1} . ArviZ integration with Stan and Julia who use 1 based indexing motivate this rcParam. This rcParam is still at an early stage and its implementation is bound to vary, therefore it has no detailed description. . data.load : {lazy, eager} . Even when not using Dask, xarray&#39;s default is to load data lazily into memory when reading from disk. ArviZ&#39;s from_netcdf also uses the same default. That is, ArviZ functions that read data from disk from_netcdf and load_arviz_data do not load the data into memory unless data.load rcParam is set to eager. . Most use cases not only do not require loading data into memory but will also benefit from lazy loading. However, there is one clear exception: when too many files are lazily opened at the same time, xarray ends up crashing with extremely cryptic error messages, these cases require setting data loading to eager mode. One example of such situation is generating ArviZ documentation, we therefore set data.load to eager in sphinx configuration file. . data.metagroups : mapping of {str : list of str} . . Warning: Do not overwrite data.metagroups as things may break, to add custom metagroups add new keys to the dictionary as shown below . One of the current projects in ArviZ is to extend the capabilities of InferenceData. One of the limitations was not allowing its functions and methods to be applied to several groups at the same time. Starting with ArviZ 0.8.0, InferenceData methods take arguments groups and filter_groups to overcome this limitation. These two combined arguments have the same capabilities as var_names+filter_vars in plotting functions: exact matching, like and regex matching like pandas and support for ArviZ ~ negation prefix and one extra feature: metagroups. So what are metagroups? Let&#39;s see . #collapse-hide for metagroup, groups in az.rcParams[&quot;data.metagroups&quot;].items(): print(f&quot;{metagroup}: n {groups} n&quot;) . . posterior_groups: (&#39;posterior&#39;, &#39;posterior_predictive&#39;, &#39;sample_stats&#39;, &#39;log_likelihood&#39;) prior_groups: (&#39;prior&#39;, &#39;prior_predictive&#39;, &#39;sample_stats_prior&#39;) posterior_groups_warmup: (&#39;_warmup_posterior&#39;, &#39;_warmup_posterior_predictive&#39;, &#39;_warmup_sample_stats&#39;) latent_vars: (&#39;posterior&#39;, &#39;prior&#39;) observed_vars: (&#39;posterior_predictive&#39;, &#39;observed_data&#39;, &#39;prior_predictive&#39;) . Imagine the data you passed to the model was rescaled, after converting to InferenceData you have to rescale the data again to its original values, but not only the observations, posterior and prior predictive values too! . Having to apply the rescaling manually to each of the three groups is tedious at best, and creating a variable called observed_vars storing a list with these 3 groups is problematic -- when doing prior checks there is no posterior_predictive group, it&#39;s a highway towards errors at every turn. Metagroups are similar to the variable approach but it&#39;s already there and it applies the function only to present groups. Let&#39;s add a new metagroup and use it to shift our data: . az.rcParams[&quot;data.metagroups&quot;][&quot;sampled&quot;] = ( &#39;posterior&#39;, &#39;posterior_predictive&#39;, &#39;sample_stats&#39;, &#39;log_likelihood&#39;, &#39;prior&#39;, &#39;prior_predictive&#39; ) shifted_idata = idata.map(lambda x: x-7, groups=&quot;sampled&quot;) . data.save_warmup : bool . If True, converter functions will store warmup iterations in the corresponding groups by default. . Note: data.save_warmup does not affect from_netcdf, all groups are always loaded from file . . Plot . General . plot.backend : {matplotlib, bokeh} . Default plotting backend. . plot.max_subplots : int . Maximum number of subplots in a single figure. Adding too many subplots into a figure can be really slow, to the point that it looks like everthing has crashed without any error message. When there are more variables to plot than max_subplots allowed, ArviZ sends a warning and plots at most max_suplots. See for yourselves: . with az.rc_context({&quot;plot.max_subplots&quot;: 3}): az.plot_posterior(idata); . /home/oriol/venvs/arviz-dev/lib/python3.6/site-packages/arviz/plots/plot_utils.py:563: UserWarning: rcParams[&#39;plot.max_subplots&#39;] (3) is smaller than the number of variables to plot (10) in plot_posterior, generating only 3 plots UserWarning, . plot.point_estimate : {mean, median, model, None} . Default point estimate to include in plots like plot_posterior or plot_density. . Bokeh . plot.bokeh.bounds_x_range, plot.bokeh.bounds_y_range : auto, None or tuple of (float, float), default auto . plot.bokeh.figure.dpi : int, default 60 . plot.bokeh.figure.height, plot.bokeh.figure.width : int, default 500 . plot.bokeh.layout.order : str, default default . Select subplot structure for bokeh. One of default, column, row, square, square_trimmed or Ncolumn (Nrow) where N is an integer number of columns (rows), here is one example to generate a subplot grid with 2 columns and the necessary rows to fit all variables. . with az.rc_context({&quot;plot.bokeh.layout.order&quot;: &quot;2column&quot;}): az.plot_ess(idata, backend=&quot;bokeh&quot;) . plot.bokeh.layout.sizing_mode : {fixed, stretch_width, stretch_height, stretch_both, scale_width, scale_height, scale_both} . plot.bokeh.layout.toolbar_location : {above, below, left, right, None} . Location for toolbar on bokeh layouts. None will hide the toolbar. . plot.bokeh.marker : str, default Cross . Default marker for bokeh plots. See bokeh reference on markers for more details. . plot.bokeh.output_backend : {webgl, canvas, svg} . plot.bokeh.show : bool, default True . Show bokeh plot before returning in ArviZ function. . plot.bokeh.tools : str, default reset,pan,box_zoom,wheel_zoom,lasso_select,undo,save,hove . Default tools in bokeh plots. More details on Configuring Plot Tools docs . Matplotlib . Matplotlib already has its own rcParams, which are actually the inspiration for ArviZ rcParams. Therefore, this section is minimalistic. . plot.matplotlib.show : bool, default False . Call plt.show from within ArviZ plotting functions. This generally makes no difference in jupyter like environments, but it can be useful for instance in the IPython terminal when we don&#39;t want to customize the plots genereted by ArviZ by changing titles or labels. . . Stats . stats.hdi_prob : float . Default probability of the calculated HDI intervals. . Important: This probability is completely arbitrary. ArviZ using 0.94 instead of the more common 0.95 aims to emphasize this arbitrary choice. . stats.information_criterion : {loo, waic} . Default information criterion used by compare and plot_elpd . stats.ic_pointwise : bool, default False . Return pointwise values when calling loo or waic. Pointwise values are an intermediate result and therefore setting ic_pointwise to true does not require extra computation. . stats.ic_scale : {log, deviance, negative_log} . Default information criterion scale. See docs on loo or waic for more detail. . . . Tip: Is there any extra rcParam you&#8217;d like to see in ArviZ? Check out arviz-devs/arviz#792, it&#8217;s more than possible you&#8217;ll be able to add it yourself! . Package versions used to generate this post: . arviz 0.9.0 last updated: Mon Jun 29 2020 CPython 3.6.9 IPython 7.15.0 watermark 2.0.2 . . Comments are not enabled for the blog, to inquiry further about the contents of the post, ask on ArviZ Issues. .",
            "url": "https://moraouf.github.io/MoSpace/project/python/arviz/2020/06/19/rcParams.html",
            "relUrl": "/project/python/arviz/2020/06/19/rcParams.html",
            "date": " • Jun 19, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "LOO-PIT tutorial",
            "content": "Introduction . One of the new functionalities I added in ArviZ during my GSoC internship is Leave One Out (LOO) Probability Integral Transform (PIT) marginal posterior predictive checks. You can see two examples of its usage in the example gallery and also some examples in its API section. However, these examples are mainly related to the usage of the functionalities, not so much on the usage of LOO-PIT itself nor its interpretability. . I feel that the LOO-PIT algorithm usage and interpretability needs a short summary with examples showing the most common issues found when checking models with LOO-PIT. This tutorial will tackle this issue: how can LOO-PIT be used for model checking and what does it tell us in a practical manner, so we can see firsthand how wrongly specified models cause LOO-PIT values to differ from a uniform distribution. I have included a short description on what is the algorithm doing, however, for a detailed explanation, see: . Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., and Rubin, D. B. (2013). Bayesian Data Analysis. Chapman &amp; Hall/CRC Press, London, third edition. (p. 152-153) | . We will use LOO-PIT checks along with non-marginal posterior predictive checks as implemented in ArviZ. This will allow to see some differences between the two kinds of posterior predictive checks as well as to provide some intuition to cases where one may be best and cases where both are needed. . Here, we will experiment with LOO-PIT using two different models. First an estimation of the mean and standard deviation of a 1D Gaussian Random Variable, and then a 1D linear regression. Afterwards, we will see how to use LOO-PIT checks with multivariate data using as example a multivariate linear regression. . Background . One of the pilars of Bayesian Statistics is working with the posterior distribution of the parameters instead of using point estimates and errors or confidence intervals. We all know how to obtain this posterior given the likelihood, the prior and the , $p( theta mid y) = p(y mid theta) p( theta) / p(y)$. In addition, in many cases we are also interested in the probability of future observations given the observed data according to our model. This is called posterior predictive, which is calculated integrating out $ theta$: . $$ p(y^* | y) = int p(y^*| theta) p( theta|y) d theta$$ . where $y^*$ is the possible unobserved data and $y$ is the observed data. Therefore, if our model is correct, the observed data and the posterior predictive follow the same probability density function (pdf). In order to check if this holds, it is common to perform posterior predictive checks comparing the posterior predictive to the observed data. This can be done directly, comparing the kernel density estimates (KDE) of the observed data and posterior predictive samples, etc. A KDEs is nothing else than an estimation of the pdf of a random variable given a finite number of samples from this random variable. . Another alternative it to perform LOO-PIT checks, which are a kind of marginal posterior predictive checks. Marginal because we compare each observation only with the corresponding posterior predictive samples instead of combining all observations and all posterior predictive samples. As the name indicates, it combines two different concepts, Leave-One-Out Cross-Validation and Probability Integral Transform. . Probability Integral Transform . Probability Integral Transform stands for the fact that given a random variable $X$, the random variable $Y = F_X(X) = P(x leq X)$ is a uniform random variable if the transformation $F_X$ is the Cumulative Density Function (CDF) of the original random variable $X$. . If instead of $F_X$ we have $n$ samples from $X$, $ {x_1, dots, x_n }$, we can use them to estimate $ hat{F_X}$ and apply it to future $X$ samples ${x^*}$. In this case, $ hat{F_X}(x^*)$ will be approximately a uniform random variable, converging to an exact uniform variable as $n$ tends to infinity. . The mathematical demonstration can be found on wikipedia itself just googling it. However here, instead of reproducing it I will try to outline the intuition behind this fact. One way to imagine it is with posterior samples from an MCMC run. If we have enough samples, the probability of a new sample falling between the two smallest values will be the same than the probability of a new sample falling inside the two values closest to the median. . This is because around the probability around the smallest values will be lower, but they will be further apart, whereas the probability around the median will be larger but they will be extremely close. These two effect compensate each other and the probability is indeed the same. Thus, the probability is constant independently of the square the new sample would fall in, which is only compatible with a uniform distribution. . Leave-One-Out Cross-Validation . Cross-Validation is one way to try to solve the problem with all the future data I have been mentioning so far. We do not have this future data, so how are we supposed to make calculations with it? Cross-Validation solves this problem by dividing the observed data in $K$ subsets, excluding one subset from the data used to fit the model (so it is data unknown to the model, aka future data) and then using this excluded subset as future data. In general, to get better results, this process is preformed $K$ times, excluding one different subset every time. . LOO-CV is one particular case where the number of subsets is equal to the number of observations so that each iteration only one observation is excluded. That is, we fit the model one time per observation excluding only this one observation. . LOO-PIT . LOO-PIT checks consist on checking the PIT using LOO-CV. That is, fit the model on all data but observation $y_i$ (we will refer to this leave one out subset as $y_{-i}$), use this model to estimate the cumulative density function of the posterior predictive and calculate the PIT, $P(y_i &lt; y^* mid y_{-i}) = int_{- infty}^{y_i} p(y^* mid y_{-i}) dy^*$, of each observation. Then, the KDE of all LOO-PIT values is estimated to see whether or not it is compatible with the LOO-PIT values being draws from a uniform variable. . Data generation . import pymc3 as pm import numpy as np import arviz as az import matplotlib.pyplot as plt import theano.tensor as tt import scipy.stats as stats np.random.seed(7) . az.style.use(&#39;arviz-darkgrid&#39;) . def plot_ppc_loopit(idata, title): fig = plt.figure(figsize=(12,9)) ax_ppc = fig.add_subplot(211) ax1 = fig.add_subplot(223); ax2 = fig.add_subplot(224) az.plot_ppc(idata, ax=ax_ppc); for ax, ecdf in zip([ax1, ax2], (False, True)): az.plot_loo_pit(idata, y=&quot;obs&quot;, ecdf=ecdf, ax=ax); ax_ppc.set_title(title) ax_ppc.set_xlabel(&quot;&quot;) return np.array([ax_ppc, ax1, ax2]) . N_obs = 170 mu_normal = -2 sd_normal = 3 data_normal = np.random.normal(loc=mu_normal, scale=sd_normal, size=N_obs) a0_lr, a1_lr = 5, -2.3 sd_lr = 1.4 data_x_regression = np.linspace(0, 10, N_obs) data_y_regression = np.random.normal(loc=a1_lr*data_x_regression+a0_lr, scale=sd_lr) . coords_normal = {&quot;obs&quot;: [&quot;observation&quot;], &quot;log_likelihood&quot;: [&quot;observation&quot;]} dims_normal = {&quot;observation&quot;: range(N_obs)} coords_regression = {&quot;y&quot;: [&quot;time&quot;], &quot;log_likelihood&quot;: [&quot;time&quot;]} dims_regression = {&quot;time&quot;: data_x_regression} . We will now plot the two datsets generated, to give graphical an idea of the data we are working with. . fig, axes = plt.subplots(1, 2, figsize=(11,5)) textsize = plt.rcParams[&quot;axes.labelsize&quot;] az.plot_dist(data_normal, rug=True, ax=axes[0], rug_kwargs={&quot;space&quot;: 0}, textsize=textsize); axes[1].plot(data_x_regression, data_y_regression, &quot;.&quot;); axes[1].tick_params(labelsize=textsize) axes[0].set_title(&quot;Gaussian random variable draws&quot;) axes[1].set_title(&quot;Data for linear regression&quot;) fig.tight_layout() . Unidimensional Gaussian variable . We will start with a model that correctly fits with the data, to show how should both checks look like. Afterwards, we will see cases were these checks deviate from this ideal case and give some hints on how to interpret these deviations. . with pm.Model() as model: # Define priors mu = pm.Normal(&quot;mu&quot;, mu=0, sd=10) sd = pm.HalfNormal(&quot;sd&quot;, sd=10) # Define likelihood likelihood = pm.Normal(&quot;obs&quot;, mu=mu, sd=sd, observed=data_normal) # Inference! trace = pm.sample() # draw posterior samples using NUTS sampling prior = pm.sample_prior_predictive() posterior_predictive = pm.sample_posterior_predictive(trace) idata_normal = az.from_pymc3( trace=trace, prior=prior, posterior_predictive=posterior_predictive, coords=coords_normal, dims=dims_normal, ) . Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (4 chains in 4 jobs) NUTS: [sd, mu] Sampling 4 chains, 0 divergences: 100%|██████████| 4000/4000 [00:00&lt;00:00, 4875.74draws/s] 100%|██████████| 2000/2000 [00:01&lt;00:00, 1748.53it/s] . plot_ppc_loopit(idata_normal, &quot;Gaussian: Calibrated model&quot;); . To begin with, it can be seen that the observed KDE is similar to the overlayed posterior predictive KDEs. The same happens with the LOO-PIT values; the LOO-PIT KDE is similar to the overlayed uniform KDEs. Thus, in this first example, similar information can be obteined from their interpretation. . Overdispersion signs . We will now move to one common mismatch between the model and the observed data. We will perform the same fit as the previous example but fixing the standard deviation of the normal random variable. This is actually not an unrealistic case, as in many cases where the instrument used to measure gives error data in addition to the measure, this error is used to fix the standard deviation. . These two examples show how the LOO-PIT looks like for overdispersed models (i.e. the error is assumed to be larger than what it actually is) and for underdispersed models (i.e. the error is assumed to be smaller than what it really is). . with pm.Model() as model: mu = pm.Normal(&quot;mu&quot;, mu=0, sd=10) likelihood = pm.Normal(&quot;obs&quot;, mu=mu, sd=1.5 * sd_normal, observed=data_normal) trace = pm.sample() prior = pm.sample_prior_predictive() posterior_predictive = pm.sample_posterior_predictive(trace) idata_normal_overdispersed = az.from_pymc3( trace=trace, prior=prior, posterior_predictive=posterior_predictive, coords=coords_normal, dims=dims_normal, ) . Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (4 chains in 4 jobs) NUTS: [mu] Sampling 4 chains, 0 divergences: 100%|██████████| 4000/4000 [00:00&lt;00:00, 6269.06draws/s] 100%|██████████| 2000/2000 [00:00&lt;00:00, 3299.82it/s] . plot_ppc_loopit(idata_normal_overdispersed, &quot;Gaussian: Overdispersed model&quot;); . In this example of overdispersed model, we can see that the posterior predictive checks show that the observed KDE is narrower than most of the posterior predictive KDEs and narrower than the mean KDE of the posterior predictive samples. However, there are still some posterior predictive samples whose KDEs are similar to the observed KDE. In the LOO-PIT check though, there is no room for confursion. The LOO-PIT KDE is not uniform between 0 and 1, its range is much quite more limited than the uniform counterparts. Moreover, the difference between the Empirical Cumulative Density Function (ECDF) and the ideal uniform CDF lays outside the envelope most of the time. . Underdispersion signs . with pm.Model() as model: mu = pm.Normal(&quot;mu&quot;, mu=0, sd=10) likelihood = pm.Normal(&quot;obs&quot;, mu=mu, sd=.75 * sd_normal, observed=data_normal) trace = pm.sample() prior = pm.sample_prior_predictive() posterior_predictive = pm.sample_posterior_predictive(trace) idata_normal_underdispersed = az.from_pymc3( trace=trace, prior=prior, posterior_predictive=posterior_predictive, coords=coords_normal, dims=dims_normal, ) . Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (4 chains in 4 jobs) NUTS: [mu] Sampling 4 chains, 0 divergences: 100%|██████████| 4000/4000 [00:00&lt;00:00, 6293.68draws/s] 100%|██████████| 2000/2000 [00:00&lt;00:00, 3354.84it/s] . plot_ppc_loopit(idata_normal_underdispersed, &quot;Gaussian: Underdispersed model&quot;); . Here, the differences are similar to the overdispersed case, modifying overdispersed by underdispersed and inverting the shapes. . Bias signs . In addition, LOO-PIT checks also show signs of model bias, as shown in the following example: . with pm.Model() as model: sd = pm.HalfNormal(&quot;sd&quot;, sd=10) likelihood = pm.Normal(&quot;obs&quot;, mu=mu_normal - sd_normal / 2, sd=sd, observed=data_normal) trace = pm.sample() prior = pm.sample_prior_predictive() posterior_predictive = pm.sample_posterior_predictive(trace) idata_normal_bias = az.from_pymc3( trace=trace, prior=prior, posterior_predictive=posterior_predictive, coords=coords_normal, dims=dims_normal, ) . Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (4 chains in 4 jobs) NUTS: [sd] Sampling 4 chains, 0 divergences: 100%|██████████| 4000/4000 [00:00&lt;00:00, 4953.10draws/s] 100%|██████████| 2000/2000 [00:01&lt;00:00, 1613.28it/s] . plot_ppc_loopit(idata_normal_bias, &quot;Gaussian: Biased model&quot;); . It is important to note though, that the LOO-PIT itself already indicates the problem with the model: . a convex KDE shape (inverted-U shape or range smaller than 0-1) or an N in the ECDF difference plot is a sign of an overdispersed model | a concave KDE shape (U shape) or an inverted-N ECDF difference is a sign of underdispersion | an asymmetrical KDE (range may also be reduced instead of 0-1) or ECDF difference is a sign for model bias | . In general though, we will probably find a combination of all these cases and it may not be straigthforward to interpretate what is wrong with the model using LOO-PIT or posterior predictive KDE checks. . Linear regression . In the case of a linear regression, the posterior predictive checks direclty do not give us much information, because each datapoint is centered at a different location, so combining them to create a single KDE won&#39;t yield useful results. It is important to note though, that this is not an issue inherent to the posterior predictive checks, and could be solved by rescaling each observation by substracting the mean and divide by the standard deviation along every observation from the posterior predictive. We will also include an example of this kind of transformation in the last example, but there should not be much to worry about as this improvement is on the ArviZ roadmap. . with pm.Model() as model: sigma = pm.HalfNormal(&#39;sigma&#39;, sd=10) a0 = pm.Normal(&quot;a0&quot;, mu=0, sd=20) a1 = pm.Normal(&quot;a1&quot;, mu=0, sd=20) likelihood = pm.Normal(&#39;obs&#39;, mu=a0 + a1 * data_x_regression, sd=sigma, observed=data_y_regression) trace = pm.sample() prior = pm.sample_prior_predictive() posterior_predictive = pm.sample_posterior_predictive(trace) idata_lr = az.from_pymc3( trace=trace, prior=prior, posterior_predictive=posterior_predictive, coords=coords_regression, dims=dims_regression, ) . Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (4 chains in 4 jobs) NUTS: [a1, a0, sigma] Sampling 4 chains, 0 divergences: 100%|██████████| 4000/4000 [00:01&lt;00:00, 2882.58draws/s] The acceptance probability does not match the target. It is 0.9065348582364419, but should be close to 0.8. Try to increase the number of tuning steps. 100%|██████████| 2000/2000 [00:01&lt;00:00, 1221.16it/s] . plot_ppc_loopit(idata_lr, &quot;Linear Regression: Calibrated model&quot;); . Now let&#39;s see how does introducing some small bias modifies the results. . with pm.Model() as model: sigma = pm.HalfNormal(&#39;sigma&#39;, sd=10) a1 = pm.Normal(&quot;a1&quot;, mu=0, sd=20) likelihood = pm.Normal(&#39;obs&#39;, mu=a0_lr + 2 + a1 * data_x_regression, sd=sigma, observed=data_y_regression) trace = pm.sample() prior = pm.sample_prior_predictive() posterior_predictive = pm.sample_posterior_predictive(trace) idata_lr_bias = az.from_pymc3( trace=trace, prior=prior, posterior_predictive=posterior_predictive, coords=coords_regression, dims=dims_regression, ) . Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (4 chains in 4 jobs) NUTS: [a1, sigma] Sampling 4 chains, 0 divergences: 100%|██████████| 4000/4000 [00:00&lt;00:00, 5018.12draws/s] The acceptance probability does not match the target. It is 0.8792817423537712, but should be close to 0.8. Try to increase the number of tuning steps. 100%|██████████| 2000/2000 [00:01&lt;00:00, 1413.29it/s] . plot_ppc_loopit(idata_lr_bias, &quot;Linear Regression: Biased model&quot;); . Now the LOO-PIT check is clearly showing signs of bias in the model, whereas due to the lack of rescaling, no bias is seen in the posterior predictive checks. . Finally, let&#39;s combine some bias with overdispersion, to see how is LOO-PIT modified. Moreover, we will rescale the posterior predictive data to see how would rescaling affect the posterior predictive checks. . with pm.Model() as model: a1 = pm.Normal(&quot;a1&quot;, mu=0, sd=20) likelihood = pm.Normal( &#39;obs&#39;, mu=a0_lr + 2 + a1 * data_x_regression, sd=1.5 * sd_lr, observed=data_y_regression ) # Inference! trace = pm.sample() # draw posterior samples using NUTS sampling prior = pm.sample_prior_predictive() posterior_predictive = pm.sample_posterior_predictive(trace) idata_lr_bias_overdispersed = az.from_pymc3( trace=trace, prior=prior, posterior_predictive=posterior_predictive, coords=coords_regression, dims=dims_regression, ) . Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (4 chains in 4 jobs) NUTS: [a1] Sampling 4 chains, 0 divergences: 100%|██████████| 4000/4000 [00:00&lt;00:00, 6382.96draws/s] 100%|██████████| 2000/2000 [00:00&lt;00:00, 2138.31it/s] . plot_ppc_loopit(idata_lr_bias_overdispersed, &quot;Linear Regression: Biased and oversidpersed model&quot;); . pp_samples = idata_lr_bias_overdispersed.posterior_predictive.obs obs_samples = idata_lr_bias_overdispersed.observed_data.obs pp_means = pp_samples.mean(dim=(&quot;chain&quot;, &quot;draw&quot;)) pp_stds = pp_samples.std(dim=(&quot;chain&quot;, &quot;draw&quot;)) idata_lr_bias_overdispersed.posterior_predictive[&quot;obs_rescaled&quot;] = (pp_samples - pp_means) / pp_stds idata_lr_bias_overdispersed.observed_data[&quot;obs_rescaled&quot;] = (obs_samples - pp_means) / pp_stds . fig, axes = plt.subplots(3, 2, figsize=(12,13), constrained_layout=True) for i, var in enumerate((&quot;obs&quot;, &quot;obs_rescaled&quot;)): for j, ecdf in enumerate((False, True)): az.plot_loo_pit(idata_lr_bias_overdispersed, y=var, ecdf=ecdf, ax=axes[j, i]); az.plot_ppc(idata_lr_bias_overdispersed, ax=axes[2]); fig.suptitle(&quot;Linear Regression: Rescaling effect nBiased and overdispersed model&quot;, fontsize=16); . As you can see, the posterior predictive check for obs_rescaled does indicate overdispersion and bias of the posterior predictive samples, whereas the one for obs does not, following what we were seeing previously. The LOO-PIT checks do not change one bit however. This is actually a property of the LOO-PIT algorithm. As it is comparing the marginal distributions of the posterior predictive and the observed data using the MCMC samples, any monotonous transformation will not modify its value because it won&#39;t modify the order between the samples. Therefore, if the observed data is larger than 36% of the posterior predictive samples, the rescaling we have done does not modify this fact. . . Comments are not enabled for the blog, to inquiry further about the contents of the post, ask on ArviZ Issues or PyMC3 Discourse .",
            "url": "https://moraouf.github.io/MoSpace/python/arviz/pymc3/2019/07/31/loo-pit-tutorial.html",
            "relUrl": "/python/arviz/pymc3/2019/07/31/loo-pit-tutorial.html",
            "date": " • Jul 31, 2019"
        }
        
    
  
    
        ,"post7": {
            "title": "LOO-CV on transformed data",
            "content": "Blog post exploring whether or not LOO-CV can be used to compare models that try to explain some data $y$ with models trying to explain the same data after a transformation $z=f(y)$. Inspired by @tiagocc question on Stan Forums. This post has two sections, the first one is the mathematical derivation of the equations used and their application on a validation example, and the second section is a real example. In addition to the LOO-CV usage examples and explanations, another goal of this notebook is to show and highlight the capabilities of ArviZ. . import pystan import pandas as pd import numpy as np import arviz as az import matplotlib.pyplot as plt . az.style.use(&quot;arviz-darkgrid&quot;) . Mathematical derivation and validation example . In the first example, we will compare two equivalent models: . $y sim text{LogNormal}( mu, sigma)$ | $ log y sim text{Normal}( mu, sigma)$ | Model definition and execution . Define the data and execute the two models . mu = 2 sigma = 1 logy = np.random.normal(loc=mu, scale=sigma, size=30) y = np.exp(logy) # y will then be distributed as lognormal data = {&#39;N&#39;: len(y), &#39;y&#39;: y, &#39;logy&#39;: logy} . #collapse-hide lognormal_code = &quot;&quot;&quot; data { int&lt;lower=0&gt; N; vector[N] y; } parameters { real mu; real&lt;lower=0&gt; sigma; } model { y ~ lognormal(mu, sigma); } generated quantities { vector[N] log_lik; vector[N] y_hat; for (i in 1:N) { log_lik[i] = lognormal_lpdf(y[i] | mu, sigma); y_hat[i] = lognormal_rng(mu, sigma); } } &quot;&quot;&quot; . . sm_lognormal = pystan.StanModel(model_code=lognormal_code) fit_lognormal = sm_lognormal.sampling(data=data, iter=1000, chains=4) . INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_af04dcd0464f65fe0e5bbc595b4eb9d6 NOW. . idata_lognormal = az.from_pystan( posterior=fit_lognormal, posterior_predictive=&#39;y_hat&#39;, observed_data=[&#39;y&#39;], log_likelihood={&#39;y&#39;: &#39;log_lik&#39;}, ) . #collapse-hide normal_on_log_code = &quot;&quot;&quot; data { int&lt;lower=0&gt; N; vector[N] logy; } parameters { real mu; real&lt;lower=0&gt; sigma; } model { logy ~ normal(mu, sigma); } generated quantities { vector[N] log_lik; vector[N] logy_hat; for (i in 1:N) { log_lik[i] = normal_lpdf(logy[i] | mu, sigma); logy_hat[i] = normal_rng(mu, sigma); } } &quot;&quot;&quot; . . sm_normal = pystan.StanModel(model_code=normal_on_log_code) fit_normal = sm_normal.sampling(data=data, iter=1000, chains=4) . INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_acd7c874588f1c862727f931f4dbf916 NOW. . idata_normal = az.from_pystan( posterior=fit_normal, posterior_predictive=&#39;logy_hat&#39;, observed_data=[&#39;logy&#39;], log_likelihood={&#39;logy&#39;: &#39;log_lik&#39;}, ) . Check model convergence. Use az.summary to in one view that the effective sample size (ESS) is large enough and $ hat{R}$ is close to one. . az.summary(idata_lognormal) . mean sd hdi_3% hdi_97% mcse_mean mcse_sd ess_mean ess_sd ess_bulk ess_tail r_hat . mu 1.857 | 0.210 | 1.485 | 2.259 | 0.005 | 0.004 | 1533.0 | 1526.0 | 1535.0 | 1483.0 | 1.0 | . sigma 1.145 | 0.164 | 0.853 | 1.451 | 0.005 | 0.004 | 1103.0 | 1046.0 | 1195.0 | 929.0 | 1.0 | . az.summary(idata_normal) . mean sd hdi_3% hdi_97% mcse_mean mcse_sd ess_mean ess_sd ess_bulk ess_tail r_hat . mu 1.854 | 0.205 | 1.448 | 2.222 | 0.005 | 0.004 | 1456.0 | 1456.0 | 1446.0 | 1187.0 | 1.0 | . sigma 1.138 | 0.155 | 0.862 | 1.420 | 0.004 | 0.003 | 1196.0 | 1070.0 | 1312.0 | 1154.0 | 1.0 | . In addition, we can plot the quantile ESS plot for one of them directly with plot_ess . az.plot_ess(idata_normal, kind=&quot;quantile&quot;, color=&quot;k&quot;); . Posterior validation . Check that both models are equivalent and do indeed give the same result for both parameters. . az.plot_density([idata_lognormal, idata_normal], data_labels=[&quot;Lognormal likelihood&quot;, &quot;Normal likelihood&quot;]); . Calculate LOO-CV . Now we get to calculate LOO-CV using Pareto Smoothed Importance Sampling as detailed in Vehtari et al., 2017. As we explained above, both models are equivalent, but one is in terms of $y$ and the other in terms of $ log y$. Therefore, their likelihoods will be on different scales, and hence, their expected log predictive density will also be different. . az.loo(idata_lognormal) . Computed from 2000 by 30 log-likelihood matrix Estimate SE elpd_loo -102.50 7.28 p_loo 1.80 - . az.loo(idata_normal) . Computed from 2000 by 30 log-likelihood matrix Estimate SE elpd_loo -46.57 3.46 p_loo 1.64 - . We have found that as expected, the two models yield different results despite being actually the same model. This is because. LOO is estimated from the log likelihood, $ log p(y_i mid theta^s)$, being $i$ the observation id, and $s$ the MCMC sample id. Following Vehtari et al., 2017, this log likelihood is used to calculate the PSIS weights and to estimate the expected log pointwise predictive density in the following way: . Calculate raw importance weights: $r_i^s = frac{1}{p(y_i mid theta^s)}$ | Smooth the $r_i^s$ (see original paper for details) to get the PSIS weights $w_i^s$ | Calculate elpd LOO as: | $$ text{elpd}_{psis-loo} = sum_{i=1}^n log left( frac{ sum_s w_i^s p(y_i| theta^s)}{ sum_s w_i^s} right) $$ . This will estimate the out of sample predictive fit of $y$ (where $y$ is the data of the model. Therefore, for the first model, using a LogNormal distribution, we are indeed calculating the desired quantity: . $$ text{elpd}_{psis-loo}^{(1)} approx sum_{i=1}^n log p(y_i|y_{-i}) $$ . Whereas for the second model, we are calculating: . $$ text{elpd}_{psis-loo}^{(2)} approx sum_{i=1}^n log p(z_i|z_{-i}) $$ . being $z_i = log y_i$. We actually have two different probability density functions, one over $y$ which from here on we will note $p_y(y)$, and $p_z(z)$. . In order to estimate the elpd loo for $y$ from the data in the second model, $z$, we have to describe $p_y(y)$ as a function of $z$ and $p_z(z)$. We know that $y$ and $z$ are actually related, and we can use this relation to find how would the random variable $y$ (which is actually a transformation of the random variable $z$) be distributed. This is done with the Jacobian. Therefore: . $$ p_y(y| theta)=p_z(z| theta)| frac{dz}{dy}|= frac{1}{|y|}p_z(z| theta)=e^{-z}p_z(z| theta) $$ In the log scale: . $$ log p_y(y| theta)=-z + log p_z(z| theta) $$ We apply the results to the log likelihood data of the second model (the normal on the logarithm instead of the lognormal) and check that now the result does coincide with the LOO-CV estimated by the lognormal model. . z = logy idata_normal.log_likelihood[&quot;y&quot;] = -z+idata_normal.log_likelihood.logy . az.loo(idata_normal, var_name=&quot;y&quot;) . Computed from 2000 by 30 log-likelihood matrix Estimate SE elpd_loo -102.30 7.26 p_loo 1.64 - . Real example . We will now use as data a subsample of a real dataset. The subset has been selected using: . df = pd.read_excel(&quot;indicator breast female incidence.xlsx&quot;).set_index(&quot;Breast Female Incidence&quot;).dropna(thresh=20).T df.to_csv(&quot;indicator_breast_female_incidence.csv&quot;) . Below, the data is loaded and plotted for inspection. . df = pd.read_csv(&quot;data/indicator_breast_female_incidence.csv&quot;, index_col=0) df.plot(figsize=(9,5.5)); . In order to show different examples of LOO on transformed data, we will take into account the following models: . $$ begin{align} &amp;y=a_1 x+a_0 &amp;y=e^{b_0}e^{b_1 x} &amp; rightarrow&amp; quad log y = z_1 = b_1 x + b_0 &amp;y=c_1^2 x^2 + 2 c_1 c_2 x + c_0^2 &amp; rightarrow&amp; quad sqrt{y} = z_2 = c_1 x + c_0 end{align} $$This models have been chosen mainly because of their simplicity. In addition, they can all be applied using the same Stan code and the data looks kind of linear. This will put the focus of the example on the loo calculation instead of on the model itself. For the online example, the data from Finland has been chosen, but feel free to download the notebook and experiment with it. . y_data = df.Finland z1_data = np.log(y_data) z2_data = np.sqrt(y_data) x_data = df.index/100 # rescale to set both to a similar scale dict_y = {&quot;N&quot;: len(x_data), &quot;y&quot;: y_data, &quot;x&quot;: x_data} dict_z1 = {&quot;N&quot;: len(x_data), &quot;y&quot;: z1_data, &quot;x&quot;: x_data} dict_z2 = {&quot;N&quot;: len(x_data), &quot;y&quot;: z2_data, &quot;x&quot;: x_data} coords = {&quot;year&quot;: x_data} dims = {&quot;y&quot;: [&quot;year&quot;], &quot;log_likelihood&quot;: [&quot;year&quot;]} . #collapse-hide lr_code = &quot;&quot;&quot; data { int&lt;lower=0&gt; N; vector[N] x; vector[N] y; } parameters { real b0; real b1; real&lt;lower=0&gt; sigma_e; } model { b0 ~ normal(0, 20); b1 ~ normal(0, 20); for (i in 1:N) { y[i] ~ normal(b0 + b1 * x[i], sigma_e); } } generated quantities { vector[N] log_lik; vector[N] y_hat; for (i in 1:N) { log_lik[i] = normal_lpdf(y[i] | b0 + b1 * x[i], sigma_e); y_hat[i] = normal_rng(b0 + b1 * x[i], sigma_e); } } &quot;&quot;&quot; . . sm_lr = pystan.StanModel(model_code=lr_code) control = {&quot;max_treedepth&quot;: 15} . INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_01dac21720941a01839c67cf2ac4a0fc NOW. . fit_y = sm_lr.sampling(data=dict_y, iter=1500, chains=6, control=control) . fit_z1 = sm_lr.sampling(data=dict_z1, iter=1500, chains=6, control=control) . fit_z2 = sm_lr.sampling(data=dict_z2, iter=1500, chains=6, control=control) . idata_y = az.from_pystan( posterior=fit_y, posterior_predictive=&#39;y_hat&#39;, observed_data=[&#39;y&#39;], log_likelihood={&#39;y&#39;: &#39;log_lik&#39;}, coords=coords, dims=dims, ) idata_y.posterior = idata_y.posterior.rename({&quot;b0&quot;: &quot;a0&quot;, &quot;b1&quot;: &quot;a1&quot;}) . idata_z1 = az.from_pystan( posterior=fit_z1, posterior_predictive=&#39;y_hat&#39;, observed_data=[&#39;y&#39;], log_likelihood={&#39;z1&#39;: &#39;log_lik&#39;}, coords=coords, dims=dims, ) . idata_z2 = az.from_pystan( posterior=fit_z2, posterior_predictive=&#39;y_hat&#39;, observed_data=[&#39;y&#39;], log_likelihood={&#39;z2&#39;: &#39;log_lik&#39;}, coords=coords, dims=dims, ) idata_z2.posterior = idata_z2.posterior.rename({&quot;b0&quot;: &quot;c0&quot;, &quot;b1&quot;: &quot;c1&quot;}) . In order to compare the out of sample predictive accuracy, we have to apply the Jacobian transformation to the 2 latter models, so that all of them are in terms of $y$. . Note: we will use LOO instead of Leave Future Out algorithm even though it may be more appropriate because the Jacobian transformation to be applied is the same in both cases. Moreover, PSIS-LOO does not require refitting, and it is already implemented in ArviZ. . The transformation to apply to the second model $z_1 = log y$ is the same as the previous example: . idata_z1.log_likelihood[&quot;y&quot;] = -z1_data.values+idata_z1.log_likelihood.z1 . In the case of the third model, $z_2 = sqrt{y}$: . $$ | frac{dz}{dy}| = | frac{1}{2 sqrt{y}}| = frac{1}{2 z_2} quad rightarrow quad log | frac{dz}{dy}| = - log (2 z_2)$$ . idata_z2.log_likelihood[&quot;y&quot;] = -np.log(2*z2_data.values)+idata_z2.log_likelihood.z2 . az.loo(idata_y) . Computed from 4500 by 46 log-likelihood matrix Estimate SE elpd_loo -194.26 3.83 p_loo 1.58 - . print(&quot;LOO before Jacobian transformation: {:.2f}&quot;.format(az.loo(idata_z1, var_name=&quot;z1&quot;).loo)) print(az.loo(idata_z1, var_name=&quot;y&quot;)) . LOO before Jacobian transformation: 70.56 Computed from 4500 by 46 log-likelihood matrix Estimate SE elpd_loo -100.44 4.47 p_loo 3.14 - . print(&quot;LOO before Jacobian transformation: {:.2f}&quot;.format(az.loo(idata_z2, var_name=&quot;z2&quot;).loo)) print(az.loo(idata_z2, var_name=&quot;y&quot;)) . LOO before Jacobian transformation: 2.21 Computed from 4500 by 46 log-likelihood matrix Estimate SE elpd_loo -115.17 3.81 p_loo 2.92 - . References . Vehtari, A., Gelman, A., and Gabry, J. (2017): Practical Bayesian Model Evaluation Using Leave-One-OutCross-Validation and WAIC, Statistics and Computing, vol. 27(5), pp. 1413–1432. . . Comments are not enabled for the blog, to inquiry further about the contents of the post, ask on ArviZ Issues or Stan Discourse .",
            "url": "https://moraouf.github.io/MoSpace/project/python/arviz/stan/2019/06/21/loo-cv-transformed-data.html",
            "relUrl": "/project/python/arviz/stan/2019/06/21/loo-cv-transformed-data.html",
            "date": " • Jun 21, 2019"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I’m a rebel physicist/engineer who loves statistical data analysis. By day, I am currently a Research Assistant on Bayesian Model Selection with David Rossell at UPF, Barcelona. By night I am a core developer of ArviZ a Python package for exploratory analysis of Bayesian models. In addition to data analysis probabilistic modeling, I also love programming and teaching. . I think that the culture in scientific research needs deep changes towards a more collaborative, open and diverse model. I am interested in open science, reproducible research and science communication. I want to pursue a career in probabilistic modeling and statistical research with special emphasis on openness and reproducibility. . In my spare time, I like playing board games and going to the beach to do water activities. I have been sailing and snorkeling regularly since I was little and more recently I added kayaking to the mix too! I generally spend the summer at the Costa Brava. Here I leave you a sneak peak of the views when nobody is around . . Projects . PyMCon 2020: PyMCon 2020 is an asynchronous-first virtual conference for the Bayesian community | . Open source work . Here are highlighted some open source projects I contribute to, check out my GitHub profile for a complete list of the projects I contribute to. . ArviZ: Exploratory analysis of Bayesian models in Python or Julia | mombf: Bayesian model selection and averaging for regression and mixtures for non-local and local priors. | exosherlock: Smooth your interactions with the NASA Exoplanet Archive using Python and pandas. | PyMC3/4: Friendly probabilistic programming in Python. | . Talks and conferences . PROBPROG 2020: Coming on autumn 2020 | StanCon 2020: ArviZ, InferenceData, and NetCDF: A unified file format for Bayesians. Slides and video presentation are available at GitHub, the slides are executable thanks to Binder! | . Publications . M. Badenas-Agusti, M. N. Günther, T. Daylan, et al., 2020, HD 191939: Three Sub-Neptunes Transiting a Sun-like Star Only 54 pc Away | D. Foreman-Mackey, W. Farr, M. Sinha, A. Archibald, et al., 2019, emcee v3: A Python ensemble sampling toolkit for affine-invariant MCMC. Get the emcee package code! | . | .",
          "url": "https://moraouf.github.io/MoSpace/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  
      ,"page2": {
          "title": "Blog",
          "content": "",
          "url": "https://moraouf.github.io/MoSpace/blog/",
          "relUrl": "/blog/",
          "date": ""
      }
      
  

  

  

  
  

  
      ,"page6": {
          "title": "Projects",
          "content": "",
          "url": "https://moraouf.github.io/MoSpace/projects/",
          "relUrl": "/projects/",
          "date": ""
      }
      
  

  
  

  

  
  

  

  
  

  
  

  
  

  
  

  
      ,"page15": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://moraouf.github.io/MoSpace/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

  
  

}