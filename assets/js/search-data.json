{
  
    
        "post0": {
            "title": "ArviZ in depth: plot_trace",
            "content": ". Warning: Post still in progress! . Introduction . plot_trace is one of the most common plots to assess the convergence of MCMC runs, therefore, it is also one of the most used ArviZ functions. plot_trace has a lot of parameters that allow creating highly customizable plots, but they may not be straightforward to use. There are many reasons that can explain this convolutedness of the arguments and their format, there is no clear culprit: ArviZ has to integrate with several libraries such as xarray and matplotlib which provide amazing features and customization power, and we&#39;d like to allow ArviZ users to access all these features. However, we also aim to keep ArviZ usage simple and with sensible defaults; plot_xyz(idata) should generate acceptable results in most situations. . This post aims to be an extension to the API section on plot_trace, focusing mostly on arguments where examples may be lacking and arguments that appear often in questions posted to ArviZ issues. . Therefore, the most common arguments such as var_names will not be covered, and for arguments that I do not remeber appearing in issues or generating confusion only some examples will be shown without an in depth description. . import arviz as az import matplotlib.pyplot as plt import numpy as np import xarray as xr # html render is not correctly rendered in blog, # comment the line below if in jupyter xr.set_options(display_style=&quot;text&quot;) rng = np.random.default_rng() . az.style.use(&quot;arviz-darkgrid&quot;) . idata_centered = az.load_arviz_data(&quot;centered_eight&quot;) idata = az.load_arviz_data(&quot;rugby&quot;) . The kind argument . az.plot_trace generates two columns. The left one calls plot_dist to plot KDE/Histogram of the data, and the right column can contain either the trace itself (which gives the name to the plot) or a rank plot for which two visualizations are available. Rank plots are an alternative to trace plots, see https://arxiv.org/abs/1903.08008 for more details. . fig, axes = plt.subplots(3,2, figsize=(12,6)) for i, kind in enumerate((&quot;trace&quot;, &quot;rank_bars&quot;, &quot;rank_vlines&quot;)): az.plot_trace(idata, var_names=&quot;home&quot;, kind=kind, ax=axes[i,:]); fig.tight_layout() . The divergences argument . If present, divergences are indicated as a black rugplot in both columns of the trace plot. By default they are placed at the bottom of the plot, but they can be placed at the top or hidden. . az.plot_trace(idata_centered, var_names=&quot;tau&quot;); . az.plot_trace(idata_centered, var_names=&quot;tau&quot;, divergences=None); . The rug argument . rug adds a rug plot with the posterior samples at the bottom of the distribution plot, there are no changes in the trace plot column. . ax = az.plot_trace(idata, var_names=&quot;home&quot;, rug=True, rug_kwargs={&quot;alpha&quot;: .4}) . But what about having both rug and divergences at the same time? Fear not, ArviZ automatically modifies the default for divergences from bottom to top to prevent rug and divergences from overlapping: . az.plot_trace(idata_centered, var_names=&quot;mu&quot;, rug=True); . The lines argument . The description about lines in plot_trace&#39;s docstring is the following: . lines : list of tuple of (str, dict, array_like), optional . List of (var_name, {‘coord’: selection}, [line, positions]) to be overplotted as vertical lines on the density and horizontal lines on the trace. . It is possible that the first thought after reading this line is similar to &quot;What is with this weird format?&quot; Well, this format is actually the stardard way ArviZ uses to iterate over xarray.Dataset objects because it contains all the info about the variable and the selected coordinates as well as the values themselves. The main helper function that handles this is arviz.plots.plot_utils.xarray_var_iter. . This section will be a little different from the other ones, and will focus on boosting plot_trace capabilities with internal ArviZ functions. You may want to skip to the section altogether of go straigh to the end. . Let&#39;s see what xarray_var_iter does with a simple dataset. We will create a dataset with two variables: a will be a 2x3 matrix and b will be a scalar. In addition, the dimensions of a will be labeled. . ds = xr.Dataset({ &quot;a&quot;: ((&quot;pos&quot;, &quot;direction&quot;), rng.normal(size=(2,3))), &quot;b&quot;: 12, &quot;pos&quot;: [&quot;top&quot;, &quot;bottom&quot;], &quot;direction&quot;: [&quot;x&quot;, &quot;y&quot;, &quot;z&quot;] }) ds . &lt;xarray.Dataset&gt; Dimensions: (direction: 3, pos: 2) Coordinates: * pos (pos) &lt;U6 &amp;#x27;top&amp;#x27; &amp;#x27;bottom&amp;#x27; * direction (direction) &lt;U1 &amp;#x27;x&amp;#x27; &amp;#x27;y&amp;#x27; &amp;#x27;z&amp;#x27; Data variables: a (pos, direction) float64 0.1042 1.328 -0.6562 ... -1.39 -1.359 b int64 12 . from arviz.plots.plot_utils import xarray_var_iter for var_name, sel, values in xarray_var_iter(ds): print(var_name, sel, values) . a {&#39;pos&#39;: &#39;top&#39;, &#39;direction&#39;: &#39;x&#39;} 0.10424154136128315 a {&#39;pos&#39;: &#39;top&#39;, &#39;direction&#39;: &#39;y&#39;} 1.32759640768798 a {&#39;pos&#39;: &#39;top&#39;, &#39;direction&#39;: &#39;z&#39;} -0.6561984012347789 a {&#39;pos&#39;: &#39;bottom&#39;, &#39;direction&#39;: &#39;x&#39;} -0.7907108124053726 a {&#39;pos&#39;: &#39;bottom&#39;, &#39;direction&#39;: &#39;y&#39;} -1.3895482305177547 a {&#39;pos&#39;: &#39;bottom&#39;, &#39;direction&#39;: &#39;z&#39;} -1.3589706334047433 b {} 12 . xarray_var_iter has iterated over every single scalar value without loosing track of where did every value come from. We can also modify the behaviour to skip some dimensions (i.e. in ArviZ we generally iterate over data dimensions and skip chain and draw dims). . for var_name, sel, values in xarray_var_iter(ds, skip_dims={&quot;direction&quot;}): print(var_name, sel, values) . a {&#39;pos&#39;: &#39;top&#39;} [ 0.10424154 1.32759641 -0.6561984 ] a {&#39;pos&#39;: &#39;bottom&#39;} [-0.79071081 -1.38954823 -1.35897063] b {} 12 . Now that we know about xarray_var_iter and what it does, we can use it to generate a list in the required format directly from xarray objects. Let&#39;s say for example we were interested in plotting the mean as a line in the trace plot: . var_names = [&quot;home&quot;, &quot;atts&quot;] lines = list(xarray_var_iter(idata.posterior[var_names].mean(dim=(&quot;chain&quot;, &quot;draw&quot;)))) az.plot_trace(idata, var_names=var_names, lines=lines); . And what about quantile lines? Lets plot the 10% and 90% quantile lines but only for defs variable: . . Note: This same approach can also be used with az.hdi skipping hdi dimension . var_names = [&quot;home&quot;, &quot;defs&quot;] quantile_ds = idata.posterior[[&quot;defs&quot;]].quantile((.1, .9), dim=(&quot;chain&quot;, &quot;draw&quot;)) lines = list(xarray_var_iter(quantile_ds, skip_dims={&quot;quantile&quot;})) az.plot_trace(idata, var_names=var_names, lines=lines); . Aggregation kwargs . This section is dedicated to 5 different kwargs, closely related to each other: compact+compact_prop, combined+chain_prop and legend. If we focus on the distribution plots of the left column, we may want to aggregate data along 2 possible dimensions, chains or variable dimension(s) -- school dimension in centered_eight data, team dimension in rugby data... As aggragation or not along these 2 possible dimensions is independent, we end up with 4 possibilities. . In az.plot_trace, the argument combined governs the aggregation of all chains into a single plot (has no effect in trace, only in distributions), and compact governs the aggregation of the variable dimension(s). In order to be able to distinguish each single line after some aggregation has taken place, a legend argument is also available to show the legend with the data labels. chain_prop and compact_prop allow customization of the aesthetics mapping. . We&#39;ll now cover all 4 possibilities to showcase all supported cases and explore related customizations. . combined=False and compact=False . The default behaviour of plot_trace is to perform no aggregation at all. In this case therefore, all subplots will have exactly one line per chain in the posterior. In this chain only setting, the default mapping is to use color to distinguish chains: . az.plot_trace(idata, var_names=[&quot;home&quot;, &quot;defs&quot;], legend=True); . combined=True and compact=False . Chains are aggregated into a single quantity if possible. Therefore, distribution column will have one line per subplot due to the aggregation but the trace column will be the same as in the previous section. This is also a chain only setting, the default mapping is to use color to distinguish chains. However, we&#39;ll use this example to show usage of chain_prop to map the chain to the linewidth: . chain_prop = {&quot;linewidth&quot;: (.5, 1, 2, 3)} az.plot_trace( idata, var_names=[&quot;home&quot;, &quot;defs&quot;], combined=True, chain_prop=chain_prop, compact=False, legend=True ); . combined=False and compact=True . You are probably tired already from scrolling down and we have only 6 teams! Imagine having a variable with a dimension of length 100 or more :scream: . In these cases, it may be more convenient to analyze a compact version of the trace plot: . az.plot_trace(idata, var_names=[&quot;home&quot;, &quot;defs&quot;], combined=False, compact=True, legend=True); . The first two things that jump to the eye are that ArviZ has drastically modified the default aesthetic of the plot and that the plot fits now comfortable in a single screen, bye bye scrolling :wave: . We can also see that legend=True has included multiple legends to the figure. The chain legend is always included in the top right trace plot, and the plots in the distribution column contain a legend if necessary. . combined=True and compact=True . To reduce even more the clutter of lines in the trace plot, we can also combine chains. Moreover, the linestyle -&gt; chain mapping can be distracting, especially if we don&#39;t care too much about distinguishing the chains between them. Like we did before, we will use chain_prop to control this. . az.plot_trace(idata, var_names=[&quot;home&quot;, &quot;defs&quot;], combined=True, chain_prop={&quot;ls&quot;: &quot;-&quot;}, compact=True); . Finally, we will explore alternative usage options for chain_prop and compact_prop. In the two previous examples we have used a 2 element tuple where the second position of the tuple contained the properties to use. Another alternative is to pass a string present in plt.rcParams[&quot;axes.prop_cycle&quot;], which in our case is color only. . az.plot_trace( idata, var_names=[&quot;home&quot;, &quot;defs&quot;], combined=True, chain_prop=&quot;color&quot;, compact=True, compact_prop={&quot;lw&quot;: np.linspace(.5, 3, 6)} ); . Summing it all up . Now that we have covered most arguments, let&#39;s put everything to practice. Try to generate a trace plot following the instructions below: . Show variables home, defs and atts showing only Scotland, Ireland, Italy, Wales coordinates. | For defs variable, plot lines showing the 70% HDI. | Map chains to the following colors: &#39;C0&#39;, &#39;C1&#39;, &#39;C2&#39;, &#39;C3&#39;, &quot;xkcd:purple blue&quot; | Map team dimension to both linestyle (solid and dashed) and linewidth | . #collapse-hide coords = {&quot;team&quot;: [&quot;Scotland&quot;, &quot;Ireland&quot;, &quot;Italy&quot;, &quot;Wales&quot;]} quantile_ds = az.hdi(idata, var_names=&quot;defs&quot;, coords=coords, hdi_prob=.7) lines = list(xarray_var_iter(quantile_ds, skip_dims={&quot;hdi&quot;, &quot;team&quot;})) chain_prop = {&quot;color&quot;: [&#39;C0&#39;, &#39;C1&#39;, &#39;C2&#39;, &#39;C3&#39;, &quot;xkcd:purple blue&quot;]} az.plot_trace( idata, var_names=[&quot;home&quot;, &quot;defs&quot;, &quot;atts&quot;], combined=True, chain_prop=chain_prop, compact=True, compact_prop={&quot;lw&quot;: np.linspace(.5, 3, 6), &quot;ls&quot;: (&quot;-&quot;, &quot;--&quot;)}, lines=lines, coords=coords ); . . . Comments are not enabled for the blog, to inquiry further about the contents of the post, ask on ArviZ Issues or PyMC Discourse .",
            "url": "https://oriolabril.github.io/oriol_unraveled/arviz/visualization/plotting/2020/06/20/plot-trace.html",
            "relUrl": "/arviz/visualization/plotting/2020/06/20/plot-trace.html",
            "date": " • Jun 20, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "ArviZ customization with rcParams",
            "content": "About . ArviZ not only builds on top of matplotlib&#39;s rcParams but also adds its own rcParams instance to handle specific settings. This post will only graze matplotlib&#39;s rcParams, which are already detailed in matplotlib&#39;s docs; it will dive into specific ArviZ rcParams. . Introduction . Paraphrasing the description on rcParams in the documentation of matplotlib: . ArviZ uses arvizrc configuration files to customize all kinds of properties, which we call rcParams. You can control the defaults of many properties in ArviZ:data loading mode (lazy or eager), automatically showing generated plots, the default information criteria and so on. There are several ways of modifying arviz.rcParams instance, each of them targeted to specific needs. . import arviz as az import matplotlib.pyplot as plt idata = az.load_arviz_data(&quot;centered_eight&quot;) . Customizing ArviZ . arvizrc file . To define default values on a per user or per project basis, arvizrc file should be used. When imported, ArviZ search for an arvizrc file in several locations sorted below by priority: . $PWD/arvizrc | $ARVIZ_DATA/arvizrc | On Linux, $XDG_CONFIG_HOME/arviz/arvizrc (if $XDG_CONFIG_HOME is defined) | or $HOME/.config/arviz/arvizrc (if $XDG_CONFIG_HOME is not defined) | . | On other platforms, $HOME/.arviz/arvizrc if $HOME is defined | . | . Once one of these files is found, ArviZ stops looking and loads its configuration. If none of them are present, the values hardcoded in ArviZ codebase are used. The file used to set the default values in ArviZ can be obtained with the following command: . import arviz as az print(az.rcparams.get_arviz_rcfile()) . /home/oriol/.config/arviz/arvizrc . ArviZ has loaded a file used to set defaults on a per user basis. Unless I use a different rc file in the current directory or modify rcParams as explained above, this configuration will be automatically used every time ArviZ is imported. . This can be really useful to define the favourite backend or information criterion, written once in the rc file and ArviZ automatically uses the desired values. . . Important: You should not rely on ArviZ defaults being always the same. . ArviZ strives to encourage best practices and therefore will change the default values whenever a new algorithm is developed to achieve this goal. If you rely on a specific value, you should either use an arvizrc template or set the defaults at the beggining of every script/notebook. . Dynamic rc settings . To set default values on a per file or per project basis, rcParams can also be modified dynamically, either overwritting a specific key: . az.rcParams[&quot;data.load&quot;] = &quot;eager&quot; . Note that rcParams is the instance to be modified, exactly like in matplotlib. Careful with capitalization! . Another option is to define a dictionary with several new defaults and update rcParams all at once. . rc = { &quot;data.load&quot;: &quot;lazy&quot;, &quot;plot.max_subplots&quot;: 30, &quot;stats.ic_scale&quot;: &quot;negative_log&quot;, &quot;plot.matplotlib.constrained_layout&quot;: False } az.rcParams.update(rc) . rc_context . And last but not least, to temporarily use a different set of defaults, ArviZ also has a rc_context function. Its main difference and advantage is that it is a context manager, therefore, all code executed inside the context will use the defaults defined by rc_context but once we exit the context, everything goes back to normal. Let&#39;s generate 3 posterior plots with the same command to show this: . _, axes = plt.subplots(1,3, figsize=(15,4)) az.plot_posterior(idata, var_names=&quot;mu&quot;, ax=axes[0]) with az.rc_context({&quot;plot.point_estimate&quot;: &quot;mode&quot;, &quot;stats.hdi_prob&quot;: 0.7}): az.plot_posterior(idata, var_names=&quot;mu&quot;, ax=axes[1]) az.plot_posterior(idata, var_names=&quot;mu&quot;, ax=axes[2]); . ArviZ default settings . This section will describe ArviZ rcParams as version 0.8.3 (see GitHub for an up to date version). . Data . The rcParams in this section are related to the data module in ArviZ, that is, they are either related to from_xyz converter functions or to InferenceData class. . data.http_protocol : {https, http} . Only the first two example datasets centered_eight and non_centered_eight come as part of ArviZ. All the others are downloaded from figshare the first time and stored locally to help reloading them the next time. We can get the names of the data available by not passing any argument to az.load_arviz_data (you can also get the description of each of them with az.list_datasets): . az.load_arviz_data().keys() . dict_keys([&#39;centered_eight&#39;, &#39;non_centered_eight&#39;, &#39;radon&#39;, &#39;rugby&#39;, &#39;regression1d&#39;, &#39;regression10d&#39;, &#39;classification1d&#39;, &#39;classification10d&#39;]) . Thus, the first time you call az.load_arviz_data(&quot;radon&quot;), ArviZ downloads the dataset using data.http_protocol. The default is set to https but if needed, it can be modified to http. Notice how there is no fallback, if downloading with https fails, there is no second try with http, an error is risen. To use http you have to set the rcParam explicitly. . data.index_origin : {0, 1} . ArviZ integration with Stan and Julia who use 1 based indexing motivate this rcParam. This rcParam is still at an early stage and its implementation is bound to vary, therefore it has no detailed description. . data.load : {lazy, eager} . Even when not using Dask, xarray&#39;s default is to load data lazily into memory when reading from disk. ArviZ&#39;s from_netcdf also uses the same default. That is, ArviZ functions that read data from disk from_netcdf and load_arviz_data do not load the data into memory unless data.load rcParam is set to eager. . Most use cases not only do not require loading data into memory but will also benefit from lazy loading. However, there is one clear exception: when too many files are lazily opened at the same time, xarray ends up crashing with extremely cryptic error messages, these cases require setting data loading to eager mode. One example of such situation is generating ArviZ documentation, we therefore set data.load to eager in sphinx configuration file. . data.metagroups : mapping of {str : list of str} . . Warning: Do not overwrite data.metagroups as things may break, to add custom metagroups add new keys to the dictionary as shown below . One of the current projects in ArviZ is to extend the capabilities of InferenceData. One of the limitations was not allowing its functions and methods to be applied to several groups at the same time. Starting with ArviZ 0.8.0, InferenceData methods take arguments groups and filter_groups to overcome this limitation. These two combined arguments have the same capabilities as var_names+filter_vars in plotting functions: exact matching, like and regex matching like pandas and support for ArviZ ~ negation prefix and one extra feature: metagroups. So what are metagroups? Let&#39;s see . #collapse-hide for metagroup, groups in az.rcParams[&quot;data.metagroups&quot;].items(): print(f&quot;{metagroup}: n {groups} n&quot;) . . posterior_groups: (&#39;posterior&#39;, &#39;posterior_predictive&#39;, &#39;sample_stats&#39;, &#39;log_likelihood&#39;) prior_groups: (&#39;prior&#39;, &#39;prior_predictive&#39;, &#39;sample_stats_prior&#39;) posterior_groups_warmup: (&#39;_warmup_posterior&#39;, &#39;_warmup_posterior_predictive&#39;, &#39;_warmup_sample_stats&#39;) latent_vars: (&#39;posterior&#39;, &#39;prior&#39;) observed_vars: (&#39;posterior_predictive&#39;, &#39;observed_data&#39;, &#39;prior_predictive&#39;) . Imagine the data you passed to the model was rescaled, after converting to InferenceData you have to rescale the data again to its original values, but not only the observations, posterior and prior predictive values too! . Having to apply the rescaling manually to each of the three groups is tedious at best, and creating a variable called observed_vars storing a list with these 3 groups is problematic -- when doing prior checks there is no posterior_predictive group, it&#39;s a highway towards errors at every turn. Metagroups are similar to the variable approach but it&#39;s already there and it applies the function only to present groups. Let&#39;s add a new metagroup and use it to shift our data: . az.rcParams[&quot;data.metagroups&quot;][&quot;sampled&quot;] = ( &#39;posterior&#39;, &#39;posterior_predictive&#39;, &#39;sample_stats&#39;, &#39;log_likelihood&#39;, &#39;prior&#39;, &#39;prior_predictive&#39; ) shifted_idata = idata.map(lambda x: x-7, groups=&quot;sampled&quot;) . data.save_warmup : bool . If True, converter functions will store warmup iterations in the corresponding groups by default. . Note: data.save_warmup does not affect from_netcdf, all groups are always loaded from file . . Plot . General . plot.backend : {matplotlib, bokeh} . Default plotting backend. . plot.max_subplots : int . Maximum number of subplots in a single figure. Adding too many subplots into a figure can be really slow, to the point that it looks like everthing has crashed without any error message. When there are more variables to plot than max_subplots allowed, ArviZ sends a warning and plots at most max_suplots. See for yourselves: . with az.rc_context({&quot;plot.max_subplots&quot;: 3}): az.plot_posterior(idata); . /home/oriol/venvs/arviz-dev/lib/python3.6/site-packages/arviz/plots/plot_utils.py:563: UserWarning: rcParams[&#39;plot.max_subplots&#39;] (3) is smaller than the number of variables to plot (10) in plot_posterior, generating only 3 plots UserWarning, . plot.point_estimate : {mean, median, model, None} . Default point estimate to include in plots like plot_posterior or plot_density. . Bokeh . plot.bokeh.bounds_x_range, plot.bokeh.bounds_y_range : auto, None or tuple of (float, float), default auto . plot.bokeh.figure.dpi : int, default 60 . plot.bokeh.figure.height, plot.bokeh.figure.width : int, default 500 . plot.bokeh.layout.order : str, default default . Select subplot structure for bokeh. One of default, column, row, square, square_trimmed or Ncolumn (Nrow) where N is an integer number of columns (rows), here is one example to generate a subplot grid with 2 columns and the necessary rows to fit all variables. . with az.rc_context({&quot;plot.bokeh.layout.order&quot;: &quot;2column&quot;}): az.plot_ess(idata, backend=&quot;bokeh&quot;) . plot.bokeh.layout.sizing_mode : {fixed, stretch_width, stretch_height, stretch_both, scale_width, scale_height, scale_both} . plot.bokeh.layout.toolbar_location : {above, below, left, right, None} . Location for toolbar on bokeh layouts. None will hide the toolbar. . plot.bokeh.marker : str, default Cross . Default marker for bokeh plots. See bokeh reference on markers for more details. . plot.bokeh.output_backend : {webgl, canvas, svg} . plot.bokeh.show : bool, default True . Show bokeh plot before returning in ArviZ function. . plot.bokeh.tools : str, default reset,pan,box_zoom,wheel_zoom,lasso_select,undo,save,hove . Default tools in bokeh plots. More details on Configuring Plot Tools docs . Matplotlib . Matplotlib already has its own rcParams, which are actually the inspiration for ArviZ rcParams. Therefore, this section is minimalistic. . plot.matplotlib.show : bool, default False . Call plt.show from within ArviZ plotting functions. This generally makes no difference in jupyter like environments, but it can be useful for instance in the IPython terminal when we don&#39;t want to customize the plots genereted by ArviZ by changing titles or labels. . . Stats . stats.hdi_prob : float . Default probability of the calculated HDI intervals. . Important: This probability is completely arbitrary. ArviZ using 0.94 instead of the more common 0.95 aims to emphasize this arbitrary choice. . stats.information_criterion : {loo, waic} . Default information criterion used by compare and plot_elpd . stats.ic_pointwise : bool, default False . Return pointwise values when calling loo or waic. Pointwise values are an intermediate result and therefore setting ic_pointwise to true does not require extra computation. . stats.ic_scale : {log, deviance, negative_log} . Default information criterion scale. See docs on loo or waic for more detail. . . . Tip: Is there any extra rcParam you&#8217;d like to see in ArviZ? Check out arviz-devs/arviz#792, it&#8217;s more than possible you&#8217;ll be able to add it yourself! . . Comments are not enabled for the blog, to inquiry further about the contents of the post, ask on ArviZ Issues. .",
            "url": "https://oriolabril.github.io/oriol_unraveled/arviz/customization/rcparams/2020/06/19/rcParams.html",
            "relUrl": "/arviz/customization/rcparams/2020/06/19/rcParams.html",
            "date": " • Jun 19, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "LOO-PIT tutorial",
            "content": "Introduction . One of the new functionalities I added in ArviZ during my GSoC internship is Leave One Out (LOO) Probability Integral Transform (PIT) marginal posterior predictive checks. You can see two examples of its usage in the example gallery and also some examples in its API section. However, these examples are mainly related to the usage of the functionalities, not so much on the usage of LOO-PIT itself nor its interpretability. . I feel that the LOO-PIT algorithm usage and interpretability needs a short summary with examples showing the most common issues found when checking models with LOO-PIT. This tutorial will tackle this issue: how can LOO-PIT be used for model checking and what does it tell us in a practical manner, so we can see firsthand how wrongly specified models cause LOO-PIT values to differ from a uniform distribution. I have included a short description on what is the algorithm doing, however, for a detailed explanation, see: . Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., and Rubin, D. B. (2013). Bayesian Data Analysis. Chapman &amp; Hall/CRC Press, London, third edition. (p. 152-153) | . We will use LOO-PIT checks along with non-marginal posterior predictive checks as implemented in ArviZ. This will allow to see some differences between the two kinds of posterior predictive checks as well as to provide some intuition to cases where one may be best and cases where both are needed. . Here, we will experiment with LOO-PIT using two different models. First an estimation of the mean and standard deviation of a 1D Gaussian Random Variable, and then a 1D linear regression. Afterwards, we will see how to use LOO-PIT checks with multivariate data using as example a multivariate linear regression. . Background . One of the pilars of Bayesian Statistics is working with the posterior distribution of the parameters instead of using point estimates and errors or confidence intervals. We all know how to obtain this posterior given the likelihood, the prior and the , $p( theta mid y) = p(y mid theta) p( theta) / p(y)$. In addition, in many cases we are also interested in the probability of future observations given the observed data according to our model. This is called posterior predictive, which is calculated integrating out $ theta$: . $$ p(y^* | y) = int p(y^*| theta) p( theta|y) d theta$$ . where $y^*$ is the possible unobserved data and $y$ is the observed data. Therefore, if our model is correct, the observed data and the posterior predictive follow the same probability density function (pdf). In order to check if this holds, it is common to perform posterior predictive checks comparing the posterior predictive to the observed data. This can be done directly, comparing the kernel density estimates (KDE) of the observed data and posterior predictive samples, etc. A KDEs is nothing else than an estimation of the pdf of a random variable given a finite number of samples from this random variable. . Another alternative it to perform LOO-PIT checks, which are a kind of marginal posterior predictive checks. Marginal because we compare each observation only with the corresponding posterior predictive samples instead of combining all observations and all posterior predictive samples. As the name indicates, it combines two different concepts, Leave-One-Out Cross-Validation and Probability Integral Transform. . Probability Integral Transform . Probability Integral Transform stands for the fact that given a random variable $X$, the random variable $Y = F_X(X) = P(x leq X)$ is a uniform random variable if the transformation $F_X$ is the Cumulative Density Function (CDF) of the original random variable $X$. . If instead of $F_X$ we have $n$ samples from $X$, $ {x_1, dots, x_n }$, we can use them to estimate $ hat{F_X}$ and apply it to future $X$ samples ${x^*}$. In this case, $ hat{F_X}(x^*)$ will be approximately a uniform random variable, converging to an exact uniform variable as $n$ tends to infinity. . The mathematical demonstration can be found on wikipedia itself just googling it. However here, instead of reproducing it I will try to outline the intuition behind this fact. One way to imagine it is with posterior samples from an MCMC run. If we have enough samples, the probability of a new sample falling between the two smallest values will be the same than the probability of a new sample falling inside the two values closest to the median. . This is because around the probability around the smallest values will be lower, but they will be further apart, whereas the probability around the median will be larger but they will be extremely close. These two effect compensate each other and the probability is indeed the same. Thus, the probability is constant independently of the square the new sample would fall in, which is only compatible with a uniform distribution. . Leave-One-Out Cross-Validation . Cross-Validation is one way to try to solve the problem with all the future data I have been mentioning so far. We do not have this future data, so how are we supposed to make calculations with it? Cross-Validation solves this problem by dividing the observed data in $K$ subsets, excluding one subset from the data used to fit the model (so it is data unknown to the model, aka future data) and then using this excluded subset as future data. In general, to get better results, this process is preformed $K$ times, excluding one different subset every time. . LOO-CV is one particular case where the number of subsets is equal to the number of observations so that each iteration only one observation is excluded. That is, we fit the model one time per observation excluding only this one observation. . LOO-PIT . LOO-PIT checks consist on checking the PIT using LOO-CV. That is, fit the model on all data but observation $y_i$ (we will refer to this leave one out subset as $y_{-i}$), use this model to estimate the cumulative density function of the posterior predictive and calculate the PIT, $P(y_i &lt; y^* mid y_{-i}) = int_{- infty}^{y_i} p(y^* mid y_{-i}) dy^*$, of each observation. Then, the KDE of all LOO-PIT values is estimated to see whether or not it is compatible with the LOO-PIT values being draws from a uniform variable. . Data generation . import pymc3 as pm import numpy as np import arviz as az import matplotlib.pyplot as plt import theano.tensor as tt import scipy.stats as stats np.random.seed(7) . az.style.use(&#39;arviz-darkgrid&#39;) . def plot_ppc_loopit(idata, title): fig = plt.figure(figsize=(12,9)) ax_ppc = fig.add_subplot(211) ax1 = fig.add_subplot(223); ax2 = fig.add_subplot(224) az.plot_ppc(idata, ax=ax_ppc); for ax, ecdf in zip([ax1, ax2], (False, True)): az.plot_loo_pit(idata, y=&quot;obs&quot;, ecdf=ecdf, ax=ax); ax_ppc.set_title(title) ax_ppc.set_xlabel(&quot;&quot;) return np.array([ax_ppc, ax1, ax2]) . N_obs = 170 mu_normal = -2 sd_normal = 3 data_normal = np.random.normal(loc=mu_normal, scale=sd_normal, size=N_obs) a0_lr, a1_lr = 5, -2.3 sd_lr = 1.4 data_x_regression = np.linspace(0, 10, N_obs) data_y_regression = np.random.normal(loc=a1_lr*data_x_regression+a0_lr, scale=sd_lr) . coords_normal = {&quot;obs&quot;: [&quot;observation&quot;], &quot;log_likelihood&quot;: [&quot;observation&quot;]} dims_normal = {&quot;observation&quot;: range(N_obs)} coords_regression = {&quot;y&quot;: [&quot;time&quot;], &quot;log_likelihood&quot;: [&quot;time&quot;]} dims_regression = {&quot;time&quot;: data_x_regression} . We will now plot the two datsets generated, to give graphical an idea of the data we are working with. . fig, axes = plt.subplots(1, 2, figsize=(11,5)) textsize = plt.rcParams[&quot;axes.labelsize&quot;] az.plot_dist(data_normal, rug=True, ax=axes[0], rug_kwargs={&quot;space&quot;: 0}, textsize=textsize); axes[1].plot(data_x_regression, data_y_regression, &quot;.&quot;); axes[1].tick_params(labelsize=textsize) axes[0].set_title(&quot;Gaussian random variable draws&quot;) axes[1].set_title(&quot;Data for linear regression&quot;) fig.tight_layout() . Unidimensional Gaussian variable . We will start with a model that correctly fits with the data, to show how should both checks look like. Afterwards, we will see cases were these checks deviate from this ideal case and give some hints on how to interpret these deviations. . with pm.Model() as model: # Define priors mu = pm.Normal(&quot;mu&quot;, mu=0, sd=10) sd = pm.HalfNormal(&quot;sd&quot;, sd=10) # Define likelihood likelihood = pm.Normal(&quot;obs&quot;, mu=mu, sd=sd, observed=data_normal) # Inference! trace = pm.sample() # draw posterior samples using NUTS sampling prior = pm.sample_prior_predictive() posterior_predictive = pm.sample_posterior_predictive(trace) idata_normal = az.from_pymc3( trace=trace, prior=prior, posterior_predictive=posterior_predictive, coords=coords_normal, dims=dims_normal, ) . Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (4 chains in 4 jobs) NUTS: [sd, mu] Sampling 4 chains, 0 divergences: 100%|██████████| 4000/4000 [00:00&lt;00:00, 4875.74draws/s] 100%|██████████| 2000/2000 [00:01&lt;00:00, 1748.53it/s] . plot_ppc_loopit(idata_normal, &quot;Gaussian: Calibrated model&quot;); . To begin with, it can be seen that the observed KDE is similar to the overlayed posterior predictive KDEs. The same happens with the LOO-PIT values; the LOO-PIT KDE is similar to the overlayed uniform KDEs. Thus, in this first example, similar information can be obteined from their interpretation. . Overdispersion signs . We will now move to one common mismatch between the model and the observed data. We will perform the same fit as the previous example but fixing the standard deviation of the normal random variable. This is actually not an unrealistic case, as in many cases where the instrument used to measure gives error data in addition to the measure, this error is used to fix the standard deviation. . These two examples show how the LOO-PIT looks like for overdispersed models (i.e. the error is assumed to be larger than what it actually is) and for underdispersed models (i.e. the error is assumed to be smaller than what it really is). . with pm.Model() as model: mu = pm.Normal(&quot;mu&quot;, mu=0, sd=10) likelihood = pm.Normal(&quot;obs&quot;, mu=mu, sd=1.5 * sd_normal, observed=data_normal) trace = pm.sample() prior = pm.sample_prior_predictive() posterior_predictive = pm.sample_posterior_predictive(trace) idata_normal_overdispersed = az.from_pymc3( trace=trace, prior=prior, posterior_predictive=posterior_predictive, coords=coords_normal, dims=dims_normal, ) . Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (4 chains in 4 jobs) NUTS: [mu] Sampling 4 chains, 0 divergences: 100%|██████████| 4000/4000 [00:00&lt;00:00, 6269.06draws/s] 100%|██████████| 2000/2000 [00:00&lt;00:00, 3299.82it/s] . plot_ppc_loopit(idata_normal_overdispersed, &quot;Gaussian: Overdispersed model&quot;); . In this example of overdispersed model, we can see that the posterior predictive checks show that the observed KDE is narrower than most of the posterior predictive KDEs and narrower than the mean KDE of the posterior predictive samples. However, there are still some posterior predictive samples whose KDEs are similar to the observed KDE. In the LOO-PIT check though, there is no room for confursion. The LOO-PIT KDE is not uniform between 0 and 1, its range is much quite more limited than the uniform counterparts. Moreover, the difference between the Empirical Cumulative Density Function (ECDF) and the ideal uniform CDF lays outside the envelope most of the time. . Underdispersion signs . with pm.Model() as model: mu = pm.Normal(&quot;mu&quot;, mu=0, sd=10) likelihood = pm.Normal(&quot;obs&quot;, mu=mu, sd=.75 * sd_normal, observed=data_normal) trace = pm.sample() prior = pm.sample_prior_predictive() posterior_predictive = pm.sample_posterior_predictive(trace) idata_normal_underdispersed = az.from_pymc3( trace=trace, prior=prior, posterior_predictive=posterior_predictive, coords=coords_normal, dims=dims_normal, ) . Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (4 chains in 4 jobs) NUTS: [mu] Sampling 4 chains, 0 divergences: 100%|██████████| 4000/4000 [00:00&lt;00:00, 6293.68draws/s] 100%|██████████| 2000/2000 [00:00&lt;00:00, 3354.84it/s] . plot_ppc_loopit(idata_normal_underdispersed, &quot;Gaussian: Underdispersed model&quot;); . Here, the differences are similar to the overdispersed case, modifying overdispersed by underdispersed and inverting the shapes. . Bias signs . In addition, LOO-PIT checks also show signs of model bias, as shown in the following example: . with pm.Model() as model: sd = pm.HalfNormal(&quot;sd&quot;, sd=10) likelihood = pm.Normal(&quot;obs&quot;, mu=mu_normal - sd_normal / 2, sd=sd, observed=data_normal) trace = pm.sample() prior = pm.sample_prior_predictive() posterior_predictive = pm.sample_posterior_predictive(trace) idata_normal_bias = az.from_pymc3( trace=trace, prior=prior, posterior_predictive=posterior_predictive, coords=coords_normal, dims=dims_normal, ) . Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (4 chains in 4 jobs) NUTS: [sd] Sampling 4 chains, 0 divergences: 100%|██████████| 4000/4000 [00:00&lt;00:00, 4953.10draws/s] 100%|██████████| 2000/2000 [00:01&lt;00:00, 1613.28it/s] . plot_ppc_loopit(idata_normal_bias, &quot;Gaussian: Biased model&quot;); . It is important to note though, that the LOO-PIT itself already indicates the problem with the model: . a convex KDE shape (inverted-U shape or range smaller than 0-1) or an N in the ECDF difference plot is a sign of an overdispersed model | a concave KDE shape (U shape) or an inverted-N ECDF difference is a sign of underdispersion | an asymmetrical KDE (range may also be reduced instead of 0-1) or ECDF difference is a sign for model bias | . In general though, we will probably find a combination of all these cases and it may not be straigthforward to interpretate what is wrong with the model using LOO-PIT or posterior predictive KDE checks. . Linear regression . In the case of a linear regression, the posterior predictive checks direclty do not give us much information, because each datapoint is centered at a different location, so combining them to create a single KDE won&#39;t yield useful results. It is important to note though, that this is not an issue inherent to the posterior predictive checks, and could be solved by rescaling each observation by substracting the mean and divide by the standard deviation along every observation from the posterior predictive. We will also include an example of this kind of transformation in the last example, but there should not be much to worry about as this improvement is on the ArviZ roadmap. . with pm.Model() as model: sigma = pm.HalfNormal(&#39;sigma&#39;, sd=10) a0 = pm.Normal(&quot;a0&quot;, mu=0, sd=20) a1 = pm.Normal(&quot;a1&quot;, mu=0, sd=20) likelihood = pm.Normal(&#39;obs&#39;, mu=a0 + a1 * data_x_regression, sd=sigma, observed=data_y_regression) trace = pm.sample() prior = pm.sample_prior_predictive() posterior_predictive = pm.sample_posterior_predictive(trace) idata_lr = az.from_pymc3( trace=trace, prior=prior, posterior_predictive=posterior_predictive, coords=coords_regression, dims=dims_regression, ) . Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (4 chains in 4 jobs) NUTS: [a1, a0, sigma] Sampling 4 chains, 0 divergences: 100%|██████████| 4000/4000 [00:01&lt;00:00, 2882.58draws/s] The acceptance probability does not match the target. It is 0.9065348582364419, but should be close to 0.8. Try to increase the number of tuning steps. 100%|██████████| 2000/2000 [00:01&lt;00:00, 1221.16it/s] . plot_ppc_loopit(idata_lr, &quot;Linear Regression: Calibrated model&quot;); . Now let&#39;s see how does introducing some small bias modifies the results. . with pm.Model() as model: sigma = pm.HalfNormal(&#39;sigma&#39;, sd=10) a1 = pm.Normal(&quot;a1&quot;, mu=0, sd=20) likelihood = pm.Normal(&#39;obs&#39;, mu=a0_lr + 2 + a1 * data_x_regression, sd=sigma, observed=data_y_regression) trace = pm.sample() prior = pm.sample_prior_predictive() posterior_predictive = pm.sample_posterior_predictive(trace) idata_lr_bias = az.from_pymc3( trace=trace, prior=prior, posterior_predictive=posterior_predictive, coords=coords_regression, dims=dims_regression, ) . Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (4 chains in 4 jobs) NUTS: [a1, sigma] Sampling 4 chains, 0 divergences: 100%|██████████| 4000/4000 [00:00&lt;00:00, 5018.12draws/s] The acceptance probability does not match the target. It is 0.8792817423537712, but should be close to 0.8. Try to increase the number of tuning steps. 100%|██████████| 2000/2000 [00:01&lt;00:00, 1413.29it/s] . plot_ppc_loopit(idata_lr_bias, &quot;Linear Regression: Biased model&quot;); . Now the LOO-PIT check is clearly showing signs of bias in the model, whereas due to the lack of rescaling, no bias is seen in the posterior predictive checks. . Finally, let&#39;s combine some bias with overdispersion, to see how is LOO-PIT modified. Moreover, we will rescale the posterior predictive data to see how would rescaling affect the posterior predictive checks. . with pm.Model() as model: a1 = pm.Normal(&quot;a1&quot;, mu=0, sd=20) likelihood = pm.Normal( &#39;obs&#39;, mu=a0_lr + 2 + a1 * data_x_regression, sd=1.5 * sd_lr, observed=data_y_regression ) # Inference! trace = pm.sample() # draw posterior samples using NUTS sampling prior = pm.sample_prior_predictive() posterior_predictive = pm.sample_posterior_predictive(trace) idata_lr_bias_overdispersed = az.from_pymc3( trace=trace, prior=prior, posterior_predictive=posterior_predictive, coords=coords_regression, dims=dims_regression, ) . Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (4 chains in 4 jobs) NUTS: [a1] Sampling 4 chains, 0 divergences: 100%|██████████| 4000/4000 [00:00&lt;00:00, 6382.96draws/s] 100%|██████████| 2000/2000 [00:00&lt;00:00, 2138.31it/s] . plot_ppc_loopit(idata_lr_bias_overdispersed, &quot;Linear Regression: Biased and oversidpersed model&quot;); . pp_samples = idata_lr_bias_overdispersed.posterior_predictive.obs obs_samples = idata_lr_bias_overdispersed.observed_data.obs pp_means = pp_samples.mean(dim=(&quot;chain&quot;, &quot;draw&quot;)) pp_stds = pp_samples.std(dim=(&quot;chain&quot;, &quot;draw&quot;)) idata_lr_bias_overdispersed.posterior_predictive[&quot;obs_rescaled&quot;] = (pp_samples - pp_means) / pp_stds idata_lr_bias_overdispersed.observed_data[&quot;obs_rescaled&quot;] = (obs_samples - pp_means) / pp_stds . fig, axes = plt.subplots(3, 2, figsize=(12,13), constrained_layout=True) for i, var in enumerate((&quot;obs&quot;, &quot;obs_rescaled&quot;)): for j, ecdf in enumerate((False, True)): az.plot_loo_pit(idata_lr_bias_overdispersed, y=var, ecdf=ecdf, ax=axes[j, i]); az.plot_ppc(idata_lr_bias_overdispersed, ax=axes[2]); fig.suptitle(&quot;Linear Regression: Rescaling effect nBiased and overdispersed model&quot;, fontsize=16); . As you can see, the posterior predictive check for obs_rescaled does indicate overdispersion and bias of the posterior predictive samples, whereas the one for obs does not, following what we were seeing previously. The LOO-PIT checks do not change one bit however. This is actually a property of the LOO-PIT algorithm. As it is comparing the marginal distributions of the posterior predictive and the observed data using the MCMC samples, any monotonous transformation will not modify its value because it won&#39;t modify the order between the samples. Therefore, if the observed data is larger than 36% of the posterior predictive samples, the rescaling we have done does not modify this fact. . . Comments are not enabled for the blog, to inquiry further about the contents of the post, ask on ArviZ Issues or PyMC3 Discourse .",
            "url": "https://oriolabril.github.io/oriol_unraveled/arviz/visualization/model%20criticism/2019/07/31/loo-pit-tutorial.html",
            "relUrl": "/arviz/visualization/model%20criticism/2019/07/31/loo-pit-tutorial.html",
            "date": " • Jul 31, 2019"
        }
        
    
  
    
        ,"post3": {
            "title": "LOO-CV on transformed data",
            "content": "Blog post exploring whether or not LOO-CV can be used to compare models that try to explain some data $y$ with models trying to explain the same data after a transformation $z=f(y)$. Inspired by @tiagocc question on Stan Forums. This post has two sections, the first one is the mathematical derivation of the equations used and their application on a validation example, and the second section is a real example. In addition to the LOO-CV usage examples and explanations, another goal of this notebook is to show and highlight the capabilities of ArviZ. . import pystan import pandas as pd import numpy as np import arviz as az import matplotlib.pyplot as plt . az.style.use(&quot;arviz-darkgrid&quot;) . Mathematical derivation and validation example . In the first example, we will compare two equivalent models: . $y sim text{LogNormal}( mu, sigma)$ | $ log y sim text{Normal}( mu, sigma)$ | Model definition and execution . Define the data and execute the two models . mu = 2 sigma = 1 logy = np.random.normal(loc=mu, scale=sigma, size=30) y = np.exp(logy) # y will then be distributed as lognormal data = {&#39;N&#39;: len(y), &#39;y&#39;: y, &#39;logy&#39;: logy} . #collapse-hide lognormal_code = &quot;&quot;&quot; data { int&lt;lower=0&gt; N; vector[N] y; } parameters { real mu; real&lt;lower=0&gt; sigma; } model { y ~ lognormal(mu, sigma); } generated quantities { vector[N] log_lik; vector[N] y_hat; for (i in 1:N) { log_lik[i] = lognormal_lpdf(y[i] | mu, sigma); y_hat[i] = lognormal_rng(mu, sigma); } } &quot;&quot;&quot; . . sm_lognormal = pystan.StanModel(model_code=lognormal_code) fit_lognormal = sm_lognormal.sampling(data=data, iter=1000, chains=4) . INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_af04dcd0464f65fe0e5bbc595b4eb9d6 NOW. . idata_lognormal = az.from_pystan( posterior=fit_lognormal, posterior_predictive=&#39;y_hat&#39;, observed_data=[&#39;y&#39;], log_likelihood={&#39;y&#39;: &#39;log_lik&#39;}, ) . #collapse-hide normal_on_log_code = &quot;&quot;&quot; data { int&lt;lower=0&gt; N; vector[N] logy; } parameters { real mu; real&lt;lower=0&gt; sigma; } model { logy ~ normal(mu, sigma); } generated quantities { vector[N] log_lik; vector[N] logy_hat; for (i in 1:N) { log_lik[i] = normal_lpdf(logy[i] | mu, sigma); logy_hat[i] = normal_rng(mu, sigma); } } &quot;&quot;&quot; . . sm_normal = pystan.StanModel(model_code=normal_on_log_code) fit_normal = sm_normal.sampling(data=data, iter=1000, chains=4) . INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_acd7c874588f1c862727f931f4dbf916 NOW. . idata_normal = az.from_pystan( posterior=fit_normal, posterior_predictive=&#39;logy_hat&#39;, observed_data=[&#39;logy&#39;], log_likelihood={&#39;logy&#39;: &#39;log_lik&#39;}, ) . Check model convergence. Use az.summary to in one view that the effective sample size (ESS) is large enough and $ hat{R}$ is close to one. . az.summary(idata_lognormal) . mean sd hdi_3% hdi_97% mcse_mean mcse_sd ess_mean ess_sd ess_bulk ess_tail r_hat . mu 1.857 | 0.210 | 1.485 | 2.259 | 0.005 | 0.004 | 1533.0 | 1526.0 | 1535.0 | 1483.0 | 1.0 | . sigma 1.145 | 0.164 | 0.853 | 1.451 | 0.005 | 0.004 | 1103.0 | 1046.0 | 1195.0 | 929.0 | 1.0 | . az.summary(idata_normal) . mean sd hdi_3% hdi_97% mcse_mean mcse_sd ess_mean ess_sd ess_bulk ess_tail r_hat . mu 1.854 | 0.205 | 1.448 | 2.222 | 0.005 | 0.004 | 1456.0 | 1456.0 | 1446.0 | 1187.0 | 1.0 | . sigma 1.138 | 0.155 | 0.862 | 1.420 | 0.004 | 0.003 | 1196.0 | 1070.0 | 1312.0 | 1154.0 | 1.0 | . In addition, we can plot the quantile ESS plot for one of them directly with plot_ess . az.plot_ess(idata_normal, kind=&quot;quantile&quot;, color=&quot;k&quot;); . Posterior validation . Check that both models are equivalent and do indeed give the same result for both parameters. . az.plot_density([idata_lognormal, idata_normal], data_labels=[&quot;Lognormal likelihood&quot;, &quot;Normal likelihood&quot;]); . Calculate LOO-CV . Now we get to calculate LOO-CV using Pareto Smoothed Importance Sampling as detailed in Vehtari et al., 2017. As we explained above, both models are equivalent, but one is in terms of $y$ and the other in terms of $ log y$. Therefore, their likelihoods will be on different scales, and hence, their expected log predictive density will also be different. . az.loo(idata_lognormal) . Computed from 2000 by 30 log-likelihood matrix Estimate SE elpd_loo -102.50 7.28 p_loo 1.80 - . az.loo(idata_normal) . Computed from 2000 by 30 log-likelihood matrix Estimate SE elpd_loo -46.57 3.46 p_loo 1.64 - . We have found that as expected, the two models yield different results despite being actually the same model. This is because. LOO is estimated from the log likelihood, $ log p(y_i mid theta^s)$, being $i$ the observation id, and $s$ the MCMC sample id. Following Vehtari et al., 2017, this log likelihood is used to calculate the PSIS weights and to estimate the expected log pointwise predictive density in the following way: . Calculate raw importance weights: $r_i^s = frac{1}{p(y_i mid theta^s)}$ | Smooth the $r_i^s$ (see original paper for details) to get the PSIS weights $w_i^s$ | Calculate elpd LOO as: | $$ text{elpd}_{psis-loo} = sum_{i=1}^n log left( frac{ sum_s w_i^s p(y_i| theta^s)}{ sum_s w_i^s} right) $$ . This will estimate the out of sample predictive fit of $y$ (where $y$ is the data of the model. Therefore, for the first model, using a LogNormal distribution, we are indeed calculating the desired quantity: . $$ text{elpd}_{psis-loo}^{(1)} approx sum_{i=1}^n log p(y_i|y_{-i}) $$ . Whereas for the second model, we are calculating: . $$ text{elpd}_{psis-loo}^{(2)} approx sum_{i=1}^n log p(z_i|z_{-i}) $$ . being $z_i = log y_i$. We actually have two different probability density functions, one over $y$ which from here on we will note $p_y(y)$, and $p_z(z)$. . In order to estimate the elpd loo for $y$ from the data in the second model, $z$, we have to describe $p_y(y)$ as a function of $z$ and $p_z(z)$. We know that $y$ and $z$ are actually related, and we can use this relation to find how would the random variable $y$ (which is actually a transformation of the random variable $z$) be distributed. This is done with the Jacobian. Therefore: . $$ p_y(y| theta)=p_z(z| theta)| frac{dz}{dy}|= frac{1}{|y|}p_z(z| theta)=e^{-z}p_z(z| theta) $$ In the log scale: . $$ log p_y(y| theta)=-z + log p_z(z| theta) $$ We apply the results to the log likelihood data of the second model (the normal on the logarithm instead of the lognormal) and check that now the result does coincide with the LOO-CV estimated by the lognormal model. . z = logy idata_normal.log_likelihood[&quot;y&quot;] = -z+idata_normal.log_likelihood.logy . az.loo(idata_normal, var_name=&quot;y&quot;) . Computed from 2000 by 30 log-likelihood matrix Estimate SE elpd_loo -102.30 7.26 p_loo 1.64 - . Real example . We will now use as data a subsample of a real dataset. The subset has been selected using: . df = pd.read_excel(&quot;indicator breast female incidence.xlsx&quot;).set_index(&quot;Breast Female Incidence&quot;).dropna(thresh=20).T df.to_csv(&quot;indicator_breast_female_incidence.csv&quot;) . Below, the data is loaded and plotted for inspection. . df = pd.read_csv(&quot;data/indicator_breast_female_incidence.csv&quot;, index_col=0) df.plot(figsize=(9,5.5)); . In order to show different examples of LOO on transformed data, we will take into account the following models: . $$ begin{align} &amp;y=a_1 x+a_0 &amp;y=e^{b_0}e^{b_1 x} &amp; rightarrow&amp; quad log y = z_1 = b_1 x + b_0 &amp;y=c_1^2 x^2 + 2 c_1 c_2 x + c_0^2 &amp; rightarrow&amp; quad sqrt{y} = z_2 = c_1 x + c_0 end{align} $$This models have been chosen mainly because of their simplicity. In addition, they can all be applied using the same Stan code and the data looks kind of linear. This will put the focus of the example on the loo calculation instead of on the model itself. For the online example, the data from Finland has been chosen, but feel free to download the notebook and experiment with it. . y_data = df.Finland z1_data = np.log(y_data) z2_data = np.sqrt(y_data) x_data = df.index/100 # rescale to set both to a similar scale dict_y = {&quot;N&quot;: len(x_data), &quot;y&quot;: y_data, &quot;x&quot;: x_data} dict_z1 = {&quot;N&quot;: len(x_data), &quot;y&quot;: z1_data, &quot;x&quot;: x_data} dict_z2 = {&quot;N&quot;: len(x_data), &quot;y&quot;: z2_data, &quot;x&quot;: x_data} coords = {&quot;year&quot;: x_data} dims = {&quot;y&quot;: [&quot;year&quot;], &quot;log_likelihood&quot;: [&quot;year&quot;]} . #collapse-hide lr_code = &quot;&quot;&quot; data { int&lt;lower=0&gt; N; vector[N] x; vector[N] y; } parameters { real b0; real b1; real&lt;lower=0&gt; sigma_e; } model { b0 ~ normal(0, 20); b1 ~ normal(0, 20); for (i in 1:N) { y[i] ~ normal(b0 + b1 * x[i], sigma_e); } } generated quantities { vector[N] log_lik; vector[N] y_hat; for (i in 1:N) { log_lik[i] = normal_lpdf(y[i] | b0 + b1 * x[i], sigma_e); y_hat[i] = normal_rng(b0 + b1 * x[i], sigma_e); } } &quot;&quot;&quot; . . sm_lr = pystan.StanModel(model_code=lr_code) control = {&quot;max_treedepth&quot;: 15} . INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_01dac21720941a01839c67cf2ac4a0fc NOW. . fit_y = sm_lr.sampling(data=dict_y, iter=1500, chains=6, control=control) . fit_z1 = sm_lr.sampling(data=dict_z1, iter=1500, chains=6, control=control) . fit_z2 = sm_lr.sampling(data=dict_z2, iter=1500, chains=6, control=control) . idata_y = az.from_pystan( posterior=fit_y, posterior_predictive=&#39;y_hat&#39;, observed_data=[&#39;y&#39;], log_likelihood={&#39;y&#39;: &#39;log_lik&#39;}, coords=coords, dims=dims, ) idata_y.posterior = idata_y.posterior.rename({&quot;b0&quot;: &quot;a0&quot;, &quot;b1&quot;: &quot;a1&quot;}) . idata_z1 = az.from_pystan( posterior=fit_z1, posterior_predictive=&#39;y_hat&#39;, observed_data=[&#39;y&#39;], log_likelihood={&#39;z1&#39;: &#39;log_lik&#39;}, coords=coords, dims=dims, ) . idata_z2 = az.from_pystan( posterior=fit_z2, posterior_predictive=&#39;y_hat&#39;, observed_data=[&#39;y&#39;], log_likelihood={&#39;z2&#39;: &#39;log_lik&#39;}, coords=coords, dims=dims, ) idata_z2.posterior = idata_z2.posterior.rename({&quot;b0&quot;: &quot;c0&quot;, &quot;b1&quot;: &quot;c1&quot;}) . In order to compare the out of sample predictive accuracy, we have to apply the Jacobian transformation to the 2 latter models, so that all of them are in terms of $y$. . Note: we will use LOO instead of Leave Future Out algorithm even though it may be more appropriate because the Jacobian transformation to be applied is the same in both cases. Moreover, PSIS-LOO does not require refitting, and it is already implemented in ArviZ. . The transformation to apply to the second model $z_1 = log y$ is the same as the previous example: . idata_z1.log_likelihood[&quot;y&quot;] = -z1_data.values+idata_z1.log_likelihood.z1 . In the case of the third model, $z_2 = sqrt{y}$: . $$ | frac{dz}{dy}| = | frac{1}{2 sqrt{y}}| = frac{1}{2 z_2} quad rightarrow quad log | frac{dz}{dy}| = - log (2 z_2)$$ . idata_z2.log_likelihood[&quot;y&quot;] = -np.log(2*z2_data.values)+idata_z2.log_likelihood.z2 . az.loo(idata_y) . Computed from 4500 by 46 log-likelihood matrix Estimate SE elpd_loo -194.26 3.83 p_loo 1.58 - . print(&quot;LOO before Jacobian transformation: {:.2f}&quot;.format(az.loo(idata_z1, var_name=&quot;z1&quot;).loo)) print(az.loo(idata_z1, var_name=&quot;y&quot;)) . LOO before Jacobian transformation: 70.56 Computed from 4500 by 46 log-likelihood matrix Estimate SE elpd_loo -100.44 4.47 p_loo 3.14 - . print(&quot;LOO before Jacobian transformation: {:.2f}&quot;.format(az.loo(idata_z2, var_name=&quot;z2&quot;).loo)) print(az.loo(idata_z2, var_name=&quot;y&quot;)) . LOO before Jacobian transformation: 2.21 Computed from 4500 by 46 log-likelihood matrix Estimate SE elpd_loo -115.17 3.81 p_loo 2.92 - . References . Vehtari, A., Gelman, A., and Gabry, J. (2017): Practical Bayesian Model Evaluation Using Leave-One-OutCross-Validation and WAIC, Statistics and Computing, vol. 27(5), pp. 1413–1432. . . Comments are not enabled for the blog, to inquiry further about the contents of the post, ask on ArviZ Issues or Stan Discourse .",
            "url": "https://oriolabril.github.io/oriol_unraveled/arviz/model%20comparison/2019/06/21/loo-cv-transformed-data.html",
            "relUrl": "/arviz/model%20comparison/2019/06/21/loo-cv-transformed-data.html",
            "date": " • Jun 21, 2019"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I’m a rebel physicist/engineer who loves statistical data analysis. By day, I am currently a Research Assistant on Bayesian Model Selection with David Rossell at UPF, Barcelona. By night I am a core developer of ArviZ a Python package for exploratory analysis of Bayesian models. In addition to data analysis probabilistic modeling, I also love programming and teaching. . I think that the culture in scientific research needs deep changes towards a more collaborative, open and diverse model. I am interested in open science, reproducible research and science communication. I want to pursue a career in probabilistic modeling and statistical research with special emphasis on openness and reproducibility. . In my spare time, I like playing board games and going to the beach to do water activities. I have been sailing and snorkeling regularly since I was little and more recently I added kayaking to the mix too! I generally spend the summer at the Costa Brava. Here I leave you a sneak peak of the views when nobody is around . . Projects . Here are highlighted some projects I contribute to, check out my GitHub profile for a complete list of the projects I contribute to. . ArviZ: Exploratory analysis of Bayesian models in Python or Julia | mombf: Bayesian model selection and averaging for regression and mixtures for non-local and local priors. | exosherlock: Smooth your interactions with the NASA Exoplanet Archive using Python and pandas. | PyMC3/4: Friendly probabilistic programming in Python. | . Publications and presentations . PROBPROG 2020: Coming on autumn 2020 | M. Badenas-Agusti, M. N. Günther, T. Daylan, et al., 2020, HD 191939: Three Sub-Neptunes Transiting a Sun-like Star Only 54 pc Away | D. Foreman-Mackey, W. Farr, M. Sinha, A. Archibald, et al., 2019, emcee v3: A Python ensemble sampling toolkit for affine-invariant MCMC. Get the emcee package code! | . | .",
          "url": "https://oriolabril.github.io/oriol_unraveled/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  
      ,"page2": {
          "title": "Blog",
          "content": "",
          "url": "https://oriolabril.github.io/oriol_unraveled/blog/",
          "relUrl": "/blog/",
          "date": ""
      }
      
  

  

  

  
  

  
  

  

  
  

  
  

  
  

  
  

  
  

  
      ,"page13": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://oriolabril.github.io/oriol_unraveled/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

  
  

}