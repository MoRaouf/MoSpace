{
  
    
        "post0": {
            "title": "ArviZ customization with rcParams",
            "content": "Comments are not enabled for the blog, to inquiry further about the contents of the post, ask on ArviZ Issues or PyMC Discourse . About . ArviZ not only builds on top of matplotlib&#39;s rcParams but also adds its own rcParams instance to handle specific settings. This post will only graze matplotlib&#39;s rcParams, which are already detailed in matplotlib&#39;s docs; it will dive into specific ArviZ rcParams. . Introduction . Paraphrasing the description on rcParams in the documentation of matplotlib: . ArviZ uses arvizrc configuration files to customize all kinds of properties, which we call rcParams. You can control the defaults of many properties in ArviZ:data loading mode (lazy or eager), automatically showing generated plots, the default information criteria and so on. There are several ways of modifying arviz.rcParams instance, each of them targeted to specific needs. . Customizing ArviZ . arvizrc file . To define default values on a per user or per project basis, arvizrc file should be used. When imported, ArviZ search for an arvizrc file in several locations sorted below by priority: . $PWD/arvizrc | $ARVIZ_DATA/arvizrc | On Linux, $XDG_CONFIG_HOME/arviz/arvizrc (if $XDG_CONFIG_HOME is defined) | or $HOME/.config/arviz/arvizrc (if $XDG_CONFIG_HOME is not defined) | . | On other platforms, $HOME/.arviz/arvizrc if $HOME is defined | . | . Once one of these files is found, ArviZ stops looking and loads its configuration. If none of them are present, the values hardcoded in ArviZ codebase are used. The file used to set the default values in ArviZ can be obtained with the following command: . import arviz as az print(az.rcparams.get_arviz_rcfile()) . /home/oriol/.config/arviz/arvizrc . ArviZ has loaded a file used to set defaults on a per user basis. Unless I use a different rc file in the current directory or modify rcParams as explained above, this configuration will be automatically used every time ArviZ is imported. This can be really useful to define the favourite backend or information criterion, written once in the rc file and ArviZ automatically uses the desired values. . Dynamic rc settings . To set default values on a per file or per project basis, rcParams can also be modified dynamically, either overwritting a specific key: . az.rcParams[&quot;data.load&quot;] = &quot;eager&quot; . Note that rcParams is the instance to be modified, exactly like in matplotlib. . Another option is to define a dictionary with several new defaults and update rcParams all at once. . rc = { &quot;data.load&quot;: &quot;lazy&quot;, &quot;plot.max_subplots&quot;: 30, &quot;stats.ic_scale&quot;: &quot;negative_log&quot;, &quot;plot.matplotlib.constrained_layout&quot;: False } az.rcParams.update(rc) . rc_context . Eventually, to temporarily use a different set of defaults, ArviZ also has a rc_context function. Its main difference and advantage is that it is a context manager, therefore, all code executed inside the context will use the defaults defined by rc_context but once we exit the context, everything goes back to normal. . #collapse-hide idata = az.load_arviz_data(&quot;centered_eight&quot;) print(az.summary(idata, var_names=&quot;theta&quot;, kind=&quot;stats&quot;)) with az.rc_context({&quot;data.index_origin&quot;: 1}): print(az.summary(idata, var_names=&quot;theta&quot;, kind=&quot;stats&quot;)) print(az.summary(idata, var_names=&quot;theta&quot;, kind=&quot;stats&quot;)) . . mean sd hpd_3% hpd_97% theta[0] 6.026 5.782 -3.707 17.337 theta[1] 4.724 4.736 -4.039 13.999 theta[2] 3.576 5.559 -6.779 13.838 theta[3] 4.478 4.939 -5.528 13.392 theta[4] 3.064 4.642 -5.972 11.547 theta[5] 3.821 4.979 -5.507 13.232 theta[6] 6.250 5.436 -3.412 16.920 theta[7] 4.544 5.521 -5.665 15.266 mean sd hpd_3% hpd_97% theta[1] 6.026 5.782 -3.707 17.337 theta[2] 4.724 4.736 -4.039 13.999 theta[3] 3.576 5.559 -6.779 13.838 theta[4] 4.478 4.939 -5.528 13.392 theta[5] 3.064 4.642 -5.972 11.547 theta[6] 3.821 4.979 -5.507 13.232 theta[7] 6.250 5.436 -3.412 16.920 theta[8] 4.544 5.521 -5.665 15.266 mean sd hpd_3% hpd_97% theta[0] 6.026 5.782 -3.707 17.337 theta[1] 4.724 4.736 -4.039 13.999 theta[2] 3.576 5.559 -6.779 13.838 theta[3] 4.478 4.939 -5.528 13.392 theta[4] 3.064 4.642 -5.972 11.547 theta[5] 3.821 4.979 -5.507 13.232 theta[6] 6.250 5.436 -3.412 16.920 theta[7] 4.544 5.521 -5.665 15.266 . ArviZ default settings . Here are the default ArviZ settings (also available in GitHub) . from arviz.rcparams import RcParams, defaultParams print(RcParams([(key, default) for key, (default, _) in defaultParams.items()])) . data.http_protocol : https data.index_origin : 0 data.load : lazy data.save_warmup : False plot.backend : matplotlib plot.bokeh.bounds_x_range: auto plot.bokeh.bounds_y_range: auto plot.bokeh.figure.dpi : 60 plot.bokeh.figure.height: 500 plot.bokeh.figure.width: 500 plot.bokeh.layout.order: default plot.bokeh.layout.sizing_mode: fixed plot.bokeh.layout.toolbar_location: above plot.bokeh.marker : Cross plot.bokeh.output_backend: webgl plot.bokeh.show : True plot.bokeh.tools : reset,pan,box_zoom,wheel_zoom,lasso_select,undo,save,hover plot.matplotlib.constrained_layout: True plot.matplotlib.show : False plot.max_subplots : 40 plot.point_estimate : mean stats.credible_interval: 0.94 stats.ic_scale : log stats.information_criterion: loo .",
            "url": "https://oriolabril.github.io/oriol_unraveled/arviz/customization/rcparams/2020/04/30/rcParams.html",
            "relUrl": "/arviz/customization/rcparams/2020/04/30/rcParams.html",
            "date": " • Apr 30, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "ArviZ customization: `plot_trace`",
            "content": "",
            "url": "https://oriolabril.github.io/oriol_unraveled/arviz/visualization/plotting/2020/04/30/plot-trace.html",
            "relUrl": "/arviz/visualization/plotting/2020/04/30/plot-trace.html",
            "date": " • Apr 30, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Introduction to ArviZ: InferenceData objects",
            "content": "About . This notebook is an introduction to InferenceData objects and their role in ArviZ. It aims to be as hands-on as possible, using examples to cover different use cases of the data in each group, examples on how to combine several InferenceData objects and so on. . The key idea behind InferenceData objects is to centralize and store all data relevant to a specific Bayesian inference run; from observed_data to predictions going through prior and sample_stats. The goal is therefore to both ease exploration and visualization of Bayesian inference results and to ease their sharing. Hence, ArviZ also provides several converter functions to transform results from common inference libraries such as PyMC3, PyStan or Pyro to InferenceData. In the future, we&#39;ll dedicate one post to each converter function, but before, we have to make sure the conversion process is worth it! .",
            "url": "https://oriolabril.github.io/oriol_unraveled/arviz/inferencedata/data/2020/04/30/InferenceData.html",
            "relUrl": "/arviz/inferencedata/data/2020/04/30/InferenceData.html",
            "date": " • Apr 30, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "LOO-PIT tutorial",
            "content": "Introduction . One of the new functionalities I added in ArviZ during my GSoC internship is Leave One Out (LOO) Probability Integral Transform (PIT) marginal posterior predictive checks. You can see two examples of its usage in the example gallery and also some examples in its API section. However, these examples are mainly related to the usage of the functionalities, not so much on the usage of LOO-PIT itself nor its interpretability. . I feel that the LOO-PIT algorithm usage and interpretability needs a short summary with examples showing the most common issues found when checking models with LOO-PIT. This tutorial will tackle this issue: how can LOO-PIT be used for model checking and what does it tell us in a practical manner, so we can see firsthand how wrongly specified models cause LOO-PIT values to differ from a uniform distribution. I have included a short description on what is the algorithm doing, however, for a detailed explanation, see: . Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., and Rubin, D. B. (2013). Bayesian Data Analysis. Chapman &amp; Hall/CRC Press, London, third edition. (p. 152-153) | . We will use LOO-PIT checks along with non-marginal posterior predictive checks as implemented in ArviZ. This will allow to see some differences between the two kinds of posterior predictive checks as well as to provide some intuition to cases where one may be best and cases where both are needed. . Here, we will experiment with LOO-PIT using two different models. First an estimation of the mean and standard deviation of a 1D Gaussian Random Variable, and then a 1D linear regression. Afterwards, we will see how to use LOO-PIT checks with multivariate data using as example a multivariate linear regression. . Background . One of the pilars of Bayesian Statistics is working with the posterior distribution of the parameters instead of using point estimates and errors or confidence intervals. We all know how to obtain this posterior given the likelihood, the prior and the , $p( theta mid y) = p(y mid theta) p( theta) / p(y)$. In addition, in many cases we are also interested in the probability of future observations given the observed data according to our model. This is called posterior predictive, which is calculated integrating out $ theta$: . $$ p(y^* | y) = int p(y^*| theta) p( theta|y) d theta$$ . where $y^*$ is the possible unobserved data and $y$ is the observed data. Therefore, if our model is correct, the observed data and the posterior predictive follow the same probability density function (pdf). In order to check if this holds, it is common to perform posterior predictive checks comparing the posterior predictive to the observed data. This can be done directly, comparing the kernel density estimates (KDE) of the observed data and posterior predictive samples, etc. A KDEs is nothing else than an estimation of the pdf of a random variable given a finite number of samples from this random variable. . Another alternative it to perform LOO-PIT checks, which are a kind of marginal posterior predictive checks. Marginal because we compare each observation only with the corresponding posterior predictive samples instead of combining all observations and all posterior predictive samples. As the name indicates, it combines two different concepts, Leave-One-Out Cross-Validation and Probability Integral Transform. . Probability Integral Transform . Probability Integral Transform stands for the fact that given a random variable $X$, the random variable $Y = F_X(X) = P(x leq X)$ is a uniform random variable if the transformation $F_X$ is the Cumulative Density Function (CDF) of the original random variable $X$. . If instead of $F_X$ we have $n$ samples from $X$, $ {x_1, dots, x_n }$, we can use them to estimate $ hat{F_X}$ and apply it to future $X$ samples ${x^*}$. In this case, $ hat{F_X}(x^*)$ will be approximately a uniform random variable, converging to an exact uniform variable as $n$ tends to infinity. . The mathematical demonstration can be found on wikipedia itself just googling it. However here, instead of reproducing it I will try to outline the intuition behind this fact. One way to imagine it is with posterior samples from an MCMC run. If we have enough samples, the probability of a new sample falling between the two smallest values will be the same than the probability of a new sample falling inside the two values closest to the median. . This is because around the probability around the smallest values will be lower, but they will be further apart, whereas the probability around the median will be larger but they will be extremely close. These two effect compensate each other and the probability is indeed the same. Thus, the probability is constant independently of the square the new sample would fall in, which is only compatible with a uniform distribution. . Leave-One-Out Cross-Validation . Cross-Validation is one way to try to solve the problem with all the future data I have been mentioning so far. We do not have this future data, so how are we supposed to make calculations with it? Cross-Validation solves this problem by dividing the observed data in $K$ subsets, excluding one subset from the data used to fit the model (so it is data unknown to the model, aka future data) and then using this excluded subset as future data. In general, to get better results, this process is preformed $K$ times, excluding one different subset every time. . LOO-CV is one particular case where the number of subsets is equal to the number of observations so that each iteration only one observation is excluded. That is, we fit the model one time per observation excluding only this one observation. . LOO-PIT . LOO-PIT checks consist on checking the PIT using LOO-CV. That is, fit the model on all data but observation $y_i$ (we will refer to this leave one out subset as $y_{-i}$), use this model to estimate the cumulative density function of the posterior predictive and calculate the PIT, $P(y_i &lt; y^* mid y_{-i}) = int_{- infty}^{y_i} p(y^* mid y_{-i}) dy^*$, of each observation. Then, the KDE of all LOO-PIT values is estimated to see whether or not it is compatible with the LOO-PIT values being draws from a uniform variable. . Data generation . import pymc3 as pm import numpy as np import arviz as az import matplotlib.pyplot as plt import theano.tensor as tt import scipy.stats as stats np.random.seed(7) . plt.style.use(&#39;arviz-darkgrid&#39;) . def plot_ppc_loopit(idata, title): fig = plt.figure(figsize=(12,9)) ax_ppc = fig.add_subplot(211) ax1 = fig.add_subplot(223); ax2 = fig.add_subplot(224) az.plot_ppc(idata, ax=ax_ppc); for ax, ecdf in zip([ax1, ax2], (False, True)): az.plot_loo_pit(idata, y=&quot;obs&quot;, ecdf=ecdf, ax=ax); ax_ppc.set_title(title) ax_ppc.set_xlabel(&quot;&quot;) return np.array([ax_ppc, ax1, ax2]) . N_obs = 170 mu_normal = -2 sd_normal = 3 data_normal = np.random.normal(loc=mu_normal, scale=sd_normal, size=N_obs) a0_lr, a1_lr = 5, -2.3 sd_lr = 1.4 data_x_regression = np.linspace(0, 10, N_obs) data_y_regression = np.random.normal(loc=a1_lr*data_x_regression+a0_lr, scale=sd_lr) . coords_normal = {&quot;obs&quot;: [&quot;observation&quot;], &quot;log_likelihood&quot;: [&quot;observation&quot;]} dims_normal = {&quot;observation&quot;: range(N_obs)} coords_regression = {&quot;y&quot;: [&quot;time&quot;], &quot;log_likelihood&quot;: [&quot;time&quot;]} dims_regression = {&quot;time&quot;: data_x_regression} . We will now plot the two datsets generated, to give graphical an idea of the data we are working with. . fig, axes = plt.subplots(1, 2, figsize=(11,5)) textsize = plt.rcParams[&quot;axes.labelsize&quot;] az.plot_dist(data_normal, rug=True, ax=axes[0], rug_kwargs={&quot;space&quot;: 0}, textsize=textsize); axes[1].plot(data_x_regression, data_y_regression, &quot;.&quot;); axes[1].tick_params(labelsize=textsize) axes[0].set_title(&quot;Gaussian random variable draws&quot;) axes[1].set_title(&quot;Data for linear regression&quot;) fig.tight_layout() . Unidimensional Gaussian variable . We will start with a model that correctly fits with the data, to show how should both checks look like. Afterwards, we will see cases were these checks deviate from this ideal case and give some hints on how to interpret these deviations. . with pm.Model() as model: # Define priors mu = pm.Normal(&quot;mu&quot;, mu=0, sd=10) sd = pm.HalfNormal(&quot;sd&quot;, sd=10) # Define likelihood likelihood = pm.Normal(&quot;obs&quot;, mu=mu, sd=sd, observed=data_normal) # Inference! trace = pm.sample() # draw posterior samples using NUTS sampling prior = pm.sample_prior_predictive() posterior_predictive = pm.sample_posterior_predictive(trace) idata_normal = az.from_pymc3( trace=trace, prior=prior, posterior_predictive=posterior_predictive, coords=coords_normal, dims=dims_normal, ) . Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (4 chains in 4 jobs) NUTS: [sd, mu] Sampling 4 chains, 0 divergences: 100%|██████████| 4000/4000 [00:00&lt;00:00, 5267.73draws/s] 100%|██████████| 2000/2000 [00:01&lt;00:00, 1172.82it/s] . plot_ppc_loopit(idata_normal, &quot;Gaussian: Calibrated model&quot;); . To begin with, it can be seen that the observed KDE is similar to the overlayed posterior predictive KDEs. The same happens with the LOO-PIT values; the LOO-PIT KDE is similar to the overlayed uniform KDEs. Thus, in this first example, similar information can be obteined from their interpretation. . Overdispersion signs . We will now move to one common mismatch between the model and the observed data. We will perform the same fit as the previous example but fixing the standard deviation of the normal random variable. This is actually not an unrealistic case, as in many cases where the instrument used to measure gives error data in addition to the measure, this error is used to fix the standard deviation. . These two examples show how the LOO-PIT looks like for overdispersed models (i.e. the error is assumed to be larger than what it actually is) and for underdispersed models (i.e. the error is assumed to be smaller than what it really is). . with pm.Model() as model: # Define priors mu = pm.Normal(&quot;mu&quot;, mu=0, sd=10) # Define likelihood likelihood = pm.Normal( &quot;obs&quot;, mu=mu, sd=1.5 * sd_normal, observed=data_normal ) # Inference! trace = pm.sample() # draw posterior samples using NUTS sampling prior = pm.sample_prior_predictive() posterior_predictive = pm.sample_posterior_predictive(trace) idata_normal_overdispersed = az.from_pymc3( trace=trace, prior=prior, posterior_predictive=posterior_predictive, coords=coords_normal, dims=dims_normal, ) . Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (4 chains in 4 jobs) NUTS: [mu] Sampling 4 chains, 0 divergences: 100%|██████████| 4000/4000 [00:00&lt;00:00, 7063.95draws/s] 100%|██████████| 2000/2000 [00:00&lt;00:00, 3180.66it/s] . plot_ppc_loopit(idata_normal_overdispersed, &quot;Gaussian: Overdispersed model&quot;); . In this example of overdispersed model, we can see that the posterior predictive checks show that the observed KDE is narrower than most of the posterior predictive KDEs and narrower than the mean KDE of the posterior predictive samples. However, there are still some posterior predictive samples whose KDEs are similar to the observed KDE. In the LOO-PIT check though, there is no room for confursion. No overlayed distribution in as extreme as the LOO-PIT KDE, and the difference between the Empirical Cumulative Density Function (ECDF) and the ideal uniform CDF lays outside the envelope most of the time. . Underdispersion signs . with pm.Model() as model: # Define priors mu = pm.Normal(&quot;mu&quot;, mu=0, sd=10) # Define likelihood likelihood = pm.Normal( &quot;obs&quot;, mu=mu, sd=.75 * sd_normal, observed=data_normal ) # Inference! trace = pm.sample() # draw posterior samples using NUTS sampling prior = pm.sample_prior_predictive() posterior_predictive = pm.sample_posterior_predictive(trace) idata_normal_underdispersed = az.from_pymc3( trace=trace, prior=prior, posterior_predictive=posterior_predictive, coords=coords_normal, dims=dims_normal, ) . Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (4 chains in 4 jobs) NUTS: [mu] Sampling 4 chains, 0 divergences: 100%|██████████| 4000/4000 [00:00&lt;00:00, 5405.15draws/s] 100%|██████████| 2000/2000 [00:00&lt;00:00, 2750.77it/s] . plot_ppc_loopit(idata_normal_underdispersed, &quot;Gaussian: Underdispersed model&quot;); . Here, the differences are similar to the overdispersed case, modifying overdispersed by underdispersed and inverting the shapes. . Bias signs . In addition, LOO-PIT checks also show signs of model bias, as shown in the following example: . with pm.Model() as model: # Define priors sd = pm.HalfNormal(&quot;sd&quot;, sd=10) # Define likelihood likelihood = pm.Normal( &quot;obs&quot;, mu=mu_normal - sd_normal / 2, sd=sd, observed=data_normal ) # Inference! trace = pm.sample() # draw posterior samples using NUTS sampling prior = pm.sample_prior_predictive() posterior_predictive = pm.sample_posterior_predictive(trace) idata_normal_bias = az.from_pymc3( trace=trace, prior=prior, posterior_predictive=posterior_predictive, coords=coords_normal, dims=dims_normal, ) . Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (4 chains in 4 jobs) NUTS: [sd] Sampling 4 chains, 0 divergences: 100%|██████████| 4000/4000 [00:00&lt;00:00, 6495.75draws/s] 100%|██████████| 2000/2000 [00:01&lt;00:00, 1615.44it/s] . plot_ppc_loopit(idata_normal_bias, &quot;Gaussian: Biased model&quot;); . It is important to note though, that the LOO-PIT itself already indicates the problem with the model: . a convex KDE shape (inverted-U shape) or an N in the ECDF difference plot is a sign of an overdispersed model | a concave KDE shape (U shape) or an inverted-N ECDF difference is a sign of underdispersion | an asymmetrical KDE or ECDF difference is a sign for model bias | . In general though, we will probably find a combination of all these cases and it may not be straigthforward to interpretate what is wrong with the model using LOO-PIT or posterior predictive KDE checks. . Linear regression . In the case of a linear regression, the posterior predictive checks direclty do not give us much information, because each datapoint is centered at a different location, so combining them to create a single KDE won&#39;t yield useful results. It is important to note though, that this is not an issue inherent to the posterior predictive checks, and could be solved by rescaling each observation by substracting the mean and divide by the standard deviation along every observation from the posterior predictive. We will also include an example of this kind of transformation in the last example, but there should not be much to worry about as this improvement is on the ArviZ roadmap. . with pm.Model() as model: # model specifications in PyMC3 are wrapped in a with-statement # Define priors sigma = pm.HalfNormal(&#39;sigma&#39;, sd=10) a0 = pm.Normal(&quot;a0&quot;, mu=0, sd=20) a1 = pm.Normal(&quot;a1&quot;, mu=0, sd=20) # Define likelihood likelihood = pm.Normal(&#39;obs&#39;, mu=a0 + a1 * data_x_regression, sd=sigma, observed=data_y_regression) # Inference! trace = pm.sample() # draw posterior samples using NUTS sampling prior = pm.sample_prior_predictive() posterior_predictive = pm.sample_posterior_predictive(trace) idata_lr = az.from_pymc3( trace=trace, prior=prior, posterior_predictive=posterior_predictive, coords=coords_regression, dims=dims_regression, ) . Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (4 chains in 4 jobs) NUTS: [a1, a0, sigma] Sampling 4 chains, 0 divergences: 100%|██████████| 4000/4000 [00:01&lt;00:00, 2992.36draws/s] The acceptance probability does not match the target. It is 0.9065348582364419, but should be close to 0.8. Try to increase the number of tuning steps. 100%|██████████| 2000/2000 [00:01&lt;00:00, 1275.84it/s] . plot_ppc_loopit(idata_lr, &quot;Linear Regression: Calibrated model&quot;); . Now let&#39;s see how does introducing some small bias modifies the results. . with pm.Model() as model: # model specifications in PyMC3 are wrapped in a with-statement # Define priors sigma = pm.HalfNormal(&#39;sigma&#39;, sd=10) a1 = pm.Normal(&quot;a1&quot;, mu=0, sd=20) # Define likelihood likelihood = pm.Normal(&#39;obs&#39;, mu=a0_lr + 2 + a1 * data_x_regression, sd=sigma, observed=data_y_regression) # Inference! trace = pm.sample() # draw posterior samples using NUTS sampling prior = pm.sample_prior_predictive() posterior_predictive = pm.sample_posterior_predictive(trace) idata_lr_bias = az.from_pymc3( trace=trace, prior=prior, posterior_predictive=posterior_predictive, coords=coords_regression, dims=dims_regression, ) . Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (4 chains in 4 jobs) NUTS: [a1, sigma] Sampling 4 chains, 0 divergences: 100%|██████████| 4000/4000 [00:00&lt;00:00, 4865.69draws/s] The acceptance probability does not match the target. It is 0.8792817423537712, but should be close to 0.8. Try to increase the number of tuning steps. 100%|██████████| 2000/2000 [00:01&lt;00:00, 1181.81it/s] . plot_ppc_loopit(idata_lr_bias, &quot;Linear Regression: Biased model&quot;); . Now the LOO-PIT check is clearly showing signs of bias in the model, whereas due to the lack of rescaling, no bias is seen in the posterior predictive checks. . Finally, let&#39;s combine some bias with overdispersion, to see how is LOO-PIT modified. Moreover, we will rescale the posterior predictive data to see how would rescaling affect the posterior predictive checks. . with pm.Model() as model: # model specifications in PyMC3 are wrapped in a with-statement # Define priors a1 = pm.Normal(&quot;a1&quot;, mu=0, sd=20) # Define likelihood likelihood = pm.Normal(&#39;obs&#39;, mu=a0_lr + 2 + a1 * data_x_regression, sd=1.5 * sd_lr, observed=data_y_regression) # Inference! trace = pm.sample() # draw posterior samples using NUTS sampling prior = pm.sample_prior_predictive() posterior_predictive = pm.sample_posterior_predictive(trace) idata_lr_bias_overdispersed = az.from_pymc3( trace=trace, prior=prior, posterior_predictive=posterior_predictive, coords=coords_regression, dims=dims_regression, ) . Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (4 chains in 4 jobs) NUTS: [a1] Sampling 4 chains, 0 divergences: 100%|██████████| 4000/4000 [00:00&lt;00:00, 5506.36draws/s] 100%|██████████| 2000/2000 [00:01&lt;00:00, 1842.99it/s] . plot_ppc_loopit(idata_lr_bias_overdispersed, &quot;Linear Regression: Biased and oversidpersed model&quot;); . pp_samples = idata_lr_bias_overdispersed.posterior_predictive.obs obs_samples = idata_lr_bias_overdispersed.observed_data.obs pp_means = pp_samples.mean(dim=(&quot;chain&quot;, &quot;draw&quot;)) pp_stds = pp_samples.std(dim=(&quot;chain&quot;, &quot;draw&quot;)) idata_lr_bias_overdispersed.posterior_predictive[&quot;obs_rescaled&quot;] = (pp_samples - pp_means) / pp_stds idata_lr_bias_overdispersed.observed_data[&quot;obs_rescaled&quot;] = (obs_samples - pp_means) / pp_stds . fig, axes = plt.subplots(3, 2, figsize=(12,13), constrained_layout=True) for i, var in enumerate((&quot;obs&quot;, &quot;obs_rescaled&quot;)): for j, ecdf in enumerate((False, True)): az.plot_loo_pit(idata_lr_bias_overdispersed, y=var, ecdf=ecdf, ax=axes[j, i]); az.plot_ppc(idata_lr_bias_overdispersed, ax=axes[2]); fig.suptitle(&quot;Linear Regression: Rescaling effect nBiased and overdispersed model&quot;, color=&quot;white&quot;, fontsize=16); . As you can see, the posterior predictive check for obs_rescaled does indicate overdispersion and bias of the posterior predictive samples, whereas the one for obs does not, following what we were seeing previously. The LOO-PIT checks do not change one bit however. This is actually a property of the LOO-PIT algorithm. As it is comparing the marginal distributions of the posterior predictive and the observed data using the MCMC samples, any monotonous transformation will not modify its value because it won&#39;t modify the order between the samples. Therefore, if the observed data is larger than 36% of the posterior predictive samples, the rescaling we have done does not modify this fact. .",
            "url": "https://oriolabril.github.io/oriol_unraveled/arviz/visualization/model%20criticism/2019/07/31/loo-pit-tutorial.html",
            "relUrl": "/arviz/visualization/model%20criticism/2019/07/31/loo-pit-tutorial.html",
            "date": " • Jul 31, 2019"
        }
        
    
  
    
        ,"post4": {
            "title": "LOO-CV on transformed data",
            "content": "Blog post exploring whether or not LOO-CV can be used to compare models that try to explain some data $y$ with models trying to explain the same data after a transformation $z=f(y)$. Inspired by @tiagocc question on Stan Forums. This post has two sections, the first one is the mathematical derivation of the equations used and their application on a validation example, and the second section is a real example. In addition to the LOO-CV usage examples and explanations, another goal of this notebook is to show and highlight the capabilities of ArviZ. . This post has been automatically generated from a Jupyter notebook that can be downloaded here . import pystan import pandas as pd import numpy as np import arviz as az import matplotlib.pyplot as plt . az.style.use(&quot;arviz-darkgrid&quot;) . Mathematical derivation and validation example . In the first example, we will compare two equivalent models: . $y sim text{LogNormal}( mu, sigma)$ | $ log y sim text{Normal}( mu, sigma)$ | Model definition and execution . Define the data and execute the two models . mu = 2 sigma = 1 logy = np.random.normal(loc=mu, scale=sigma, size=30) y = np.exp(logy) # y will then be distributed as lognormal data = { &#39;N&#39;: len(y), &#39;y&#39;: y, &#39;logy&#39;: logy } . #collapse-hide lognormal_code = &quot;&quot;&quot; data { int&lt;lower=0&gt; N; vector[N] y; } parameters { real mu; real&lt;lower=0&gt; sigma; } model { y ~ lognormal(mu, sigma); } generated quantities { vector[N] log_lik; vector[N] y_hat; for (i in 1:N) { log_lik[i] = lognormal_lpdf(y[i] | mu, sigma); y_hat[i] = lognormal_rng(mu, sigma); } } &quot;&quot;&quot; . . sm_lognormal = pystan.StanModel(model_code=lognormal_code) fit_lognormal = sm_lognormal.sampling(data=data, iter=1000, chains=4) . INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_af04dcd0464f65fe0e5bbc595b4eb9d6 NOW. . idata_lognormal = az.from_pystan( posterior=fit_lognormal, posterior_predictive=&#39;y_hat&#39;, observed_data=[&#39;y&#39;], log_likelihood={&#39;y&#39;: &#39;log_lik&#39;}, ) . #collapse-hide normal_on_log_code = &quot;&quot;&quot; data { int&lt;lower=0&gt; N; vector[N] logy; } parameters { real mu; real&lt;lower=0&gt; sigma; } model { logy ~ normal(mu, sigma); } generated quantities { vector[N] log_lik; vector[N] logy_hat; for (i in 1:N) { log_lik[i] = normal_lpdf(logy[i] | mu, sigma); logy_hat[i] = normal_rng(mu, sigma); } } &quot;&quot;&quot; . . sm_normal = pystan.StanModel(model_code=normal_on_log_code) fit_normal = sm_normal.sampling(data=data, iter=1000, chains=4) . INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_acd7c874588f1c862727f931f4dbf916 NOW. . idata_normal = az.from_pystan( posterior=fit_normal, posterior_predictive=&#39;logy_hat&#39;, observed_data=[&#39;logy&#39;], log_likelihood={&#39;logy&#39;: &#39;log_lik&#39;}, ) . Check model convergence. Use az.summary to in one view that the effective sample size (ESS) is large enough and $ hat{R}$ is close to one. . az.summary(idata_lognormal) . mean sd hdi_3% hdi_97% mcse_mean mcse_sd ess_mean ess_sd ess_bulk ess_tail r_hat . mu 2.193 | 0.199 | 1.832 | 2.577 | 0.005 | 0.004 | 1353.0 | 1350.0 | 1360.0 | 1374.0 | 1.0 | . sigma 1.112 | 0.157 | 0.844 | 1.398 | 0.004 | 0.003 | 1314.0 | 1182.0 | 1559.0 | 1091.0 | 1.0 | . az.summary(idata_normal) . mean sd hdi_3% hdi_97% mcse_mean mcse_sd ess_mean ess_sd ess_bulk ess_tail r_hat . mu 2.196 | 0.209 | 1.805 | 2.590 | 0.006 | 0.004 | 1213.0 | 1189.0 | 1223.0 | 1054.0 | 1.0 | . sigma 1.112 | 0.154 | 0.861 | 1.401 | 0.005 | 0.003 | 1169.0 | 1107.0 | 1293.0 | 956.0 | 1.0 | . In addition, we can plot the quantile ESS plot for one of them directly with plot_ess . az.plot_ess(idata_normal, kind=&quot;quantile&quot;, color=&quot;k&quot;); . Posterior validation . Check that both models are equivalent and do indeed give the same result for both parameters. . az.plot_density([idata_lognormal, idata_normal], data_labels=[&quot;Lognormal likelihood&quot;, &quot;Normal likelihood&quot;]); . Calculate LOO-CV . Now we get to calculate LOO-CV using Pareto Smoothed Importance Sampling as detailed in Vehtari et al., 2017. As we explained above, both models are equivalent, but one is in terms of $y$ and the other in terms of $ log y$. Therefore, their likelihoods will be on different scales, and hence, their expected log predictive density will also be different. . az.loo(idata_lognormal) . Computed from 2000 by 30 log-likelihood matrix Estimate SE elpd_loo -111.67 7.18 p_loo 2.14 - The scale is now log by default. Use &#39;scale&#39; argument or &#39;stats.ic_scale&#39; rcParam if you rely on a specific value. A higher log-score (or a lower deviance) indicates a model with better predictive accuracy. . az.loo(idata_normal) . Computed from 2000 by 30 log-likelihood matrix Estimate SE elpd_loo -46.23 4.56 p_loo 2.22 - The scale is now log by default. Use &#39;scale&#39; argument or &#39;stats.ic_scale&#39; rcParam if you rely on a specific value. A higher log-score (or a lower deviance) indicates a model with better predictive accuracy. . We have found that as expected, the two models yield different results despite being actually the same model. This is because. LOO is estimated from the log likelihood, $ log p(y_i mid theta^s)$, being $i$ the observation id, and $s$ the MCMC sample id. Following Vehtari et al., 2017, this log likelihood is used to calculate the PSIS weights and to estimate the expected log pointwise predictive density in the following way: . Calculate raw importance weights: $r_i^s = frac{1}{p(y_i mid theta^s)}$ | Smooth the $r_i^s$ (see original paper for details) to get the PSIS weights $w_i^s$ | Calculate elpd LOO as: | $$ text{elpd}_{psis-loo} = sum_{i=1}^n log left( frac{ sum_s w_i^s p(y_i| theta^s)}{ sum_s w_i^s} right) $$ . This will estimate the out of sample predictive fit of $y$ (where $y$ is the data of the model. Therefore, for the first model, using a LogNormal distribution, we are indeed calculating the desired quantity: . $$ text{elpd}_{psis-loo}^{(1)} approx sum_{i=1}^n log p(y_i|y_{-i}) $$ . Whereas for the second model, we are calculating: . $$ text{elpd}_{psis-loo}^{(2)} approx sum_{i=1}^n log p(z_i|z_{-i}) $$ . being $z_i = log y_i$. We actually have two different probability density functions, one over $y$ which from here on we will note $p_y(y)$, and $p_z(z)$. . In order to estimate the elpd loo for $y$ from the data in the second model, $z$, we have to describe $p_y(y)$ as a function of $z$ and $p_z(z)$. We know that $y$ and $z$ are actually related, and we can use this relation to find how would the random variable $y$ (which is actually a transformation of the random variable $z$) be distributed. This is done with the Jacobian. Therefore: . $$ p_y(y| theta)=p_z(z| theta)| frac{dz}{dy}|= frac{1}{|y|}p_z(z| theta)=e^{-z}p_z(z| theta) $$ In the log scale: . $$ log p_y(y| theta)=-z + log p_z(z| theta) $$ We apply the results to the log likelihood data of the second model (the normal on the logarithm instead of the lognormal) and check that now the result does coincide with the LOO-CV estimated by the lognormal model. . z = logy idata_normal.log_likelihood[&quot;y&quot;] = -z+idata_normal.log_likelihood.logy . az.loo(idata_normal, var_name=&quot;y&quot;) . Computed from 2000 by 30 log-likelihood matrix Estimate SE elpd_loo -111.75 7.18 p_loo 2.22 - The scale is now log by default. Use &#39;scale&#39; argument or &#39;stats.ic_scale&#39; rcParam if you rely on a specific value. A higher log-score (or a lower deviance) indicates a model with better predictive accuracy. . Real example . We will now use as data a subsample of a real dataset. The subset has been selected using: . df = pd.read_excel(&quot;indicator breast female incidence.xlsx&quot;).set_index(&quot;Breast Female Incidence&quot;).dropna(thresh=20).T df.to_csv(&quot;indicator_breast_female_incidence.csv&quot;) . Below, the data is loaded and plotted for inspection. . df = pd.read_csv(&quot;data/indicator_breast_female_incidence.csv&quot;, index_col=0) df.plot(figsize=(9,5.5)); . In order to show different examples of LOO on transformed data, we will take into account the following models: . $$ begin{align} &amp;y=a_1 x+a_0 &amp;y=e^{b_0}e^{b_1 x} &amp; rightarrow&amp; quad log y = z_1 = b_1 x + b_0 &amp;y=c_1^2 x^2 + 2 c_1 c_2 x + c_0^2 &amp; rightarrow&amp; quad sqrt{y} = z_2 = c_1 x + c_0 end{align} $$This models have been chosen mainly because of their simplicity. In addition, they can all be applied using the same Stan code and the data looks kind of linear. This will put the focus of the example on the loo calculation instead of on the model itself. For the online example, the data from Finland has been chosen, but feel free to download the notebook and experiment with it. . y_data = df.Finland z1_data = np.log(y_data) z2_data = np.sqrt(y_data) x_data = df.index/100 # rescale to set both to a similar scale dict_y = {&quot;N&quot;: len(x_data), &quot;y&quot;: y_data, &quot;x&quot;: x_data} dict_z1 = {&quot;N&quot;: len(x_data), &quot;y&quot;: z1_data, &quot;x&quot;: x_data} dict_z2 = {&quot;N&quot;: len(x_data), &quot;y&quot;: z2_data, &quot;x&quot;: x_data} coords = {&quot;year&quot;: x_data} dims = {&quot;y&quot;: [&quot;year&quot;], &quot;log_likelihood&quot;: [&quot;year&quot;]} . #collapse-hide lr_code = &quot;&quot;&quot; data { int&lt;lower=0&gt; N; vector[N] x; vector[N] y; } parameters { real b0; real b1; real&lt;lower=0&gt; sigma_e; } model { b0 ~ normal(0, 20); b1 ~ normal(0, 20); for (i in 1:N) { y[i] ~ normal(b0 + b1 * x[i], sigma_e); } } generated quantities { vector[N] log_lik; vector[N] y_hat; for (i in 1:N) { log_lik[i] = normal_lpdf(y[i] | b0 + b1 * x[i], sigma_e); y_hat[i] = normal_rng(b0 + b1 * x[i], sigma_e); } } &quot;&quot;&quot; . . sm_lr = pystan.StanModel(model_code=lr_code) control = {&quot;max_treedepth&quot;: 15} . INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_01dac21720941a01839c67cf2ac4a0fc NOW. . fit_y = sm_lr.sampling(data=dict_y, iter=1500, chains=6, control=control) . fit_z1 = sm_lr.sampling(data=dict_z1, iter=1500, chains=6, control=control) . fit_z2 = sm_lr.sampling(data=dict_z2, iter=1500, chains=6, control=control) . idata_y = az.from_pystan( posterior=fit_y, posterior_predictive=&#39;y_hat&#39;, observed_data=[&#39;y&#39;], log_likelihood={&#39;y&#39;: &#39;log_lik&#39;}, coords=coords, dims=dims, ) idata_y.posterior = idata_y.posterior.rename({&quot;b0&quot;: &quot;a0&quot;, &quot;b1&quot;: &quot;a1&quot;}) . idata_z1 = az.from_pystan( posterior=fit_z1, posterior_predictive=&#39;y_hat&#39;, observed_data=[&#39;y&#39;], log_likelihood={&#39;z1&#39;: &#39;log_lik&#39;}, coords=coords, dims=dims, ) . idata_z2 = az.from_pystan( posterior=fit_z2, posterior_predictive=&#39;y_hat&#39;, observed_data=[&#39;y&#39;], log_likelihood={&#39;z2&#39;: &#39;log_lik&#39;}, coords=coords, dims=dims, ) idata_z2.posterior = idata_z2.posterior.rename({&quot;b0&quot;: &quot;c0&quot;, &quot;b1&quot;: &quot;c1&quot;}) . In order to compare the out of sample predictive accuracy, we have to apply the Jacobian transformation to the 2 latter models, so that all of them are in terms of $y$. . Note: we will use LOO instead of Leave Future Out algorithm even though it may be more appropriate because the Jacobian transformation to be applied is the same in both cases. Moreover, PSIS-LOO does not require refitting, and it is already implemented in ArviZ. . The transformation to apply to the second model $z_1 = log y$ is the same as the previous example: . idata_z1.log_likelihood[&quot;y&quot;] = -z1_data.values+idata_z1.log_likelihood.z1 . In the case of the third model, $z_2 = sqrt{y}$: . $$ | frac{dz}{dy}| = | frac{1}{2 sqrt{y}}| = frac{1}{2 z_2} quad rightarrow quad log | frac{dz}{dy}| = - log (2 z_2)$$ . idata_z2.log_likelihood[&quot;y&quot;] = -np.log(2*z2_data.values)+idata_z2.log_likelihood.z2 . az.loo(idata_y) . Computed from 4500 by 46 log-likelihood matrix Estimate SE elpd_loo -194.31 3.88 p_loo 1.64 - The scale is now log by default. Use &#39;scale&#39; argument or &#39;stats.ic_scale&#39; rcParam if you rely on a specific value. A higher log-score (or a lower deviance) indicates a model with better predictive accuracy. . print(&quot;LOO before Jacobian transformation: {:.2f}&quot;.format(az.loo(idata_z1, var_name=&quot;z1&quot;).loo)) print(az.loo(idata_z1, var_name=&quot;y&quot;)) . LOO before Jacobian transformation: 70.70 Computed from 4500 by 46 log-likelihood matrix Estimate SE elpd_loo -100.30 4.41 p_loo 2.99 - The scale is now log by default. Use &#39;scale&#39; argument or &#39;stats.ic_scale&#39; rcParam if you rely on a specific value. A higher log-score (or a lower deviance) indicates a model with better predictive accuracy. . print(&quot;LOO before Jacobian transformation: {:.2f}&quot;.format(az.loo(idata_z2, var_name=&quot;z2&quot;).loo)) print(az.loo(idata_z2, var_name=&quot;y&quot;)) . LOO before Jacobian transformation: 2.23 Computed from 4500 by 46 log-likelihood matrix Estimate SE elpd_loo -115.15 3.83 p_loo 2.93 - The scale is now log by default. Use &#39;scale&#39; argument or &#39;stats.ic_scale&#39; rcParam if you rely on a specific value. A higher log-score (or a lower deviance) indicates a model with better predictive accuracy. . References . Vehtari, A., Gelman, A., and Gabry, J. (2017): Practical Bayesian Model Evaluation Using Leave-One-OutCross-Validation and WAIC, Statistics and Computing, vol. 27(5), pp. 1413–1432. .",
            "url": "https://oriolabril.github.io/oriol_unraveled/arviz/model%20comparison/2019/06/21/loo-cv-transformed-data.html",
            "relUrl": "/arviz/model%20comparison/2019/06/21/loo-cv-transformed-data.html",
            "date": " • Jun 21, 2019"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me | Oriol unraveled",
          "content": "I’m a rebel physicist/engineer who loves statistical data analysis. By day, I am currently a Research Assistant on Bayesian Model Selection with David Rossell at UPF, Barcelona. By night I am a core developer of ArviZ a Python package for exploratory analysis of Bayesian models. In addition to data analysis probabilistic modeling, I also love programming and teaching. . I think that the culture in scientific research needs deep changes towards a more collaborative, open and diverse model. I am interested in open science, reproducible research and science communication. I want to pursue a career in probabilistic modeling and statistical research with special emphasis on openness and reproducibility. . In my spare time, I like playing board games and going to the beach to do water activities. I have been sailing and snorkeling regularly since I was little and more I recently added kayaking to the mix too! I generally spend the summer at the Costa Brava. Here I leave you a sneak peak of the views when nobody is around . . Projects . Here are highlighted some projects I contribute to, check out my GitHub profile for a complete list of the projects I contribute to. . ArviZ: Exploratory analysis of Bayesian models in Python or Julia | mombf: Bayesian model selection and averaging for regression and mixtures for non-local and local priors. | exosherlock: Smooth your interactions with the NASA Exoplanet Archive using Python and pandas. | PyMC3/4: Friendly probabilistic programming in Python. | . Publications and presentations . PROBPROG 2020: Coming on autumn 2020 | M. Badenas-Agusti, M. N. Günther, T. Daylan, et al., 2020, HD 191939: Three Sub-Neptunes Transiting a Sun-like Star Only 54 pc Away | D. Foreman-Mackey, W. Farr, M. Sinha, A. Archibald, et al., 2019, emcee v3: A Python ensemble sampling toolkit for affine-invariant MCMC. Get the emcee package code! | . | .",
          "url": "https://oriolabril.github.io/oriol_unraveled/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  
      ,"page2": {
          "title": "Blog | Oriol unraveled",
          "content": "",
          "url": "https://oriolabril.github.io/oriol_unraveled/blog.html",
          "relUrl": "/blog.html",
          "date": ""
      }
      
  

  

  

  
  

  
  

  

  
  

  
  

  
  

  
  

  
  

  
      ,"page13": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://oriolabril.github.io/oriol_unraveled/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

  
  

}